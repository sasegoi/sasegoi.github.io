<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sasegoi.github.io.git","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="机器学习课程复习2Lesson 5Neural NetworksArtificial Neural Network (ANN) Black box methodComplex mathematical underpinnings of models and results that are challenging to interpret.">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习课程复习2">
<meta property="og:url" content="https://sasegoi.github.io.git/2020/04/05/Machine-Learning-Lecture-Review-2/index.html">
<meta property="og:site_name" content="Sasegoi&#39;s Blog">
<meta property="og:description" content="机器学习课程复习2Lesson 5Neural NetworksArtificial Neural Network (ANN) Black box methodComplex mathematical underpinnings of models and results that are challenging to interpret.">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-04-05T00:38:08.000Z">
<meta property="article:modified_time" content="2020-04-05T13:10:12.951Z">
<meta property="article:author" content="sasegoi">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://sasegoi.github.io.git/2020/04/05/Machine-Learning-Lecture-Review-2/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>机器学习课程复习2 | Sasegoi's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Sasegoi's Blog" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Sasegoi's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">如人饮水，冷暖自知</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>站点地图</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sasegoi.github.io.git/2020/04/05/Machine-Learning-Lecture-Review-2/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sasegoi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sasegoi's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习课程复习2
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2020-04-05 08:38:08 / 修改时间：21:10:12" itemprop="dateCreated datePublished" datetime="2020-04-05T08:38:08+08:00">2020-04-05</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>57k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>52 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h1 id="机器学习课程复习2"><a href="#机器学习课程复习2" class="headerlink" title="机器学习课程复习2"></a>机器学习课程复习2</h1><h2 id="Lesson-5"><a href="#Lesson-5" class="headerlink" title="Lesson 5"></a>Lesson 5</h2><h3 id="Neural-Networks"><a href="#Neural-Networks" class="headerlink" title="Neural Networks"></a>Neural Networks</h3><p>Artificial Neural Network (ANN)</p>
<h4 id="Black-box-method"><a href="#Black-box-method" class="headerlink" title="Black box method"></a>Black box method</h4><p>Complex mathematical underpinnings of models and results that are challenging to interpret.</p>
<a id="more"></a>
<h4 id="How-ANN-works"><a href="#How-ANN-works" class="headerlink" title="How ANN works?"></a>How ANN works?</h4><ul>
<li>Activation function<ul>
<li>Threshold activations function (0-1)</li>
<li>Sigmoid activation function</li>
<li>LInear</li>
<li>Saturated Linear</li>
<li>Hyperbolic Tangent</li>
<li>Gaussian</li>
</ul>
</li>
<li>Network architecture (topology)<ul>
<li>determines the complexity of tasks implemented within the network</li>
<li>identify more subtle patterns or sophisticated decision boundaries</li>
<li>The way that units are aranged also matters </li>
<li>Multilayer networks are usually fully connected, but not always<ul>
<li>Every node in one layer is connected to every node in the next layer</li>
</ul>
</li>
<li>Recurrent (feedback) networks<ul>
<li>Delay stands for short term memory<ul>
<li>capability to understand sequence of events</li>
<li>suitablefor forecasting    </li>
</ul>
</li>
</ul>
</li>
</ul>
</li>
<li>Training algorithm</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># import the data</span><br><span class="line">concrete &lt;- read.csv(&quot;compresive_strength_concrete.csv&quot;)</span><br><span class="line">str(concrete)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># normalize features</span><br><span class="line">normalize &lt;- function(x) &#123;</span><br><span class="line">return((x - min(x)) &#x2F; (max(x) - min(x)))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">concrete_norm &lt;- as.data.frame(lapply(concrete, normalize))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># check the outcome of normalization</span><br><span class="line">summary(concrete_norm$Strength)</span><br><span class="line"></span><br><span class="line"># how it was before?</span><br><span class="line">summary(concrete$Strength)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># training and testing datasets</span><br><span class="line">concrete_train &lt;- concrete_norm[1:773, ]</span><br><span class="line">concrete_test &lt;- concrete_norm[774:1030, ]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># training a model</span><br><span class="line">install.packages(&quot;neuralnet&quot;)</span><br><span class="line">library(neuralnet)</span><br><span class="line"></span><br><span class="line"># create a simple multilayer feedforward network with a single hidden node</span><br><span class="line">concrete_model &lt;- neuralnet(Strength ~ Cement + Slag +</span><br><span class="line">                              Ash + Water + Superplastic +</span><br><span class="line">                              Coarseagg + Fineagg + Age,</span><br><span class="line">                            data &#x3D; concrete_train)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># plot the outcome</span><br><span class="line">plot(concrete_model)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># evaluate the model</span><br><span class="line"># measure the correlation between our predicted concrete strength and the true value</span><br><span class="line"># insight into the strength of the linear association between the two variables</span><br><span class="line">model_results &lt;- compute(concrete_model, concrete_test[1:8])</span><br><span class="line">predicted_strength &lt;- model_results$net.result</span><br><span class="line">cor(predicted_strength, concrete_test$strength)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># improve the model</span><br><span class="line">concrete_model2 &lt;- neuralnet(Strength ~ Cement + Slag +</span><br><span class="line">                               Ash + Water + Superplastic +</span><br><span class="line">                               Coarseagg + Fineagg + Age,</span><br><span class="line">                             data &#x3D; concrete_train, hidden &#x3D; 5)</span><br><span class="line"></span><br><span class="line">plot(concrete_model2)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># is it better?</span><br><span class="line">model_results2 &lt;- compute(concrete_model2, concrete_test[1:8])</span><br><span class="line">predicted_strength2 &lt;- model_results2$net.result</span><br><span class="line">cor(predicted_strength2, concrete_test$strength)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># load the data</span><br><span class="line">divdata &lt;- read.csv(&quot;dividendinfo.csv&quot;)</span><br><span class="line">attach(divdata)</span><br><span class="line">str(divdata)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># scaling the data</span><br><span class="line">scaleddata&lt;-scale(divdata)</span><br><span class="line"></span><br><span class="line"># max min normalization</span><br><span class="line">normalize &lt;- function(x) &#123;</span><br><span class="line">  return ((x - min(x)) &#x2F; (max(x) - min(x)))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">maxmindf &lt;- as.data.frame(lapply(divdata, normalize))</span><br><span class="line"></span><br><span class="line">str(maxmindf)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># training and testing sets</span><br><span class="line">trainset &lt;- maxmindf[1:160, ]</span><br><span class="line">testset &lt;- maxmindf[161:200, ]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># creating neural nets</span><br><span class="line">install.packages(&quot;neuralnet&quot;)</span><br><span class="line">library(neuralnet)</span><br><span class="line">nn &lt;- neuralnet(dividend ~ fcfps + earnings_growth + de + mcap + current_ratio, data&#x3D;trainset, hidden&#x3D;c(2,1), linear.output&#x3D;FALSE, threshold&#x3D;0.01)</span><br><span class="line">nn$result.matrix</span><br><span class="line">plot(nn)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># testing the model</span><br><span class="line">temp_test &lt;- subset(testset, select &#x3D; c(&quot;fcfps&quot;,&quot;earnings_growth&quot;, &quot;de&quot;, &quot;mcap&quot;, &quot;current_ratio&quot;))</span><br><span class="line">head(temp_test)</span><br><span class="line">nn.results &lt;- compute(nn, temp_test)</span><br><span class="line">results &lt;- data.frame(actual &#x3D; testset$dividend, prediction &#x3D; nn.results$net.result)</span><br><span class="line">results</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># confusion matrix</span><br><span class="line">roundedresults&lt;-sapply(results,round,digits&#x3D;0)</span><br><span class="line">roundedresultsdf&#x3D;data.frame(roundedresults)</span><br><span class="line">attach(roundedresultsdf)</span><br><span class="line">table(actual,prediction)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># install relevant packages</span><br><span class="line">install.packages(&quot;neuralnet&quot;)</span><br><span class="line">library(neuralnet)</span><br><span class="line">require(neuralnet)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># create a dataset</span><br><span class="line"># let&#39;s assume that we deal with job applications</span><br><span class="line"># ITK stands for it knowledge of a candidate </span><br><span class="line"># SSK is for social skills</span><br><span class="line"># PAS means previous applications status</span><br><span class="line">ITK&#x3D;c(15,40,10,90,70,80)</span><br><span class="line">SSK&#x3D;c(20,5,60,85,50,30)</span><br><span class="line">PAS&#x3D;c(1,0,1,1,0,0)</span><br><span class="line">df&#x3D;data.frame(ITK, SSK, PAS)</span><br><span class="line">df</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># develop a neural network classifier</span><br><span class="line"># PAS is a label and ITK&amp;SSK are features</span><br><span class="line"># hidden&#x3D;3: represents single layer with 3 neurons respectively</span><br><span class="line"># act.fct &#x3D; &quot;logistic&quot; used for smoothing the result</span><br><span class="line"># linear.ouput&#x3D;FALSE: set FALSE for apply act.fct otherwise TRUE</span><br><span class="line">require(neuralnet)</span><br><span class="line">print(nnc&lt;-neuralnet(PAS~ITK+SSK, data&#x3D;df, hidden&#x3D;3, act.fct &#x3D; &quot;logistic&quot;,</span><br><span class="line">                     linear.output &#x3D; FALSE, rep&#x3D;5))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">plot(nnc)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># create a test dataset</span><br><span class="line">SSK&#x3D;c(10,5,80)</span><br><span class="line">ITK&#x3D;c(30,25,70)</span><br><span class="line">test&#x3D;data.frame(SSK,ITK)</span><br><span class="line">test</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># do the prediction</span><br><span class="line">Predict&#x3D;compute(nnc,test)</span><br><span class="line">Predict$net.result</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># convert the outcome into binary classes</span><br><span class="line">prob &lt;- Predict$net.result</span><br><span class="line">pred &lt;- ifelse(prob&gt;0.5, 1, 0)</span><br><span class="line">pred</span><br></pre></td></tr></table></figure>

<h3 id="Regression-Methods"><a href="#Regression-Methods" class="headerlink" title="Regression Methods"></a>Regression Methods</h3><h4 id="Regression-trees-and-model-trees"><a href="#Regression-trees-and-model-trees" class="headerlink" title="Regression trees and model trees"></a>Regression trees and model trees</h4><ul>
<li>Regression trees<ul>
<li>Make predictions based on the average value of examples that reach a leaf</li>
</ul>
</li>
<li>Model trees<ul>
<li>Like above, but at leaf, a multiple linear regression model is built from examples reaching that node</li>
<li>A lot of such models may be built this way</li>
<li>More accurate models (usually) than regression trees</li>
</ul>
</li>
</ul>
<p>Standard deviation reduction (SDR) measures the reducion in standard deviation from the original value to the weighted standard deviation post-split.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># load the data</span><br><span class="line">insurance &#x3D; read.csv(&quot;insurance.csv&quot;, stringsAsFactors &#x3D; T)</span><br><span class="line">str(insurance)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># inspect the data</span><br><span class="line">summary(insurance)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># see the histogram</span><br><span class="line">hist(insurance$expenses, breaks &#x3D; 30)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># check various features</span><br><span class="line">table(insurance$sex)</span><br><span class="line">table(insurance$smoker)</span><br><span class="line">table(insurance$region)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># correlation matrix</span><br><span class="line">cor(insurance[c(&quot;age&quot;, &quot;bmi&quot;, &quot;children&quot;, &quot;expenses&quot;)])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># plots of relationships</span><br><span class="line">pairs(insurance[c(&quot;age&quot;, &quot;bmi&quot;, &quot;children&quot;, &quot;expenses&quot;)])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># more visualization</span><br><span class="line">install.packages(&quot;psych&quot;)</span><br><span class="line">library(psych)</span><br><span class="line">pairs.panels(insurance[c(&quot;age&quot;, &quot;bmi&quot;, &quot;children&quot;, &quot;expenses&quot;)])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># train a model for forecasting</span><br><span class="line">ins_model &lt;- lm(expenses ~ ., data &#x3D; insurance)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary(ins_model)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># improve the model</span><br><span class="line">insurance$age2 &lt;- insurance$age^2</span><br><span class="line">insurance$bmi30 &lt;- ifelse(insurance$bmi &gt;&#x3D; 30, 1, 0)</span><br><span class="line">ins_model2 &lt;- lm(expenses ~ age + age2 + children + bmi + sex +</span><br><span class="line">                   bmi30*smoker + region, data &#x3D; insurance)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary(ins_model2)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># upload and inspect the data</span><br><span class="line">wine &lt;- read.csv(&quot;whitewines.csv&quot;)</span><br><span class="line">str(wine)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># make a histogram</span><br><span class="line">hist(wine$quality)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># create subsets for training and testing</span><br><span class="line">wine_train &lt;- wine[1:3750, ]</span><br><span class="line">wine_test &lt;- wine[3751:4898, ]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># install relevant packages</span><br><span class="line">install.packages(&quot;rpart&quot;)</span><br><span class="line">library(rpart)</span><br><span class="line">install.packages(&quot;rpart.plot&quot;)</span><br><span class="line">library(rpart.plot)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># train a model</span><br><span class="line">m.rpart &lt;- rpart(quality ~ ., data &#x3D; wine_train)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># inspect the object</span><br><span class="line">summary(m.rpart)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># visualize the outcome</span><br><span class="line">rpart.plot(m.rpart, digits &#x3D; 3)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># put the leaf nodes at the bottom of the plot</span><br><span class="line">rpart.plot(m.rpart, digits &#x3D; 4, fallen.leaves &#x3D; TRUE,</span><br><span class="line">             type &#x3D; 3, extra &#x3D; 101)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># evaluate the model</span><br><span class="line">p.rpart &lt;- predict(m.rpart, wine_test)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary(p.rpart)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary(wine_test$quality)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># correlation only measures how strongly the predictions are related to the true value</span><br><span class="line"># it is not a measure of how far off the predictions were from the true values</span><br><span class="line">cor(p.rpart, wine_test$quality)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># more visualization</span><br><span class="line">plotcp(m.rpart)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">m.rpart$cptable</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># another measure: mean absolute error</span><br><span class="line">MAE &lt;- function(actual, predicted) &#123;</span><br><span class="line">    mean(abs(actual - predicted))</span><br><span class="line">&#125;</span><br><span class="line">MAE(p.rpart, wine_test$quality)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># mean quality rating</span><br><span class="line">mean(wine_train$quality)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">MAE(5.89, wine_test$quality)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># suggestions: M5P or caret or ipred approaches</span><br><span class="line"># caret way</span><br><span class="line">install.packages(&quot;caret&quot;)</span><br><span class="line">library(caret)</span><br><span class="line">install.packages(&quot;e1071&quot;)</span><br><span class="line">library(e1071)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"># specify 10-fold cross validation</span><br><span class="line">ctrl &lt;- trainControl(method &#x3D; &quot;cv&quot;,  number &#x3D; 10) </span><br><span class="line"></span><br><span class="line"># CV bagged model</span><br><span class="line">bagged_cv &lt;- train(</span><br><span class="line">  quality ~ .,</span><br><span class="line">  data &#x3D; wine_train,</span><br><span class="line">  method &#x3D; &quot;treebag&quot;,</span><br><span class="line">  trControl &#x3D; ctrl,</span><br><span class="line">  importance &#x3D; TRUE</span><br><span class="line">  )</span><br><span class="line"></span><br><span class="line"># assess results</span><br><span class="line">bagged_cv</span><br><span class="line"></span><br><span class="line"># plot most important variables</span><br><span class="line">plot(varImp(bagged_cv), 20)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># do the prediction</span><br><span class="line">pred &lt;- predict(bagged_cv, wine_test)</span><br><span class="line">RMSE(pred, ames_test$quality)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># ipred way</span><br><span class="line">install.packages(&quot;ipred&quot;)</span><br><span class="line">library(ipred)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># make bootstrapping reproducible</span><br><span class="line">set.seed(123)</span><br><span class="line"></span><br><span class="line"># train bagged model</span><br><span class="line">bagged_m1 &lt;- bagging(</span><br><span class="line">  formula &#x3D; quality ~ .,</span><br><span class="line">  data    &#x3D; wine_train,</span><br><span class="line">  coob    &#x3D; TRUE</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">bagged_m1</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"># assess 10-50 bagged trees</span><br><span class="line">ntree &lt;- 10:50</span><br><span class="line"></span><br><span class="line"># create empty vector to store OOB RMSE values</span><br><span class="line">rmse &lt;- vector(mode &#x3D; &quot;numeric&quot;, length &#x3D; length(ntree))</span><br><span class="line"></span><br><span class="line">for (i in seq_along(ntree)) &#123;</span><br><span class="line">  # reproducibility</span><br><span class="line">  set.seed(123)</span><br><span class="line">  </span><br><span class="line">  # perform bagged model</span><br><span class="line">  model &lt;- bagging(</span><br><span class="line">  formula &#x3D; quality ~ .,</span><br><span class="line">  data    &#x3D; wine_train,</span><br><span class="line">  coob    &#x3D; TRUE,</span><br><span class="line">  nbagg   &#x3D; ntree[i]</span><br><span class="line">)</span><br><span class="line">  # get OOB error</span><br><span class="line">  rmse[i] &lt;- model$err</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">plot(ntree, rmse, type &#x3D; &#39;l&#39;, lwd &#x3D; 2)</span><br><span class="line">abline(v &#x3D; 25, col &#x3D; &quot;red&quot;, lty &#x3D; &quot;dashed&quot;)</span><br></pre></td></tr></table></figure>

<h2 id="Lesson-6"><a href="#Lesson-6" class="headerlink" title="Lesson 6"></a>Lesson 6</h2><h3 id="Measuring-Model-Performance"><a href="#Measuring-Model-Performance" class="headerlink" title="Measuring Model Performance"></a>Measuring Model Performance</h3><ul>
<li>prediction</li>
<li>confusion matrices</li>
<li>the kappa statistic<ul>
<li>very good agreement: .8 to 1</li>
</ul>
</li>
<li>sensitivity (the true positive rate)</li>
<li>specificity (teh true negative rate)</li>
<li>precision</li>
<li>recall</li>
<li>F-measure/ F1 score</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">credit &lt;- read.csv(&quot;credit_ger.csv&quot;)</span><br><span class="line">#divide data into 3 pieces</span><br><span class="line">#the first line creates a vector of randomly ordered row IDs from 1 to 1000</span><br><span class="line">random_ids &lt;- order(runif(1000))</span><br><span class="line">credit_train &lt;- credit[random_ids[1:500],]</span><br><span class="line">credit_validate &lt;- credit[random_ids[501:750], ]</span><br><span class="line">credit_test &lt;- credit[random_ids[751:1000], ]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&quot;caret&quot;)</span><br><span class="line">library(caret)</span><br><span class="line">#a vector of class values must be specified</span><br><span class="line">#p specifies the proportion of instances to be included in the partition</span><br><span class="line">in_train &lt;- createDataPartition(credit$class, p &#x3D; 0.75, list &#x3D; FALSE)</span><br><span class="line">credit_train &lt;- credit[in_train, ]</span><br><span class="line">credit_test &lt;- credit[-in_train, ]</span><br><span class="line">#cross-validation</span><br><span class="line">#let&#39;s create 10 folds</span><br><span class="line">folds &lt;- createFolds(credit$class, k &#x3D; 10)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">str(folds)</span><br><span class="line">#list of vectors storing the row numbers for each of the k &#x3D; 10 requested folds</span><br><span class="line">#assign selected examples to subsets</span><br><span class="line">credit01_train &lt;- credit[folds$Fold01, ]</span><br><span class="line">credit01_test &lt;- credit[-folds$Fold01, ]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#automated way</span><br><span class="line">install.packages(&quot;C50&quot;)</span><br><span class="line">library(C50)</span><br><span class="line">install.packages(&quot;irr&quot;)</span><br><span class="line">library(irr)</span><br><span class="line">#create a list of 10 folds as we have done previously</span><br><span class="line">set.seed(123)</span><br><span class="line">folds &lt;- createFolds(credit$class, k &#x3D; 10)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">#the function below divides the credit data frame into training and test data, </span><br><span class="line">#creates a decision tree using the C5.0() function on the training data, </span><br><span class="line">#generates a set of predictions from the test data, and compares the predicted </span><br><span class="line">#and actual values using the kappa2() function</span><br><span class="line">cv_results &lt;- lapply(folds, function(x) &#123;</span><br><span class="line">credit_train &lt;- credit[x, ]</span><br><span class="line">credit_test &lt;- credit[-x, ]</span><br><span class="line">credit_model &lt;- C5.0(class ~ ., data &#x3D; credit_train) </span><br><span class="line">credit_pred &lt;- predict(credit_model, credit_test) </span><br><span class="line">credit_actual &lt;- credit_test$class</span><br><span class="line">kappa &lt;- kappa2(data.frame(credit_actual, credit_pred))$value</span><br><span class="line">    return(kappa)</span><br><span class="line">  &#125;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#kappa statistics are compiled into a list stored in the cv_results</span><br><span class="line">str(cv_results)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#we have transformed our list of IDs for 10 folds into a list of kappa statistics</span><br><span class="line">#we need to calculate the average of these 10 values</span><br><span class="line">mean(unlist(cv_results))</span><br><span class="line">#this kappa statistic is fairly low—in fact, </span><br><span class="line">#this corresponds to &quot;poor&quot; on the interpretation scale</span><br><span class="line">#which suggests that the credit scoring model does not perform much better than random chance</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#now we start training our model on the data applied</span><br><span class="line">#we use kNN from the class package</span><br><span class="line">library(class)</span><br><span class="line"></span><br><span class="line">#Euclidean distance is a baseline choice</span><br><span class="line">#we have to specify the k number</span><br><span class="line">#let&#39;s try with k&#x3D;21 as it is equal to the square root of 469</span><br><span class="line">wdbc_test_pred &lt;- knn(train &#x3D; wdbc_train, test &#x3D; wdbc_test, </span><br><span class="line">                      cl &#x3D; wdbc_train_labels, k&#x3D;21)</span><br><span class="line"></span><br><span class="line">#evaluate model performance</span><br><span class="line">#how well the predicted classes match up with the known values?</span><br><span class="line">install.packages(&quot;gmodels&quot;)</span><br><span class="line">library(gmodels)</span><br><span class="line">CrossTable(x&#x3D;wdbc_test_labels, y&#x3D;wdbc_test_pred,</span><br><span class="line">           prop.chisq &#x3D; FALSE)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#true negative (top-left cell) and true positive (bottom-right cell)</span><br><span class="line">#fails on the other diagonal (top-right and bottom-left cells)</span><br><span class="line">#costly mistakes due to classification</span><br><span class="line"></span><br><span class="line">#try to improve our model with z-score standardization</span><br><span class="line">#allow the outliers to be weighted more heavily in the distance calculation</span><br><span class="line">#built-in scale function for rescaling via the z-score standardization</span><br><span class="line">wdbc_z &lt;- as.data.frame(scale(wdbc[-1]))</span><br><span class="line">summary(wdbc_z$area_mean)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">#the mean after the z-score stand should always be 0</span><br><span class="line">#z-score values larger than 3 or less than -3 suggest extremely rare value</span><br><span class="line">#try to do the prediction once again</span><br><span class="line">wdbc_train &lt;- wdbc_z[1:469,]</span><br><span class="line">wdbc_test &lt;- wdbc_z[470:569,]</span><br><span class="line">wdbc_train_labels &lt;- wdbc[1:469,1]</span><br><span class="line">wdbc_test_labels &lt;- wdbc[470:569,1]</span><br><span class="line">wdbc_test_pred &lt;- knn(train &#x3D; wdbc_train, test &#x3D; wdbc_test,</span><br><span class="line">                      cl &#x3D; wdbc_train_labels, k&#x3D;21)</span><br><span class="line">CrossTable(x &#x3D; wdbc_test_labels, wdbc_test_pred,</span><br><span class="line">           prop.chisq &#x3D; FALSE)</span><br><span class="line"></span><br><span class="line">#our accuracy after changes?</span><br><span class="line">#try with different k</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#less elegant table</span><br><span class="line">table(wdbc_test_labels,wdbc_test_pred)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#how about confusion matrix?</span><br><span class="line">install.packages(&quot;caret&quot;)</span><br><span class="line">library(caret)</span><br><span class="line">install.packages(&quot;e1071&quot;)</span><br><span class="line">library(e1071)</span><br><span class="line">confusionMatrix(wdbc_test_pred,wdbc_test_labels)</span><br><span class="line">confusionMatrix(wdbc_test_pred,wdbc_test_labels, positive &#x3D; &quot;Malignant&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">#you see the kappa stat above and other measures as well</span><br><span class="line">#naturally, you can compute them manually if you want</span><br><span class="line">#separate codes for the kappa stat</span><br><span class="line">install.packages(&quot;vcd&quot;)</span><br><span class="line">library(vcd)</span><br><span class="line">Kappa(table(wdbc_test_labels, wdbc_test_pred))</span><br><span class="line">#sensitivity and specificity</span><br><span class="line">#also available avove</span><br><span class="line">sensitivity(wdbc_test_pred, wdbc_test_labels,</span><br><span class="line">positive &#x3D; &quot;Malignant&quot;)</span><br><span class="line">specificity(wdbc_test_pred, wdbc_test_labels,</span><br><span class="line">negative &#x3D; &quot;Benign&quot;)</span><br><span class="line">#precision</span><br><span class="line">posPredValue(wdbc_test_pred, wdbc_test_labels,</span><br><span class="line">positive &#x3D; &quot;Malignant&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#f-score</span><br><span class="line">install.packages(&quot;MLmetrics&quot;)</span><br><span class="line">library(MLmetrics)</span><br><span class="line">F1_Score(wdbc_test_labels, wdbc_test_pred, positive &#x3D; NULL)</span><br><span class="line">F1_Score(wdbc_test_labels, wdbc_test_pred, positive &#x3D; &quot;Malignant&quot;)</span><br></pre></td></tr></table></figure>

<h3 id="Visualizing-Performance-Tradeoffs"><a href="#Visualizing-Performance-Tradeoffs" class="headerlink" title="Visualizing Performance Tradeoffs"></a>Visualizing Performance Tradeoffs</h3><h4 id="ROC-curve"><a href="#ROC-curve" class="headerlink" title="ROC curve"></a>ROC curve</h4><p>Used to examine the tradeoff between the detection of true positives, while avoiding the false positives.</p>
<p>The area under the ROC curve: abbreviated AUC (ranges from .5 to 1). AUC closed to 1 shows better result.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#simple ROCR approach</span><br><span class="line">#use built-in dataset</span><br><span class="line">install.packages(&quot;ROCR&quot;)</span><br><span class="line">library(ROCR)</span><br><span class="line">data(ROCR.simple)</span><br><span class="line">head(cbind(ROCR.simple$predictions, ROCR.simple$labels), 5)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#make a prediction</span><br><span class="line">pred &lt;- prediction(ROCR.simple$predictions,ROCR.simple$labels)</span><br><span class="line">class(pred)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">slotNames(pred)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#lengths of slots</span><br><span class="line">sn &#x3D; slotNames(pred)</span><br><span class="line">sapply(sn, function(x) length(slot(pred, x)))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sapply(sn, function(x) class(slot(pred, x)))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#multiple predictions and labels</span><br><span class="line">data(ROCR.hiv)</span><br><span class="line">manypred &#x3D; prediction(ROCR.hiv$hiv.nn$predictions, ROCR.hiv$hiv.nn$labels)</span><br><span class="line">sapply(sn, function(x) length(slot(manypred, x)))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sapply(sn, function(x) class(slot(manypred, x)))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#one set of predictions and labels</span><br><span class="line">roc.perf &#x3D; performance(pred, measure &#x3D; &quot;tpr&quot;, x.measure &#x3D; &quot;fpr&quot;)</span><br><span class="line">plot(roc.perf)</span><br><span class="line">abline(a&#x3D;0, b&#x3D; 1)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#multiple predictions and labels</span><br><span class="line">many.roc.perf &#x3D; performance(manypred, measure &#x3D; &quot;tpr&quot;, x.measure &#x3D; &quot;fpr&quot;)</span><br><span class="line">plot(many.roc.perf, col&#x3D;1:10)</span><br><span class="line">abline(a&#x3D;0, b&#x3D; 1)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#partial ROC curve</span><br><span class="line">pROC &#x3D; function(pred, fpr.stop)&#123;</span><br><span class="line">    perf &lt;- performance(pred,&quot;tpr&quot;,&quot;fpr&quot;)</span><br><span class="line">    for (iperf in seq_along(perf@x.values))&#123;</span><br><span class="line">        ind &#x3D; which(perf@x.values[[iperf]] &lt;&#x3D; fpr.stop)</span><br><span class="line">        perf@y.values[[iperf]] &#x3D; perf@y.values[[iperf]][ind]</span><br><span class="line">        perf@x.values[[iperf]] &#x3D; perf@x.values[[iperf]][ind]</span><br><span class="line">    &#125;</span><br><span class="line">    return(perf)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">proc.perf &#x3D; pROC(pred, fpr.stop&#x3D;0.1)</span><br><span class="line">plot(proc.perf)</span><br><span class="line">abline(a&#x3D;0, b&#x3D; 1)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#optimal cut</span><br><span class="line">#we want the point closest to the TPR of (1) and FPR of (0)</span><br><span class="line">#it weighs both sensitivity and specificity equally</span><br><span class="line">opt.cut &#x3D; function(perf, pred)&#123;</span><br><span class="line">    cut.ind &#x3D; mapply(FUN&#x3D;function(x, y, p)&#123;</span><br><span class="line">        d &#x3D; (x - 0)^2 + (y-1)^2</span><br><span class="line">        ind &#x3D; which(d &#x3D;&#x3D; min(d))</span><br><span class="line">        c(sensitivity &#x3D; y[[ind]], specificity &#x3D; 1-x[[ind]], </span><br><span class="line">            cutoff &#x3D; p[[ind]])</span><br><span class="line">    &#125;, perf@x.values, perf@y.values, pred@cutoffs)</span><br><span class="line">&#125;</span><br><span class="line">print(opt.cut(roc.perf, pred))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cost.perf &#x3D; performance(pred, &quot;cost&quot;)</span><br><span class="line">pred@cutoffs[[1]][which.min(cost.perf@y.values[[1]])]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#different costs of FN and FP</span><br><span class="line">#false positives are twice as costly as false negatives - assumption</span><br><span class="line">cost.perf &#x3D; performance(pred, &quot;cost&quot;, cost.fp &#x3D; 2, cost.fn &#x3D; 1)</span><br><span class="line">pred@cutoffs[[1]][which.min(cost.perf@y.values[[1]])]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(opt.cut(many.roc.perf, manypred))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">acc.perf &#x3D; performance(pred, measure &#x3D; &quot;acc&quot;)</span><br><span class="line">plot(acc.perf)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">ind &#x3D; which.max( slot(acc.perf, &quot;y.values&quot;)[[1]] )</span><br><span class="line">acc &#x3D; slot(acc.perf, &quot;y.values&quot;)[[1]][ind]</span><br><span class="line">cutoff &#x3D; slot(acc.perf, &quot;x.values&quot;)[[1]][ind]</span><br><span class="line">print(c(accuracy&#x3D; acc, cutoff &#x3D; cutoff))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#same with multiple predictions and labels</span><br><span class="line">many.acc.perf &#x3D; performance(manypred, measure &#x3D; &quot;acc&quot;)</span><br><span class="line">sapply(manypred@labels, function(x) mean(x &#x3D;&#x3D; 1))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">mapply(function(x, y)&#123;</span><br><span class="line">  ind &#x3D; which.max( y )</span><br><span class="line">  acc &#x3D; y[ind]</span><br><span class="line">  cutoff &#x3D; x[ind]</span><br><span class="line">  return(c(accuracy&#x3D; acc, cutoff &#x3D; cutoff))</span><br><span class="line">&#125;, slot(many.acc.perf, &quot;x.values&quot;), slot(many.acc.perf, &quot;y.values&quot;))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#area under curve (AUC)summarizes the ROC curve just by taking the area between the curve and the x-axis</span><br><span class="line">auc.perf &#x3D; performance(pred, measure &#x3D; &quot;auc&quot;)</span><br><span class="line">auc.perf@y.values</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#partial AUC</span><br><span class="line">#we only want to accept a fixed FPR</span><br><span class="line">pauc.perf &#x3D; performance(pred, measure &#x3D; &quot;auc&quot;, fpr.stop&#x3D;0.1)</span><br><span class="line">pauc.perf@y.values</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#standardize this output</span><br><span class="line">pauc.perf@y.values &#x3D; lapply(pauc.perf@y.values, function(x) x &#x2F; 0.1)</span><br><span class="line">pauc.perf@y.values</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">manypauc.perf &#x3D; performance(manypred, measure &#x3D; &quot;auc&quot;, fpr.stop&#x3D;0.1)</span><br><span class="line">manypauc.perf@y.values &#x3D; lapply(manypauc.perf@y.values, function(x) x &#x2F; 0.1)</span><br><span class="line">manypauc.perf@y.values</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#few types of ROCR visualization below</span><br><span class="line">#first one like before, but colorized</span><br><span class="line">data(ROCR.simple)</span><br><span class="line">df &lt;- data.frame(ROCR.simple)</span><br><span class="line">pred &lt;- prediction(df$predictions, df$labels)</span><br><span class="line">perf &lt;- performance(pred,&quot;tpr&quot;,&quot;fpr&quot;)</span><br><span class="line">plot(perf,colorize&#x3D;TRUE)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&quot;pROC&quot;)</span><br><span class="line">library(pROC)</span><br><span class="line">pROC_obj &lt;- roc(df$labels,df$predictions,</span><br><span class="line">            smoothed &#x3D; TRUE,</span><br><span class="line">            ci&#x3D;TRUE, ci.alpha&#x3D;0.9, stratified&#x3D;FALSE,</span><br><span class="line">            plot&#x3D;TRUE, auc.polygon&#x3D;TRUE, max.auc.polygon&#x3D;TRUE, grid&#x3D;TRUE,</span><br><span class="line">            print.auc&#x3D;TRUE, show.thres&#x3D;TRUE)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sens.ci &lt;- ci.se(pROC_obj)</span><br><span class="line">plot(sens.ci, type&#x3D;&quot;shape&quot;, col&#x3D;&quot;lightblue&quot;)</span><br><span class="line"></span><br><span class="line">plot(sens.ci, type&#x3D;&quot;bars&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#another way of plotting the outcome</span><br><span class="line">install.packages(&quot;PRROC&quot;)</span><br><span class="line">library(PRROC)</span><br><span class="line">PRROC_obj &lt;- roc.curve(scores.class0 &#x3D; df$predictions, weights.class0&#x3D;df$labels,</span><br><span class="line">                       curve&#x3D;TRUE)</span><br><span class="line">plot(PRROC_obj)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#with points marked</span><br><span class="line">install.packages(&quot;plotROC&quot;)</span><br><span class="line">library(plotROC)</span><br><span class="line">rocplot &lt;- ggplot(df, aes(m &#x3D; predictions, d &#x3D; labels))+ geom_roc(n.cuts&#x3D;20,labels&#x3D;FALSE)</span><br><span class="line">rocplot + style_roc(theme &#x3D; theme_grey) + geom_rocci(fill&#x3D;&quot;pink&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#ROC, precision, recall</span><br><span class="line">install.packages(&quot;precrec&quot;)</span><br><span class="line">library(precrec)</span><br><span class="line">precrec_obj &lt;- evalmod(scores &#x3D; df$predictions, labels &#x3D; df$labels)</span><br><span class="line">autoplot(precrec_obj)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">precrec_obj2 &lt;- evalmod(scores &#x3D; df$predictions, labels &#x3D; df$labels, mode&#x3D;&quot;basic&quot;)</span><br><span class="line">autoplot(precrec_obj2)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#brand new package</span><br><span class="line">install.packages(&quot;ROCit&quot;)</span><br><span class="line">library(ROCit)</span><br><span class="line">ROCit_obj &lt;- rocit(score&#x3D;df$predictions,class&#x3D;df$labels)</span><br><span class="line">plot(ROCit_obj)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#cumulative densities of the positive and negative responses </span><br><span class="line">#the KS statistic shows the maximum distance between the two curves</span><br><span class="line">ksplot(ROCit_obj)</span><br></pre></td></tr></table></figure>

<h3 id="Estimating-Future-Performance"><a href="#Estimating-Future-Performance" class="headerlink" title="Estimating Future Performance"></a>Estimating Future Performance</h3><ul>
<li>holdout method</li>
<li>cross-validation</li>
<li>bootstrap sampling</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#short example of boostraping</span><br><span class="line">#bootstrap of the ratio of means using the city data included in the boot package</span><br><span class="line">install.packages(&quot;boot&quot;)</span><br><span class="line">library(boot)</span><br><span class="line">#loading the data from the package</span><br><span class="line">data(city)</span><br><span class="line">#define the ratio function</span><br><span class="line">ratio &lt;- function(d, w) sum(d$x * w)&#x2F;sum(d$u * w)</span><br><span class="line">#apply the function and get the output</span><br><span class="line">boot(city, ratio, R&#x3D;999, stype&#x3D;&quot;w&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># another example</span><br><span class="line">install.packages(&quot;tidyverse&quot;)</span><br><span class="line">library(tidyverse)</span><br><span class="line">install.packages(&quot;caret&quot;)</span><br><span class="line">library(caret)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#data predicting fertility score on the basis of socio-economic indicators</span><br><span class="line">#built-in dataset</span><br><span class="line">data(&quot;swiss&quot;)</span><br><span class="line">sample_n(swiss, 3)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#bootstrap with 100 resamples to test a linear regression model</span><br><span class="line">#assign training control</span><br><span class="line">train.control &lt;- trainControl(method &#x3D; &quot;boot&quot;, number &#x3D; 100)</span><br><span class="line">#train the model</span><br><span class="line">model &lt;- train(Fertility ~., data &#x3D; swiss, method &#x3D; &quot;lm&quot;,</span><br><span class="line">               trControl &#x3D; train.control)</span><br><span class="line">#the results</span><br><span class="line">print(model)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#bootstrap approach can be used to quantify the uncertainty (or standard error) </span><br><span class="line">#associated with any given statistical estimator</span><br><span class="line">#the function below returns the regression coefficients</span><br><span class="line">#use boot_fun() to the full data set of 47 observations in order to compute the coefficients</span><br><span class="line">model_coef &lt;- function(data, index)&#123;</span><br><span class="line">  coef(lm(Fertility ~., data &#x3D; data, subset &#x3D; index))</span><br><span class="line">&#125;</span><br><span class="line">model_coef(swiss, 1:47)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#compute the standard errors of 200 bootstrap estimates for the coefficients</span><br><span class="line">boot(swiss, model_coef, 200)</span><br><span class="line">#original column corresponds to the regression coefficients</span><br><span class="line">#associated standard errors are given in the column std.error</span><br><span class="line">#t1 corresponds to the intercept, t2 corresponds to Agriculture...</span><br><span class="line">#the standard error (SE) of the regression coefficient associated with Agriculture is 0.069</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#without bootstraping you get different results</span><br><span class="line">summary(lm(Fertility ~., data &#x3D; swiss))$coef</span><br></pre></td></tr></table></figure>

<h2 id="Lesson-7"><a href="#Lesson-7" class="headerlink" title="Lesson 7"></a>Lesson 7</h2><h3 id="Improving-Model-Performance"><a href="#Improving-Model-Performance" class="headerlink" title="Improving Model Performance"></a>Improving Model Performance</h3><p>Keep variance and bias low! Biasare the simplifying assumptions made by a model to make the target function easier to learn. Low bias suggests less assumptions about the form of the target fuction. High bias suggests more assumptions about the form of the target function. Variance is the amout that the estimate of the target function will change if different training data was used. Low variance suggests small changes to the estimate of the target function with changes to the training dataset. High variance suggests large changes to the estimate of the target fuction with changes to the training dataset.</p>
<p>自动调参和手动调参参考examples。</p>
<h3 id="Customized-Tuning"><a href="#Customized-Tuning" class="headerlink" title="Customized Tuning"></a>Customized Tuning</h3><h3 id="Meta-learning"><a href="#Meta-learning" class="headerlink" title="Meta-learning"></a>Meta-learning</h3><ul>
<li>Ensembles: 三个臭皮匠顶个诸葛亮</li>
<li>Bagging: used with decision trees</li>
<li>Boosting: allows one to increase performance to an arbitrary threshold simply by adding more weak learners</li>
<li>Random forests</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#upload the dataset</span><br><span class="line">#credit data that we have already used</span><br><span class="line">credit &lt;- read.csv(&quot;credit_ger.csv&quot;)</span><br><span class="line">#inspect the dataset</span><br><span class="line">str(credit)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#install relevant packages</span><br><span class="line">install.packages(&quot;caret&quot;)</span><br><span class="line">library(caret)</span><br><span class="line">install.packages(&quot;C50&quot;)</span><br><span class="line">library(C50)</span><br><span class="line">install.packages(&quot;e1071&quot;)</span><br><span class="line">library(e1071)</span><br><span class="line">#simple C5.0 decision tree tuning</span><br><span class="line">#set.seet to initialize the random number generator just to set starting position</span><br><span class="line">#so that the random numbers follow a predefined sequence</span><br><span class="line">#this is fine for simulations that use random sampling and useful for replicating results</span><br><span class="line">#feature of interest is about the type of bank client</span><br><span class="line">set.seed(300)</span><br><span class="line">m &lt;- train(class ~ ., data&#x3D;credit, method &#x3D; &quot;C5.0&quot;)</span><br><span class="line">#this part from above may take quite long</span><br><span class="line">#because of repeatedly generating random samples of data, building decision trees </span><br><span class="line">#computing performance statistics, and evaluating the result</span><br><span class="line">#check the output</span><br><span class="line">m</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#we see the number of samples, predictors and classes of the target variable</span><br><span class="line">#report of preprocessing and resampling methods applied is given</span><br><span class="line">#25 bootstrap samples, each including 1000 examples, were used to train the models</span><br><span class="line"></span><br><span class="line">#below there is a list of candidate models evaluated</span><br><span class="line">#12 different models were tested, based on combinations of three C5.0 tuning parameters: model, trials, and winnow</span><br><span class="line"></span><br><span class="line">#the average and standard deviation (labeled SD) of the accuracy </span><br><span class="line">#and kappa statistics for each candidate model are available</span><br><span class="line"></span><br><span class="line">#how to choose the best model?</span><br><span class="line">#the model with the largest accuracy value was chosen as the best</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#look closer at the output</span><br><span class="line">p &lt;- predict(m, credit)</span><br><span class="line">table(p, credit$class)</span><br><span class="line">#no of misclassified observations is the resubstitution error </span><br><span class="line">#and should not be viewed as indicative of performance on unseen data</span><br><span class="line"></span><br><span class="line">#the bootstrap estimate of 74% (shown in the summary output) </span><br><span class="line">#is a more realistic estimate of model&#39;s future performance</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#obtaining predicted class values</span><br><span class="line">head(predict(m, credit))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#obtaining predicted class probabilities</span><br><span class="line">head(predict(m, credit, type &#x3D; &quot;prob&quot;))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#customized tuning</span><br><span class="line">#check the essential function</span><br><span class="line">?trainControl</span><br><span class="line">#develop an object that uses 10 CV and the oneSE selection function</span><br><span class="line">ctrl &lt;- trainControl(method &#x3D; &quot;cv&quot;, number &#x3D; 10, selectionFunction &#x3D; &quot;oneSE&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">grid &lt;- expand.grid(.model &#x3D; &quot;tree&quot;,</span><br><span class="line">.trials &#x3D; c(1, 5, 10, 15, 20, 25, 30, 35),</span><br><span class="line">.winnow &#x3D; &quot;FALSE&quot;)</span><br><span class="line">#check the output</span><br><span class="line">grid</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">set.seed(300)</span><br><span class="line">m &lt;- train(class ~ ., data &#x3D; credit, method &#x3D; &quot;C5.0&quot;,</span><br><span class="line">metric &#x3D; &quot;Kappa&quot;, trControl &#x3D; ctrl, tuneGrid &#x3D; grid)</span><br><span class="line">#inspect the results</span><br><span class="line">m</span><br></pre></td></tr></table></figure>

<h2 id="Lesson-8"><a href="#Lesson-8" class="headerlink" title="Lesson 8"></a>Lesson 8</h2><p>Codes form lesson 7.</p>
<ol>
<li>bagging</li>
<li>ensemble-average and majority</li>
<li>ensemble-carseats</li>
<li>ensemble-ionosphere</li>
<li>ensemble-superlearner</li>
<li>random forest</li>
</ol>
<p>Plz refer to W8 codes.</p>
<h2 id="Lesson-9-amp-10"><a href="#Lesson-9-amp-10" class="headerlink" title="Lesson 9 &amp; 10"></a>Lesson 9 &amp; 10</h2><h3 id="Text-Mining"><a href="#Text-Mining" class="headerlink" title="Text Mining"></a>Text Mining</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># upload the dataset - Amazon reviews</span><br><span class="line">product_review &lt;- readr::read_csv(&quot;amazon_reviews.csv&quot;)</span><br><span class="line"># inspect the dataset</span><br><span class="line">head(product_review)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&quot;pander&quot;)</span><br><span class="line">library(pander)</span><br><span class="line"># more elegant way </span><br><span class="line">pandoc.table(product_review[2:4,1:3], </span><br><span class="line">             justify &#x3D; c(&#39;left&#39;, &#39;left&#39;, &#39;center&#39;), style &#x3D; &#39;grid&#39;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># how about the table of ratings?</span><br><span class="line">table(product_review$rating)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&quot;dplyr&quot;)</span><br><span class="line">library(dplyr)</span><br><span class="line"></span><br><span class="line"># put a 1 for great reviews (4 or 5) or a 0 for bad reviews (1 or 2)</span><br><span class="line"># remove all the reviews that have a rating of 3 as they might be considered as neutral</span><br><span class="line">product_review &lt;- product_review %&gt;% filter(rating !&#x3D; 3) %&gt;% </span><br><span class="line">                    mutate(rating_new &#x3D; if_else(rating &gt;&#x3D; 4, 1, 0))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># create the training set </span><br><span class="line">product_review_training &lt;-  product_review[1:150, ]</span><br><span class="line"># create our corpus, then tokenize it, then make it back to a data frame</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&quot;tm&quot;)</span><br><span class="line">library(tm)</span><br><span class="line"></span><br><span class="line">corpus_toy &lt;- Corpus(VectorSource(product_review_training$review))</span><br><span class="line">tdm_toy &lt;- DocumentTermMatrix(corpus_toy, list(removePunctuation &#x3D; TRUE, </span><br><span class="line">                                               removeNumbers &#x3D; TRUE))</span><br><span class="line"></span><br><span class="line">training_set_toy &lt;- as.matrix(tdm_toy)</span><br><span class="line"></span><br><span class="line">training_set_toy &lt;- cbind(training_set_toy, product_review_training$rating_new)</span><br><span class="line"></span><br><span class="line">colnames(training_set_toy)[ncol(training_set_toy)] &lt;- &quot;y&quot;</span><br><span class="line"></span><br><span class="line">training_set_toy &lt;- as.data.frame(training_set_toy)</span><br><span class="line">training_set_toy$y &lt;- as.factor(training_set_toy$y)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"># create our model using SVM</span><br><span class="line">install.packages(&quot;caret&quot;)</span><br><span class="line">library(caret)</span><br><span class="line">install.packages(&quot;LiblineaR&quot;)</span><br><span class="line">library(LiblineaR)</span><br><span class="line">install.packages(&quot;e1071&quot;)</span><br><span class="line">library(e1071)</span><br><span class="line">review_toy_model &lt;- train(y ~., data &#x3D; training_set_toy, method &#x3D; &#39;svmLinear3&#39;)</span><br><span class="line"></span><br><span class="line"># run the model</span><br><span class="line">test_review_data &lt;- product_review[151:191, ]</span><br><span class="line"></span><br><span class="line">test_corpus &lt;- Corpus(VectorSource(test_review_data$review))</span><br><span class="line">test_tdm &lt;- DocumentTermMatrix(test_corpus, control&#x3D;list(dictionary &#x3D; Terms(tdm_toy)))</span><br><span class="line">test_tdm &lt;- as.matrix(test_tdm)</span><br><span class="line"></span><br><span class="line"># make the prediction  </span><br><span class="line">model_toy_result &lt;- predict(review_toy_model, newdata &#x3D; test_tdm)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># inspect the output</span><br><span class="line">check_accuracy &lt;- as.data.frame(cbind(prediction &#x3D; model_toy_result, rating &#x3D; test_review_data$rating_new))</span><br><span class="line">check_accuracy &lt;- check_accuracy %&gt;% mutate(prediction &#x3D; as.integer(prediction) - 1)</span><br><span class="line">check_accuracy$accuracy &lt;- if_else(check_accuracy$prediction &#x3D;&#x3D; check_accuracy$rating, 1, 0)</span><br><span class="line">round(prop.table(table(check_accuracy$accuracy)), 3)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&quot;gmodels&quot;)</span><br><span class="line">library(gmodels)</span><br><span class="line">CrossTable(x &#x3D; test_review_data$rating_new, model_toy_result,</span><br><span class="line">           prop.chisq &#x3D; FALSE)</span><br></pre></td></tr></table></figure>

<h3 id="Natural-Language-Processing-NLP"><a href="#Natural-Language-Processing-NLP" class="headerlink" title="Natural Language Processing (NLP)"></a>Natural Language Processing (NLP)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># NLP - Text Parsing, Stemming, Stopword removal, Term Frequency Matrix</span><br><span class="line"></span><br><span class="line">install.packages(&quot;tm&quot;)  # text mining</span><br><span class="line">library(tm)</span><br><span class="line"># for stemming</span><br><span class="line">install.packages(&quot;SnowballC&quot;) # text stemming</span><br><span class="line">library(SnowballC)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># read a file MLK_speech.txt</span><br><span class="line">text &lt;- readLines(&quot;MLK_speech.txt&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">docs &lt;- Corpus(VectorSource(text))</span><br><span class="line"># check the document with inspect()</span><br><span class="line">inspect(docs)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># with a function tm_map () </span><br><span class="line"># replace special characters from text. </span><br><span class="line"># substituting &quot;&#x2F;&quot;, &quot;@&quot; i &quot;|&quot; by a space.</span><br><span class="line"></span><br><span class="line">toSpace &lt;- content_transformer(function (x , pattern ) gsub(pattern, &quot; &quot;, x))</span><br><span class="line">docs &lt;- tm_map(docs, toSpace, &quot;&#x2F;&quot;)</span><br><span class="line">docs &lt;- tm_map(docs, toSpace, &quot;@&quot;)</span><br><span class="line">docs &lt;- tm_map(docs, toSpace, &quot;\\|&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"># The tm_map() function is also used to further clean the text</span><br><span class="line"># a) to remove unnecessary spaces, punctuation and numbers</span><br><span class="line"></span><br><span class="line"># remove unnecessary spaces</span><br><span class="line">docs &lt;- tm_map(docs, stripWhitespace)</span><br><span class="line"></span><br><span class="line"># remove unnecessary punctuation</span><br><span class="line">docs &lt;- tm_map(docs, removePunctuation)</span><br><span class="line"></span><br><span class="line"># remove unnecessary numbers</span><br><span class="line">docs &lt;- tm_map(docs, removeNumbers)</span><br><span class="line"></span><br><span class="line"># b) change letters to lower case</span><br><span class="line"></span><br><span class="line"># change to lowercase</span><br><span class="line">docs &lt;- tm_map(docs, content_transformer(tolower))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># remove English stopwords</span><br><span class="line">docs &lt;- tm_map(docs, removeWords, stopwords(&quot;english&quot;))</span><br><span class="line"></span><br><span class="line"># if necessaary: remove your own stopwords - as a vector of words:</span><br><span class="line">docs &lt;- tm_map(docs, removeWords, c(&quot;blabla1&quot;, &quot;blabla2&quot;))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># stem document</span><br><span class="line">docs &lt;- tm_map(docs, stemDocument)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># Matrix is a table containing the frequency of words.</span><br><span class="line"># The TermDocumentMatrix () function is used in the following way:</span><br><span class="line"></span><br><span class="line">dtm &lt;- TermDocumentMatrix(docs)</span><br><span class="line">m &lt;- as.matrix(dtm)</span><br><span class="line">v &lt;- sort(rowSums(m),decreasing&#x3D;TRUE)</span><br><span class="line">d &lt;- data.frame(word &#x3D; names(v),freq&#x3D;v)</span><br><span class="line"></span><br><span class="line">d</span><br><span class="line"></span><br><span class="line">head(d, 10)</span><br></pre></td></tr></table></figure>

<h3 id="Text-Categorization"><a href="#Text-Categorization" class="headerlink" title="Text Categorization"></a>Text Categorization</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&quot;dplyr&quot;)</span><br><span class="line">library(dplyr)</span><br><span class="line">install.packages(&quot;tidytext&quot;)</span><br><span class="line">library(tidytext)</span><br><span class="line">install.packages(&quot;janeaustenr&quot;)</span><br><span class="line">library(janeaustenr)</span><br><span class="line"></span><br><span class="line">austen_bigrams &lt;- austen_books() %&gt;%</span><br><span class="line">  unnest_tokens(bigram, text, token &#x3D; &quot;ngrams&quot;, n &#x3D; 2)</span><br><span class="line"></span><br><span class="line">austen_bigrams</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># data structure is still a variation of the tidy text format</span><br><span class="line"># each token now represents a bigram</span><br><span class="line"># examine the most common bigrams using dplyr’s count()</span><br><span class="line">austen_bigrams %&gt;%</span><br><span class="line">  count(bigram, sort &#x3D; TRUE)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># a lot of the most common bigrams are pairs of common (uninteresting) words</span><br><span class="line"># use tidyr’s separate(), which splits a column into multiple based on a delimiter</span><br><span class="line"></span><br><span class="line"># lets us separate it into two columns, “word1” and “word2”, </span><br><span class="line"># at which point we can remove cases where either is a stop-word</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&quot;tidyr&quot;)</span><br><span class="line">library(tidyr)</span><br><span class="line"></span><br><span class="line">bigrams_separated &lt;- austen_bigrams %&gt;%</span><br><span class="line">  separate(bigram, c(&quot;word1&quot;, &quot;word2&quot;), sep &#x3D; &quot; &quot;)</span><br><span class="line"></span><br><span class="line">bigrams_filtered &lt;- bigrams_separated %&gt;%</span><br><span class="line">  filter(!word1 %in% stop_words$word) %&gt;%</span><br><span class="line">  filter(!word2 %in% stop_words$word)</span><br><span class="line"></span><br><span class="line"># new bigram counts:</span><br><span class="line">bigram_counts &lt;- bigrams_filtered %&gt;% </span><br><span class="line">  count(word1, word2, sort &#x3D; TRUE)</span><br><span class="line"></span><br><span class="line">bigram_counts</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># it appears that names (whether first and last or with a salutation) are the most common pairs in Jane Austen books</span><br><span class="line"># “separate&#x2F;filter&#x2F;count&#x2F;unite” let us find the most common bigrams not containing stop-words</span><br><span class="line">bigrams_united &lt;- bigrams_filtered %&gt;%</span><br><span class="line">  unite(bigram, word1, word2, sep &#x3D; &quot; &quot;)</span><br><span class="line"></span><br><span class="line">bigrams_united</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># how about trigrams, which are consecutive sequences of 3 words?</span><br><span class="line">austen_books() %&gt;%</span><br><span class="line">  unnest_tokens(trigram, text, token &#x3D; &quot;ngrams&quot;, n &#x3D; 3) %&gt;%</span><br><span class="line">  separate(trigram, c(&quot;word1&quot;, &quot;word2&quot;, &quot;word3&quot;), sep &#x3D; &quot; &quot;) %&gt;%</span><br><span class="line">  filter(!word1 %in% stop_words$word,</span><br><span class="line">         !word2 %in% stop_words$word,</span><br><span class="line">         !word3 %in% stop_words$word) %&gt;%</span><br><span class="line">  count(word1, word2, word3, sort &#x3D; TRUE)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># bigram analysis</span><br><span class="line"># the most common “streets” mentioned in each book</span><br><span class="line">bigrams_filtered %&gt;%</span><br><span class="line">  filter(word2 &#x3D;&#x3D; &quot;street&quot;) %&gt;%</span><br><span class="line">  count(book, word1, sort &#x3D; TRUE)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># a bigram can also be treated as a term in a document in the same way that we treated individual words</span><br><span class="line"># tf-idf values can be visualized within each book, just as we did for words</span><br><span class="line">bigram_tf_idf &lt;- bigrams_united %&gt;%</span><br><span class="line">  count(book, bigram) %&gt;%</span><br><span class="line">  bind_tf_idf(bigram, book, n) %&gt;%</span><br><span class="line">  arrange(desc(tf_idf))</span><br><span class="line"></span><br><span class="line">bigram_tf_idf</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># using bigrams to provide context in sentiment analysis</span><br><span class="line"></span><br><span class="line"># the words “happy” and “like” will be counted as positive, </span><br><span class="line"># even in a sentence like “I’m not happy and I don’t like it!”</span><br><span class="line"></span><br><span class="line"># how often words are preceded by a word like “not”</span><br><span class="line">bigrams_separated %&gt;%</span><br><span class="line">  filter(word1 &#x3D;&#x3D; &quot;not&quot;) %&gt;%</span><br><span class="line">  count(word1, word2, sort &#x3D; TRUE)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># sentiment analysis with the AFINN </span><br><span class="line">AFINN &lt;- get_sentiments(&quot;afinn&quot;)</span><br><span class="line">AFINN</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># examine the most frequent words that were preceded by “not” and were associated with a sentiment</span><br><span class="line">not_words &lt;- bigrams_separated %&gt;%</span><br><span class="line">  filter(word1 &#x3D;&#x3D; &quot;not&quot;) %&gt;%</span><br><span class="line">  inner_join(AFINN, by &#x3D; c(word2 &#x3D; &quot;word&quot;)) %&gt;%</span><br><span class="line">  count(word2, score, sort &#x3D; TRUE)</span><br><span class="line"></span><br><span class="line">not_words</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&quot;ggplot2&quot;)</span><br><span class="line">library(ggplot2)</span><br><span class="line">not_words %&gt;%</span><br><span class="line">  mutate(contribution &#x3D; n * score) %&gt;%</span><br><span class="line">  arrange(desc(abs(contribution))) %&gt;%</span><br><span class="line">  head(20) %&gt;%</span><br><span class="line">  mutate(word2 &#x3D; reorder(word2, contribution)) %&gt;%</span><br><span class="line">  ggplot(aes(word2, n * score, fill &#x3D; n * score &gt; 0)) +</span><br><span class="line">  geom_col(show.legend &#x3D; FALSE) +</span><br><span class="line">  xlab(&quot;Words preceded by \&quot;not\&quot;&quot;) +</span><br><span class="line">  ylab(&quot;Sentiment value * number of occurrences&quot;) +</span><br><span class="line">  coord_flip()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&quot;igraph&quot;)</span><br><span class="line">library(igraph)</span><br><span class="line"># original counts</span><br><span class="line">bigram_counts</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># filter for only relatively common combinations</span><br><span class="line">bigram_graph &lt;- bigram_counts %&gt;%</span><br><span class="line">  filter(n &gt; 20) %&gt;%</span><br><span class="line">  graph_from_data_frame()</span><br><span class="line"></span><br><span class="line">bigram_graph</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># counting and correlating pairs of words with the widyr package</span><br><span class="line"># words that tend to co-occur within particular documents or particular chapters, </span><br><span class="line"># even if they don’t occur next to each other</span><br><span class="line"></span><br><span class="line"># what words tend to appear within the same section</span><br><span class="line"># Pride and Prejudice book</span><br><span class="line">austen_section_words &lt;- austen_books() %&gt;%</span><br><span class="line">  filter(book &#x3D;&#x3D; &quot;Pride &amp; Prejudice&quot;) %&gt;%</span><br><span class="line">  mutate(section &#x3D; row_number() %&#x2F;% 10) %&gt;%</span><br><span class="line">  filter(section &gt; 0) %&gt;%</span><br><span class="line">  unnest_tokens(word, text) %&gt;%</span><br><span class="line">  filter(!word %in% stop_words$word)</span><br><span class="line"></span><br><span class="line">austen_section_words</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&quot;widyr&quot;)</span><br><span class="line">library(widyr)</span><br><span class="line"># count words co-occuring within sections</span><br><span class="line">word_pairs &lt;- austen_section_words %&gt;%</span><br><span class="line">  pairwise_count(word, section, sort &#x3D; TRUE)</span><br><span class="line"></span><br><span class="line">word_pairs</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># words that most often occur with Darcy</span><br><span class="line">word_pairs %&gt;%</span><br><span class="line">  filter(item1 &#x3D;&#x3D; &quot;darcy&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># we need to filter for at least relatively common words first</span><br><span class="line">word_cors &lt;- austen_section_words %&gt;%</span><br><span class="line">  group_by(word) %&gt;%</span><br><span class="line">  filter(n() &gt;&#x3D; 20) %&gt;%</span><br><span class="line">  pairwise_cor(word, section, sort &#x3D; TRUE)</span><br><span class="line"></span><br><span class="line">word_cors</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># find words most correlated with a word like “pounds” using a filter operation</span><br><span class="line">word_cors %&gt;%</span><br><span class="line">  filter(item1 &#x3D;&#x3D; &quot;pounds&quot;)</span><br><span class="line"></span><br><span class="line"># words and find the other words most associated with them</span><br><span class="line">word_cors %&gt;%</span><br><span class="line">  filter(item1 %in% c(&quot;elizabeth&quot;, &quot;pounds&quot;, &quot;married&quot;, &quot;pride&quot;)) %&gt;%</span><br><span class="line">  group_by(item1) %&gt;%</span><br><span class="line">  top_n(6) %&gt;%</span><br><span class="line">  ungroup() %&gt;%</span><br><span class="line">  mutate(item2 &#x3D; reorder(item2, correlation)) %&gt;%</span><br><span class="line">  ggplot(aes(item2, correlation)) +</span><br><span class="line">  geom_bar(stat &#x3D; &quot;identity&quot;) +</span><br><span class="line">  facet_wrap(~ item1, scales &#x3D; &quot;free&quot;) +</span><br><span class="line">  coord_flip()</span><br></pre></td></tr></table></figure>

<h3 id="Topic-Modeling"><a href="#Topic-Modeling" class="headerlink" title="Topic Modeling"></a>Topic Modeling</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"># use the movie review data: http:&#x2F;&#x2F;www.cs.cornell.edu&#x2F;people&#x2F;pabo&#x2F;movie-review-data&#x2F;</span><br><span class="line">install.packages(&quot;LDAvis&quot;)</span><br><span class="line">library(LDAvis)</span><br><span class="line">devtools::install_github(&quot;cpsievert&#x2F;LDAvisData&quot;)</span><br><span class="line">data(reviews, package &#x3D; &quot;LDAvisData&quot;)</span><br><span class="line">install.packages(&quot;tm&quot;)</span><br><span class="line">library(tm)</span><br><span class="line"></span><br><span class="line"># read in some stopwords</span><br><span class="line">stop_words &lt;- stopwords(&quot;SMART&quot;)</span><br><span class="line"></span><br><span class="line"># pre-processing</span><br><span class="line">reviews &lt;- gsub(&quot;&#39;&quot;, &quot;&quot;, reviews)  # remove apostrophes</span><br><span class="line">reviews &lt;- gsub(&quot;[[:punct:]]&quot;, &quot; &quot;, reviews)  # replace punctuation with space</span><br><span class="line">reviews &lt;- gsub(&quot;[[:cntrl:]]&quot;, &quot; &quot;, reviews)  # replace control characters with space</span><br><span class="line">reviews &lt;- gsub(&quot;^[[:space:]]+&quot;, &quot;&quot;, reviews) # remove whitespace at beginning of documents</span><br><span class="line">reviews &lt;- gsub(&quot;[[:space:]]+$&quot;, &quot;&quot;, reviews) # remove whitespace at end of documents</span><br><span class="line">reviews &lt;- tolower(reviews)  # force to lowercase</span><br><span class="line"></span><br><span class="line"># tokenize on space and output as a list</span><br><span class="line">doc.list &lt;- strsplit(reviews, &quot;[[:space:]]+&quot;)</span><br><span class="line"></span><br><span class="line"># compute the table of terms</span><br><span class="line">term.table &lt;- table(unlist(doc.list))</span><br><span class="line">term.table &lt;- sort(term.table, decreasing &#x3D; TRUE)</span><br><span class="line"></span><br><span class="line"># remove terms that are stop words or occur fewer than 5 times</span><br><span class="line">del &lt;- names(term.table) %in% stop_words | term.table &lt; 5</span><br><span class="line">term.table &lt;- term.table[!del]</span><br><span class="line">vocab &lt;- names(term.table)</span><br><span class="line"></span><br><span class="line"># now put the documents into the format required by the lda package</span><br><span class="line">get.terms &lt;- function(x) &#123;</span><br><span class="line">  index &lt;- match(x, vocab)</span><br><span class="line">  index &lt;- index[!is.na(index)]</span><br><span class="line">  rbind(as.integer(index - 1), as.integer(rep(1, length(index))))</span><br><span class="line">&#125;</span><br><span class="line">documents &lt;- lapply(doc.list, get.terms)</span><br><span class="line"></span><br><span class="line"># model fitting</span><br><span class="line"># Compute some statistics related to the data set:</span><br><span class="line">D &lt;- length(documents)  # number of documents (2,000)</span><br><span class="line">W &lt;- length(vocab)  # number of terms in the vocab (14,568)</span><br><span class="line">doc.length &lt;- sapply(documents, function(x) sum(x[2, ]))  # number of tokens per document [312, 288, 170, 436, 291, ...]</span><br><span class="line">N &lt;- sum(doc.length)  # total number of tokens in the data (546,827)</span><br><span class="line">term.frequency &lt;- as.integer(term.table)  # frequencies of terms in the corpus [8939, 5544, 2411, 2410, 2143, ...]</span><br><span class="line"></span><br><span class="line">install.packages(&quot;lda&quot;)</span><br><span class="line">library(lda)</span><br><span class="line"></span><br><span class="line"># MCMC and model tuning parameters:</span><br><span class="line">K &lt;- 20</span><br><span class="line">G &lt;- 1000</span><br><span class="line">alpha &lt;- 0.02</span><br><span class="line">eta &lt;- 0.02</span><br><span class="line"></span><br><span class="line"># Fit the model:</span><br><span class="line">set.seed(357)</span><br><span class="line">t1 &lt;- Sys.time()</span><br><span class="line">fit &lt;- lda.collapsed.gibbs.sampler(documents &#x3D; documents, K &#x3D; K, vocab &#x3D; vocab, </span><br><span class="line">                                   num.iterations &#x3D; G, alpha &#x3D; alpha, </span><br><span class="line">                                   eta &#x3D; eta, initial &#x3D; NULL, burnin &#x3D; 0,</span><br><span class="line">                                   compute.log.likelihood &#x3D; TRUE)</span><br><span class="line">t2 &lt;- Sys.time()</span><br><span class="line">t2 - t1 </span><br><span class="line"></span><br><span class="line"># visualize the fitted model</span><br><span class="line"># we need estimates of the document-topic distributions</span><br><span class="line"># we denote by the D×K matrix θ, and the set of topic-term distributions, which we denote by the K×W matrix ϕ</span><br><span class="line"></span><br><span class="line"># estimate the “smoothed” versions of these distributions (“smoothed” means </span><br><span class="line"># that we&#39;ve incorporated the effects of the priors into the estimates)</span><br><span class="line"># by cross-tabulating the latent topic assignments from the last iteration </span><br><span class="line"># of the collapsed Gibbs sampler with the documents and the terms respectively, </span><br><span class="line"># and then adding pseudocounts according to the priors</span><br><span class="line"></span><br><span class="line">theta &lt;- t(apply(fit$document_sums + alpha, 2, function(x) x&#x2F;sum(x)))</span><br><span class="line">phi &lt;- t(apply(t(fit$topics) + eta, 2, function(x) x&#x2F;sum(x)))</span><br><span class="line"></span><br><span class="line"># we have already computed the number of tokens per document and the frequency of the terms across the entire corpus</span><br><span class="line">MovieReviews &lt;- list(phi &#x3D; phi,</span><br><span class="line">                     theta &#x3D; theta,</span><br><span class="line">                     doc.length &#x3D; doc.length,</span><br><span class="line">                     vocab &#x3D; vocab,</span><br><span class="line">                     term.frequency &#x3D; term.frequency)</span><br><span class="line"></span><br><span class="line"># create the JSON object to feed the visualization:</span><br><span class="line">json &lt;- createJSON(phi &#x3D; MovieReviews$phi, </span><br><span class="line">                   theta &#x3D; MovieReviews$theta, </span><br><span class="line">                   doc.length &#x3D; MovieReviews$doc.length, </span><br><span class="line">                   vocab &#x3D; MovieReviews$vocab, </span><br><span class="line">                   term.frequency &#x3D; MovieReviews$term.frequency)</span><br><span class="line"></span><br><span class="line">install.packages(&quot;servr&quot;) </span><br><span class="line">library(servr)</span><br><span class="line">serVis(json, out.dir &#x3D; &#39;visual&#39;, open.browser &#x3D; FALSE)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br></pre></td><td class="code"><pre><span class="line"># Latent Dirichlet allocation (LDA) is a particularly popular method for fitting a topic model. It treats each document as a mixture of topics, and each topic as a mixture of words. LDA allows documents to “overlap” each other in terms of content, rather than being separated into discrete groups.</span><br><span class="line"></span><br><span class="line">library(tidyverse)</span><br><span class="line">library(tidytext)</span><br><span class="line">library(topicmodels)</span><br><span class="line">library(tm)</span><br><span class="line">library(SnowballC)</span><br><span class="line">reviews &lt;- read_csv(&quot;deceptive-opinion.csv&quot;)</span><br><span class="line"></span><br><span class="line">top_terms_by_topic_LDA &lt;- function(input_text, # should be a columm from a dataframe</span><br><span class="line">                                   plot &#x3D; T, # return a plot? TRUE by defult</span><br><span class="line">                                   number_of_topics &#x3D; 4) # number of topics (4 by default)</span><br><span class="line">&#123;    </span><br><span class="line">  # create a corpus (type of object expected by tm) and document term matrix</span><br><span class="line">  Corpus &lt;- Corpus(VectorSource(input_text)) # make a corpus object</span><br><span class="line">  DTM &lt;- DocumentTermMatrix(Corpus) # get the count of words&#x2F;document</span><br><span class="line">  </span><br><span class="line">  # remove any empty rows in our document term matrix (if there are any </span><br><span class="line">  # we&#39;ll get an error when we try to run our LDA)</span><br><span class="line">  unique_indexes &lt;- unique(DTM$i) # get the index of each unique value</span><br><span class="line">  DTM &lt;- DTM[unique_indexes,] # get a subset of only those indexes</span><br><span class="line">  </span><br><span class="line">  # preform LDA &amp; get the words&#x2F;topic in a tidy text format</span><br><span class="line">  lda &lt;- LDA(DTM, k &#x3D; number_of_topics, control &#x3D; list(seed &#x3D; 1234))</span><br><span class="line">  topics &lt;- tidy(lda, matrix &#x3D; &quot;beta&quot;)</span><br><span class="line">  </span><br><span class="line">  # get the top ten terms for each topic</span><br><span class="line">  top_terms &lt;- topics  %&gt;% # take the topics data frame and..</span><br><span class="line">    group_by(topic) %&gt;% # treat each topic as a different group</span><br><span class="line">    top_n(10, beta) %&gt;% # get the top 10 most informative words</span><br><span class="line">    ungroup() %&gt;% # ungroup</span><br><span class="line">    arrange(topic, -beta) # arrange words in descending informativeness</span><br><span class="line">  </span><br><span class="line">  # if the user asks for a plot (TRUE by default)</span><br><span class="line">  if(plot &#x3D;&#x3D; T)&#123;</span><br><span class="line">    # plot the top ten terms for each topic in order</span><br><span class="line">    top_terms %&gt;% # take the top terms</span><br><span class="line">      mutate(term &#x3D; reorder(term, beta)) %&gt;% # sort terms by beta value </span><br><span class="line">      ggplot(aes(term, beta, fill &#x3D; factor(topic))) + # plot beta by theme</span><br><span class="line">      geom_col(show.legend &#x3D; FALSE) + # as a bar plot</span><br><span class="line">      facet_wrap(~ topic, scales &#x3D; &quot;free&quot;) + # which each topic in a seperate plot</span><br><span class="line">      labs(x &#x3D; NULL, y &#x3D; &quot;Beta&quot;) + # no x label, change y label </span><br><span class="line">      coord_flip() # turn bars sideways</span><br><span class="line">  &#125;else&#123; </span><br><span class="line">    # if the user does not request a plot</span><br><span class="line">    # return a list of sorted terms instead</span><br><span class="line">    return(top_terms)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># plot top ten terms in the hotel reviews by topic</span><br><span class="line">top_terms_by_topic_LDA(reviews$text, number_of_topics &#x3D; 2)</span><br><span class="line"></span><br><span class="line"># create a document term matrix to clean</span><br><span class="line">reviewsCorpus &lt;- Corpus(VectorSource(reviews$text)) </span><br><span class="line">reviewsDTM &lt;- DocumentTermMatrix(reviewsCorpus)</span><br><span class="line"></span><br><span class="line"># convert the document term matrix to a tidytext corpus</span><br><span class="line">reviewsDTM_tidy &lt;- tidy(reviewsDTM)</span><br><span class="line"></span><br><span class="line"># I&#39;m going to add my own custom stop words that I don&#39;t think will be</span><br><span class="line"># very informative in hotel reviews</span><br><span class="line">custom_stop_words &lt;- tibble(word &#x3D; c(&quot;hotel&quot;, &quot;room&quot;))</span><br><span class="line"></span><br><span class="line"># remove stopwords</span><br><span class="line">reviewsDTM_tidy_cleaned &lt;- reviewsDTM_tidy %&gt;% # take our tidy dtm and...</span><br><span class="line">  anti_join(stop_words, by &#x3D; c(&quot;term&quot; &#x3D; &quot;word&quot;)) %&gt;% # remove English stopwords and...</span><br><span class="line">  anti_join(custom_stop_words, by &#x3D; c(&quot;term&quot; &#x3D; &quot;word&quot;)) # remove my custom stopwords</span><br><span class="line"></span><br><span class="line"># reconstruct cleaned documents (so that each word shows up the correct number of times)</span><br><span class="line">cleaned_documents &lt;- reviewsDTM_tidy_cleaned %&gt;%</span><br><span class="line">  group_by(document) %&gt;% </span><br><span class="line">  mutate(terms &#x3D; toString(rep(term, count))) %&gt;%</span><br><span class="line">  select(document, terms) %&gt;%</span><br><span class="line">  unique()</span><br><span class="line"></span><br><span class="line"># check out what the cleaned documents look like (should just be a bunch of content words)</span><br><span class="line"># in alphabetic order</span><br><span class="line">head(cleaned_documents)</span><br><span class="line"></span><br><span class="line"># now let&#39;s look at the new most informative terms</span><br><span class="line">top_terms_by_topic_LDA(cleaned_documents$terms, number_of_topics &#x3D; 2)</span><br><span class="line"></span><br><span class="line"># stem the words (e.g. convert each word to its stem, where applicable)</span><br><span class="line">reviewsDTM_tidy_cleaned &lt;- reviewsDTM_tidy_cleaned %&gt;% </span><br><span class="line">  mutate(stem &#x3D; wordStem(term))</span><br><span class="line"></span><br><span class="line"># reconstruct our documents</span><br><span class="line">cleaned_documents &lt;- reviewsDTM_tidy_cleaned %&gt;%</span><br><span class="line">  group_by(document) %&gt;% </span><br><span class="line">  mutate(terms &#x3D; toString(rep(stem, count))) %&gt;%</span><br><span class="line">  select(document, terms) %&gt;%</span><br><span class="line">  unique()</span><br><span class="line"></span><br><span class="line"># now let&#39;s look at the new most informative terms</span><br><span class="line">top_terms_by_topic_LDA(cleaned_documents$terms, number_of_topics &#x3D; 2)</span><br><span class="line"></span><br><span class="line">top_terms_by_topic_tfidf &lt;- function(text_df, text_column, group_column, plot &#x3D; T)&#123;</span><br><span class="line">  # name for the column we&#39;re going to unnest_tokens_ to</span><br><span class="line">  # (you only need to worry about enquo stuff if you&#39;re</span><br><span class="line">  # writing a function using using tidyverse packages)</span><br><span class="line">  group_column &lt;- enquo(group_column)</span><br><span class="line">  text_column &lt;- enquo(text_column)</span><br><span class="line">  </span><br><span class="line">  # get the count of each word in each review</span><br><span class="line">  words &lt;- text_df %&gt;%</span><br><span class="line">    unnest_tokens(word, !!text_column) %&gt;%</span><br><span class="line">    count(!!group_column, word) %&gt;% </span><br><span class="line">    ungroup()</span><br><span class="line">  </span><br><span class="line">  # get the number of words per text</span><br><span class="line">  total_words &lt;- words %&gt;% </span><br><span class="line">    group_by(!!group_column) %&gt;% </span><br><span class="line">    summarize(total &#x3D; sum(n))</span><br><span class="line">  </span><br><span class="line">  # combine the two dataframes we just made</span><br><span class="line">  words &lt;- left_join(words, total_words)</span><br><span class="line">  </span><br><span class="line">  # get the tf_idf &amp; order the words by degree of relevence</span><br><span class="line">  tf_idf &lt;- words %&gt;%</span><br><span class="line">    bind_tf_idf(word, !!group_column, n) %&gt;%</span><br><span class="line">    select(-total) %&gt;%</span><br><span class="line">    arrange(desc(tf_idf)) %&gt;%</span><br><span class="line">    mutate(word &#x3D; factor(word, levels &#x3D; rev(unique(word))))</span><br><span class="line">  </span><br><span class="line">  if(plot &#x3D;&#x3D; T)&#123;</span><br><span class="line">    # convert &quot;group&quot; into a quote of a name</span><br><span class="line">    # (this is due to funkiness with calling ggplot2</span><br><span class="line">    # in functions)</span><br><span class="line">    group_name &lt;- quo_name(group_column)</span><br><span class="line">    </span><br><span class="line">    # plot the 10 most informative terms per topic</span><br><span class="line">    tf_idf %&gt;% </span><br><span class="line">      group_by(!!group_column) %&gt;% </span><br><span class="line">      top_n(10) %&gt;% </span><br><span class="line">      ungroup %&gt;%</span><br><span class="line">      ggplot(aes(word, tf_idf, fill &#x3D; as.factor(group_name))) +</span><br><span class="line">      geom_col(show.legend &#x3D; FALSE) +</span><br><span class="line">      labs(x &#x3D; NULL, y &#x3D; &quot;tf-idf&quot;) +</span><br><span class="line">      facet_wrap(reformulate(group_name), scales &#x3D; &quot;free&quot;) +</span><br><span class="line">      coord_flip()</span><br><span class="line">  &#125;else&#123;</span><br><span class="line">    # return the entire tf_idf dataframe</span><br><span class="line">    return(tf_idf)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># let&#39;s see what our most informative deceptive words are</span><br><span class="line">top_terms_by_topic_tfidf(text_df &#x3D; reviews, # dataframe</span><br><span class="line">                         text_column &#x3D; text, # column with text</span><br><span class="line">                         group_column &#x3D; deceptive, # column with topic label</span><br><span class="line">                         plot &#x3D; T) # return a plot</span><br><span class="line"></span><br><span class="line"># look for the most informative words for postive and negative reveiws</span><br><span class="line">top_terms_by_topic_tfidf(text_df &#x3D; reviews, </span><br><span class="line">                         text_column &#x3D; text, </span><br><span class="line">                         group &#x3D; polarity, </span><br><span class="line">                         plot &#x3D; T)</span><br><span class="line"></span><br><span class="line"># check out the reviews for each hotel individually</span><br><span class="line"># get just the tf-idf output for the hotel topics</span><br><span class="line">reviews_tfidf_byHotel &lt;- top_terms_by_topic_tfidf(text_df &#x3D; reviews, </span><br><span class="line">                                                  text_column &#x3D; text, </span><br><span class="line">                                                  group &#x3D; hotel, </span><br><span class="line">                                                  plot &#x3D; F)</span><br><span class="line"></span><br><span class="line"># do our own plotting</span><br><span class="line">reviews_tfidf_byHotel  %&gt;% </span><br><span class="line">  group_by(hotel) %&gt;% </span><br><span class="line">  top_n(5) %&gt;% </span><br><span class="line">  ungroup %&gt;%</span><br><span class="line">  ggplot(aes(word, tf_idf, fill &#x3D; hotel)) +</span><br><span class="line">  geom_col(show.legend &#x3D; FALSE) +</span><br><span class="line">  labs(x &#x3D; NULL, y &#x3D; &quot;tf-idf&quot;) +</span><br><span class="line">  facet_wrap(~hotel, ncol &#x3D; 4, scales &#x3D; &quot;free&quot;, ) +</span><br><span class="line">  coord_flip()</span><br></pre></td></tr></table></figure>

<h3 id="Sentiment-Analysis"><a href="#Sentiment-Analysis" class="headerlink" title="Sentiment Analysis"></a>Sentiment Analysis</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&quot;tidytext&quot;)</span><br><span class="line">library(tidytext)</span><br><span class="line">get_sentiments(&quot;afinn&quot;)</span><br><span class="line">get_sentiments(&quot;bing&quot;)</span><br><span class="line">get_sentiments(&quot;nrc&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&quot;janeaustenr&quot;)</span><br><span class="line">library(janeaustenr)</span><br><span class="line">install.packages(&quot;dplyr&quot;)</span><br><span class="line">library(dplyr)</span><br><span class="line">install.packages(&quot;stringr&quot;)</span><br><span class="line">library(stringr)</span><br><span class="line"></span><br><span class="line">tidy_books &lt;- austen_books() %&gt;%</span><br><span class="line">  group_by(book) %&gt;%</span><br><span class="line">  mutate(linenumber &#x3D; row_number(),</span><br><span class="line">         chapter &#x3D; cumsum(str_detect(text, regex(&quot;^chapter [\\divxlc]&quot;, </span><br><span class="line">                                                 ignore_case &#x3D; TRUE)))) %&gt;%</span><br><span class="line">  ungroup() %&gt;%</span><br><span class="line">  unnest_tokens(word, text)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"># name word for the output column from unnest_tokens()</span><br><span class="line"># sentiment lexicons and stop word datasets have columns named word; </span><br><span class="line"># performing inner joins and anti-joins is thus easier</span><br><span class="line"></span><br><span class="line"># use the NRC lexicon and filter() for the joy words</span><br><span class="line"># filter() the data frame with the text from the books for the words from Emma and then use inner_join()</span><br><span class="line"></span><br><span class="line"># what are the most common joy words in Emma?</span><br><span class="line">nrc_joy &lt;- get_sentiments(&quot;nrc&quot;) %&gt;% </span><br><span class="line">  filter(sentiment &#x3D;&#x3D; &quot;joy&quot;)</span><br><span class="line"></span><br><span class="line">tidy_books %&gt;%</span><br><span class="line">  filter(book &#x3D;&#x3D; &quot;Emma&quot;) %&gt;%</span><br><span class="line">  inner_join(nrc_joy) %&gt;%</span><br><span class="line">  count(word, sort &#x3D; TRUE)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&quot;tidyr&quot;)</span><br><span class="line">library(tidyr)</span><br><span class="line">jane_austen_sentiment &lt;- tidy_books %&gt;%</span><br><span class="line">  inner_join(get_sentiments(&quot;bing&quot;)) %&gt;%</span><br><span class="line">  count(book, index &#x3D; linenumber %&#x2F;% 80, sentiment) %&gt;%</span><br><span class="line">  spread(sentiment, n, fill &#x3D; 0) %&gt;%</span><br><span class="line">  mutate(sentiment &#x3D; positive - negative)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&quot;ggplot2&quot;)</span><br><span class="line">library(ggplot2)</span><br><span class="line">ggplot(jane_austen_sentiment, aes(index, sentiment, fill &#x3D; book)) +</span><br><span class="line">  geom_col(show.legend &#x3D; FALSE) +</span><br><span class="line">  facet_wrap(~book, ncol &#x3D; 2, scales &#x3D; &quot;free_x&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># comparing the three dictionaries</span><br><span class="line">pride_prejudice &lt;- tidy_books %&gt;% </span><br><span class="line">  filter(book &#x3D;&#x3D; &quot;Pride &amp; Prejudice&quot;)</span><br><span class="line"></span><br><span class="line">pride_prejudice</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"># use inner_join() to calculate the sentiment in different ways</span><br><span class="line"># count(), spread(), and mutate() to find the net sentiment in each of these sections of text</span><br><span class="line">afinn &lt;- pride_prejudice %&gt;% </span><br><span class="line">  inner_join(get_sentiments(&quot;afinn&quot;)) %&gt;% </span><br><span class="line">  group_by(index &#x3D; linenumber %&#x2F;% 80) %&gt;% </span><br><span class="line">  summarise(sentiment &#x3D; sum(score)) %&gt;% </span><br><span class="line">  mutate(method &#x3D; &quot;AFINN&quot;)</span><br><span class="line"></span><br><span class="line">bing_and_nrc &lt;- bind_rows(pride_prejudice %&gt;% </span><br><span class="line">                            inner_join(get_sentiments(&quot;bing&quot;)) %&gt;%</span><br><span class="line">                            mutate(method &#x3D; &quot;Bing et al.&quot;),</span><br><span class="line">                          pride_prejudice %&gt;% </span><br><span class="line">                            inner_join(get_sentiments(&quot;nrc&quot;) %&gt;% </span><br><span class="line">                                         filter(sentiment %in% c(&quot;positive&quot;, </span><br><span class="line">                                                                 &quot;negative&quot;))) %&gt;%</span><br><span class="line">                            mutate(method &#x3D; &quot;NRC&quot;)) %&gt;%</span><br><span class="line">  count(method, index &#x3D; linenumber %&#x2F;% 80, sentiment) %&gt;%</span><br><span class="line">  spread(sentiment, n, fill &#x3D; 0) %&gt;%</span><br><span class="line">  mutate(sentiment &#x3D; positive - negative)</span><br><span class="line"></span><br><span class="line"># bind them together and visualize</span><br><span class="line">bind_rows(afinn, </span><br><span class="line">          bing_and_nrc) %&gt;%</span><br><span class="line">  ggplot(aes(index, sentiment, fill &#x3D; method)) +</span><br><span class="line">  geom_col(show.legend &#x3D; FALSE) +</span><br><span class="line">  facet_wrap(~method, ncol &#x3D; 1, scales &#x3D; &quot;free_y&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># common positive and negative words</span><br><span class="line">bing_word_counts &lt;- tidy_books %&gt;%</span><br><span class="line">  inner_join(get_sentiments(&quot;bing&quot;)) %&gt;%</span><br><span class="line">  count(word, sentiment, sort &#x3D; TRUE) %&gt;%</span><br><span class="line">  ungroup()</span><br><span class="line"></span><br><span class="line">bing_word_counts</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">bing_word_counts %&gt;%</span><br><span class="line">  group_by(sentiment) %&gt;%</span><br><span class="line">  top_n(10) %&gt;%</span><br><span class="line">  ungroup() %&gt;%</span><br><span class="line">  mutate(word &#x3D; reorder(word, n)) %&gt;%</span><br><span class="line">  ggplot(aes(word, n, fill &#x3D; sentiment)) +</span><br><span class="line">  geom_col(show.legend &#x3D; FALSE) +</span><br><span class="line">  facet_wrap(~sentiment, scales &#x3D; &quot;free_y&quot;) +</span><br><span class="line">  labs(y &#x3D; &quot;Contribution to sentiment&quot;,</span><br><span class="line">       x &#x3D; NULL) +</span><br><span class="line">  coord_flip()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># custom changes: we could easily add “miss” to a custom stop-words list using bind_rows()</span><br><span class="line">custom_stop_words &lt;- bind_rows(tibble(word &#x3D; c(&quot;miss&quot;), </span><br><span class="line">                                          lexicon &#x3D; c(&quot;custom&quot;)), </span><br><span class="line">                               stop_words)</span><br><span class="line"></span><br><span class="line">custom_stop_words</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># simple wordclouds</span><br><span class="line">install.packages(&quot;wordcloud&quot;)</span><br><span class="line">library(wordcloud)</span><br><span class="line">tidy_books %&gt;%</span><br><span class="line">  anti_join(stop_words) %&gt;%</span><br><span class="line">  count(word) %&gt;%</span><br><span class="line">  with(wordcloud(word, n, max.words &#x3D; 100))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># wordcloud with sentiment</span><br><span class="line">install.packages(&quot;reshape2&quot;)</span><br><span class="line">library(reshape2)</span><br><span class="line">tidy_books %&gt;%</span><br><span class="line">  inner_join(get_sentiments(&quot;bing&quot;)) %&gt;%</span><br><span class="line">  count(word, sentiment, sort &#x3D; TRUE) %&gt;%</span><br><span class="line">  acast(word ~ sentiment, value.var &#x3D; &quot;n&quot;, fill &#x3D; 0) %&gt;%</span><br><span class="line">  comparison.cloud(colors &#x3D; c(&quot;gray20&quot;, &quot;gray80&quot;),</span><br><span class="line">                   max.words &#x3D; 100)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># inspecting sentiment of sentences</span><br><span class="line">PandP_sentences &lt;- tibble(text &#x3D; prideprejudice) %&gt;% </span><br><span class="line">  unnest_tokens(sentence, text, token &#x3D; &quot;sentences&quot;)</span><br><span class="line"></span><br><span class="line"># take a look at one</span><br><span class="line">PandP_sentences$sentence[2]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"># coding matters: UTF-8 or latin1</span><br><span class="line"># split into tokens using a regex pattern</span><br><span class="line">austen_chapters &lt;- austen_books() %&gt;%</span><br><span class="line">  group_by(book) %&gt;%</span><br><span class="line">  unnest_tokens(chapter, text, token &#x3D; &quot;regex&quot;, </span><br><span class="line">                pattern &#x3D; &quot;Chapter|CHAPTER [\\dIVXLC]&quot;) %&gt;%</span><br><span class="line">  ungroup()</span><br><span class="line"></span><br><span class="line">austen_chapters %&gt;% </span><br><span class="line">  group_by(book) %&gt;% </span><br><span class="line">  summarise(chapters &#x3D; n())</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">bingnegative &lt;- get_sentiments(&quot;bing&quot;) %&gt;% </span><br><span class="line">  filter(sentiment &#x3D;&#x3D; &quot;negative&quot;)</span><br><span class="line"></span><br><span class="line">wordcounts &lt;- tidy_books %&gt;%</span><br><span class="line">  group_by(book, chapter) %&gt;%</span><br><span class="line">  summarize(words &#x3D; n())</span><br><span class="line"></span><br><span class="line">tidy_books %&gt;%</span><br><span class="line">  semi_join(bingnegative) %&gt;%</span><br><span class="line">  group_by(book, chapter) %&gt;%</span><br><span class="line">  summarize(negativewords &#x3D; n()) %&gt;%</span><br><span class="line">  left_join(wordcounts, by &#x3D; c(&quot;book&quot;, &quot;chapter&quot;)) %&gt;%</span><br><span class="line">  mutate(ratio &#x3D; negativewords&#x2F;words) %&gt;%</span><br><span class="line">  filter(chapter !&#x3D; 0) %&gt;%</span><br><span class="line">  top_n(1) %&gt;%</span><br><span class="line">  ungroup()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"># tokenization example by Hvitfeldt and Silge (2020) [selected parts]</span><br><span class="line"></span><br><span class="line">install.packages(&quot;tokenizers&quot;)</span><br><span class="line">library(tokenizers)</span><br><span class="line">install.packages(&quot;tidyverse&quot;)</span><br><span class="line">library(tidyverse)</span><br><span class="line">install.packages(&quot;tidytext&quot;)</span><br><span class="line">library(tidytext)</span><br><span class="line">install.packages(&quot;hcandersenr&quot;)</span><br><span class="line">library(hcandersenr)</span><br><span class="line"></span><br><span class="line"># books by Hans Christian Andersen, available in the hcandersenr package (Hvitfeldt 2019a)</span><br><span class="line"># as we look at the first paragraph of one story titled “The Fir Tree”, </span><br><span class="line"># we find the text of the story is in a character vector: </span><br><span class="line"># a series of letters, spaces, and punctuation stored as a vector</span><br><span class="line"></span><br><span class="line">the_fir_tree &lt;- hcandersen_en %&gt;%</span><br><span class="line">  filter(book &#x3D;&#x3D; &quot;The fir tree&quot;) %&gt;%</span><br><span class="line">  pull(text)</span><br><span class="line"></span><br><span class="line">head(the_fir_tree, 9)</span><br><span class="line"></span><br><span class="line">strsplit(the_fir_tree[1:2], &quot;[^a-zA-Z0-9]+&quot;)</span><br><span class="line"># the tokenizers package contains a wealth of fast, consistent tokenizers we can use</span><br><span class="line">tokenize_words(the_fir_tree[1:2])</span><br><span class="line"></span><br><span class="line"># we can generalize the idea of a token beyond only a single word to other units of text</span><br><span class="line"># think of characters, words, sentences, lines, paragraphs, n-grams</span><br><span class="line"></span><br><span class="line"># same tokenization can also be done using the tidytext</span><br><span class="line">sample_vector &lt;- c(</span><br><span class="line">  &quot;This is the first of two strings&quot;,</span><br><span class="line">  &quot;And here is the second string.&quot;</span><br><span class="line">)</span><br><span class="line">sample_tibble &lt;- tibble(text &#x3D; sample_vector)</span><br><span class="line"></span><br><span class="line">tokenize_words(sample_vector)</span><br><span class="line"></span><br><span class="line"># another way of doing pretty the same</span><br><span class="line"># the difference is about the data structure</span><br><span class="line">sample_tibble %&gt;%</span><br><span class="line">  unnest_tokens(word, text, token &#x3D; &quot;words&quot;)</span><br><span class="line"></span><br><span class="line"># the unnest tokens way</span><br><span class="line">sample_tibble %&gt;%</span><br><span class="line">  unnest_tokens(word, text, token &#x3D; &quot;words&quot;, strip_punct &#x3D; FALSE)</span><br><span class="line"></span><br><span class="line"># character tokenization splits texts into character</span><br><span class="line"># the function below has arguments to convert to lowercase and to strip all non-alphanumeric characters</span><br><span class="line"># the defaults will reduce the number of different tokens that are returned</span><br><span class="line"># tokenize_*() functions by default return a list of character vectors, </span><br><span class="line"># one character vector for each string in the input</span><br><span class="line"></span><br><span class="line">tft_token_characters &lt;- tokenize_characters(</span><br><span class="line">  x &#x3D; the_fir_tree,</span><br><span class="line">  lowercase &#x3D; TRUE,</span><br><span class="line">  strip_non_alphanum &#x3D; TRUE,</span><br><span class="line">  simplify &#x3D; FALSE</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">head(tft_token_characters) %&gt;%</span><br><span class="line">  glimpse()</span><br><span class="line"></span><br><span class="line"># word tokenization - most popular</span><br><span class="line"># lowercase &#x3D; TRUE, and strip_punct &#x3D; TRUE and strip_numeric &#x3D; FALSE </span><br><span class="line"># control whether we remove punctuation and numeric characters respectively</span><br><span class="line"></span><br><span class="line">tft_token_words &lt;- tokenize_words(</span><br><span class="line">  x &#x3D; the_fir_tree,</span><br><span class="line">  lowercase &#x3D; TRUE,</span><br><span class="line">  stopwords &#x3D; NULL,</span><br><span class="line">  strip_punct &#x3D; TRUE,</span><br><span class="line">  strip_numeric &#x3D; FALSE</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">head(tft_token_words) %&gt;%</span><br><span class="line">  glimpse()</span><br><span class="line"></span><br><span class="line"># let’s create a tibble with two fairy tales, “The Fir Tree” and “The Little Mermaid”</span><br><span class="line"># find the most commonly used words in each</span><br><span class="line">hcandersen_en %&gt;%</span><br><span class="line">  filter(book %in% c(&quot;The fir tree&quot;, &quot;The little mermaid&quot;)) %&gt;%</span><br><span class="line">  unnest_tokens(word, text) %&gt;%</span><br><span class="line">  count(book, word) %&gt;%</span><br><span class="line">  group_by(book) %&gt;%</span><br><span class="line">  arrange(desc(n)) %&gt;%</span><br><span class="line">  slice(1:5)</span><br><span class="line"></span><br><span class="line"># tokenizers to split text into larger units of text like lines, sentences, </span><br><span class="line"># and paragraphs are rarely used directly for modeling purposes, </span><br><span class="line"># as the tokens produced tend to be fairly unique</span><br><span class="line"></span><br><span class="line"># on the other hand, these tokenizers are useful for preprocessing and labeling</span><br><span class="line"># an n-gram (sometimes written “ngram”) is a term in linguistics for a contiguous sequence </span><br><span class="line"># of n items from a given sequence of text or speech</span><br><span class="line"></span><br><span class="line"># the item can be phonemes, syllables, letters, or words depending on the application, but when most people talk about n-grams, </span><br><span class="line"># they mean a group of n words</span><br><span class="line"></span><br><span class="line"># we use Latin prefixes, such that a 1-gram is called a unigram, 2-gram is called a bigram, </span><br><span class="line"># 3-gram called a trigram and so on</span><br><span class="line"></span><br><span class="line"># the advantage of using n-grams compared to words is that we can capture word order which would otherwise be lost</span><br><span class="line"></span><br><span class="line"># when we use character n-grams, we can model the beginning and end of words, because a space will be located </span><br><span class="line"># at the end of an n-gram for the end of a word and at the beginning of an n-gram of the beginning of a word</span><br><span class="line"></span><br><span class="line"># using n &#x3D; 1 returns unigrams, n &#x3D; 2 bigrams, n &#x3D; 3 gives trigrams, and so on</span><br><span class="line"># related to n is the n_min argument, which specifies the minimum number of n-grams to include</span><br><span class="line"># by default both n and n_min are set to 3 making tokenize_ngrams() return only trigrams</span><br><span class="line"># by setting n &#x3D; 3 and n_min &#x3D; 1, we will get all unigrams, bigrams, and trigrams of a tex</span><br><span class="line"># we have the ngram_delim argument, which specifies the separator between words in the n-grams </span><br><span class="line"># notice that this defaults to a space</span><br><span class="line"></span><br><span class="line">tft_token_ngram &lt;- tokenize_ngrams(</span><br><span class="line">  x &#x3D; the_fir_tree,</span><br><span class="line">  lowercase &#x3D; TRUE,</span><br><span class="line">  n &#x3D; 3L,</span><br><span class="line">  n_min &#x3D; 3L,</span><br><span class="line">  stopwords &#x3D; character(),</span><br><span class="line">  ngram_delim &#x3D; &quot; &quot;,</span><br><span class="line">  simplify &#x3D; FALSE</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"># the output </span><br><span class="line">tft_token_ngram[[1]]</span><br><span class="line"></span><br><span class="line"># n-gram tokenization slides along the text to create overlapping sets of tokens</span><br><span class="line"># important to choose the right value for n when using n-grams for the question we want to answer</span><br><span class="line"></span><br><span class="line"># using unigrams is faster and more efficient, but we don’t capture information about word order</span><br><span class="line"># using a higher value for n keeps more information, but the vector space of tokens increases dramatically, </span><br><span class="line"># corresponding to a reduction in token counts</span><br><span class="line"># a sensible starting point in most cases is three</span><br><span class="line"># if you don’t have a large vocabulary in your dataset, consider starting at two instead three</span><br></pre></td></tr></table></figure>

<h3 id="Knowledge-Representation"><a href="#Knowledge-Representation" class="headerlink" title="Knowledge Representation"></a>Knowledge Representation</h3><h3 id="Information-Extraction"><a href="#Information-Extraction" class="headerlink" title="Information Extraction"></a>Information Extraction</h3><h3 id="Visualization"><a href="#Visualization" class="headerlink" title="Visualization"></a>Visualization</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&quot;wordcloud&quot;)</span><br><span class="line">library(wordcloud)</span><br><span class="line"></span><br><span class="line">install.packages(&quot;RColorBrewer&quot;)</span><br><span class="line">library(RColorBrewer)</span><br><span class="line"></span><br><span class="line">conventions &lt;- read.table(&quot;conventions.csv&quot;, header &#x3D; TRUE, sep &#x3D; &quot;,&quot;)</span><br><span class="line"></span><br><span class="line"># 1. First word cloud </span><br><span class="line"></span><br><span class="line">png(&quot;dnc.png&quot;)</span><br><span class="line">wordcloud(conventions$wordper25k, # words</span><br><span class="line">          conventions$democrats, # frequencies</span><br><span class="line">          scale &#x3D; c(4,1), # size of largest and smallest words</span><br><span class="line">          colors &#x3D; brewer.pal(9,&quot;Blues&quot;), # number of colors, palette</span><br><span class="line">          rot.per &#x3D; 0) # proportion of words to rotate 90 degrees</span><br><span class="line">dev.off()</span><br><span class="line"></span><br><span class="line"># 2. Second word cloud</span><br><span class="line"></span><br><span class="line">png(&quot;rnc.png&quot;)</span><br><span class="line">wordcloud(conventions$wordper25k,</span><br><span class="line">          conventions$republicans,</span><br><span class="line">          scale &#x3D; c(4,1),</span><br><span class="line">          colors &#x3D; brewer.pal(9,&quot;Reds&quot;),</span><br><span class="line">          rot.per &#x3D; 0)</span><br><span class="line">dev.off()</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line">install.packages(&quot;tm&quot;)</span><br><span class="line">library(&quot;tm&quot;)</span><br><span class="line">install.packages(&quot;SnowballC&quot;)</span><br><span class="line">library(&quot;SnowballC&quot;)</span><br><span class="line">install.packages(&quot;wordcloud&quot;)</span><br><span class="line">library(&quot;wordcloud&quot;)</span><br><span class="line">install.packages(&quot;RColorBrewer&quot;)</span><br><span class="line">library(&quot;RColorBrewer&quot;)</span><br><span class="line">install.packages(&quot;XML&quot;)</span><br><span class="line">library(&quot;XML&quot;)</span><br><span class="line">install.packages(&quot;RCurl&quot;)</span><br><span class="line">library(&quot;RCurl&quot;)</span><br><span class="line"></span><br><span class="line"># the function</span><br><span class="line">rquery.wordcloud &lt;- function(x, type&#x3D;c(&quot;text&quot;, &quot;url&quot;, &quot;file&quot;), </span><br><span class="line">                          lang&#x3D;&quot;english&quot;, excludeWords&#x3D;NULL, </span><br><span class="line">                          textStemming&#x3D;FALSE,  colorPalette&#x3D;&quot;Dark2&quot;,</span><br><span class="line">                          min.freq&#x3D;3, max.words&#x3D;200)</span><br><span class="line">&#123; </span><br><span class="line">  library(&quot;tm&quot;)</span><br><span class="line">  library(&quot;SnowballC&quot;)</span><br><span class="line">  library(&quot;wordcloud&quot;)</span><br><span class="line">  library(&quot;RColorBrewer&quot;) </span><br><span class="line">  </span><br><span class="line">  if(type[1]&#x3D;&#x3D;&quot;file&quot;) text &lt;- readLines(x)</span><br><span class="line">  else if(type[1]&#x3D;&#x3D;&quot;url&quot;) text &lt;- html_to_text(x)</span><br><span class="line">  else if(type[1]&#x3D;&#x3D;&quot;text&quot;) text &lt;- x</span><br><span class="line">  </span><br><span class="line">  # Load the text as a corpus</span><br><span class="line">  docs &lt;- Corpus(VectorSource(text))</span><br><span class="line">  # Convert the text to lower case</span><br><span class="line">  docs &lt;- tm_map(docs, content_transformer(tolower))</span><br><span class="line">  # Remove numbers</span><br><span class="line">  docs &lt;- tm_map(docs, removeNumbers)</span><br><span class="line">  # Remove stopwords for the language </span><br><span class="line">  docs &lt;- tm_map(docs, removeWords, stopwords(lang))</span><br><span class="line">  # Remove punctuations</span><br><span class="line">  docs &lt;- tm_map(docs, removePunctuation)</span><br><span class="line">  # Eliminate extra white spaces</span><br><span class="line">  docs &lt;- tm_map(docs, stripWhitespace)</span><br><span class="line">  # Remove your own stopwords</span><br><span class="line">  if(!is.null(excludeWords)) </span><br><span class="line">    docs &lt;- tm_map(docs, removeWords, excludeWords) </span><br><span class="line">  # Text stemming</span><br><span class="line">  if(textStemming) docs &lt;- tm_map(docs, stemDocument)</span><br><span class="line">  # Create term-document matrix</span><br><span class="line">  tdm &lt;- TermDocumentMatrix(docs)</span><br><span class="line">  m &lt;- as.matrix(tdm)</span><br><span class="line">  v &lt;- sort(rowSums(m),decreasing&#x3D;TRUE)</span><br><span class="line">  d &lt;- data.frame(word &#x3D; names(v),freq&#x3D;v)</span><br><span class="line">  # check the color palette name </span><br><span class="line">  if(!colorPalette %in% rownames(brewer.pal.info)) colors &#x3D; colorPalette</span><br><span class="line">  else colors &#x3D; brewer.pal(8, colorPalette) </span><br><span class="line">  # Plot the word cloud</span><br><span class="line">  set.seed(1234)</span><br><span class="line">  wordcloud(d$word,d$freq, min.freq&#x3D;min.freq, max.words&#x3D;max.words,</span><br><span class="line">            random.order&#x3D;FALSE, rot.per&#x3D;0.35, </span><br><span class="line">            use.r.layout&#x3D;FALSE, colors&#x3D;colors)</span><br><span class="line">  </span><br><span class="line">  invisible(list(tdm&#x3D;tdm, freqTable &#x3D; d))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">source(&#39;http:&#x2F;&#x2F;www.sthda.com&#x2F;upload&#x2F;rquery_wordcloud.r&#39;)</span><br><span class="line">filePath &lt;- &quot;http:&#x2F;&#x2F;www.sthda.com&#x2F;sthda&#x2F;RDoc&#x2F;example-files&#x2F;martin-luther-king-i-have-a-dream-speech.txt&quot;</span><br><span class="line">res&lt;-rquery.wordcloud(filePath, type &#x3D;&quot;file&quot;, lang &#x3D; &quot;english&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># how about more words?</span><br><span class="line">res&lt;-rquery.wordcloud(filePath, type &#x3D;&quot;file&quot;, lang &#x3D; &quot;english&quot;,</span><br><span class="line">                 min.freq &#x3D; 1,  max.words &#x3D; 200)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># play with colors</span><br><span class="line"># Reds color palette</span><br><span class="line">res&lt;-rquery.wordcloud(filePath, type &#x3D;&quot;file&quot;, lang &#x3D; &quot;english&quot;,</span><br><span class="line">                      colorPalette &#x3D; &quot;Reds&quot;)</span><br><span class="line"># RdBu color palette</span><br><span class="line">res&lt;-rquery.wordcloud(filePath, type &#x3D;&quot;file&quot;, lang &#x3D; &quot;english&quot;,</span><br><span class="line">                      colorPalette &#x3D; &quot;RdBu&quot;)</span><br><span class="line"># use unique color</span><br><span class="line">res&lt;-rquery.wordcloud(filePath, type &#x3D;&quot;file&quot;, lang &#x3D; &quot;english&quot;,</span><br><span class="line">                      colorPalette &#x3D; &quot;black&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># inspect the numerical results</span><br><span class="line">tdm &lt;- res$tdm</span><br><span class="line">freqTable &lt;- res$freqTable</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># Show the top10 words and their frequency</span><br><span class="line">head(freqTable, 10)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Bar plot of the frequency for the top10</span><br><span class="line">barplot(freqTable[1:10,]$freq, las &#x3D; 2, </span><br><span class="line">        names.arg &#x3D; freqTable[1:10,]$word,</span><br><span class="line">        col &#x3D;&quot;lightblue&quot;, main &#x3D;&quot;Most frequent words&quot;,</span><br><span class="line">        ylab &#x3D; &quot;Word frequencies&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># words that appear at least 5 times</span><br><span class="line">findFreqTerms(tdm, lowfreq &#x3D; 5)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># words associated with &#39;dream&#39;</span><br><span class="line"># simple correlation</span><br><span class="line">findAssocs(tdm, terms &#x3D; &quot;dream&quot;, corlimit &#x3D; 0.4)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># direct wordcloud of a webpage</span><br><span class="line">url &#x3D; &quot;https:&#x2F;&#x2F;en.wikipedia.org&#x2F;wiki&#x2F;Singapore&quot;</span><br><span class="line">rquery.wordcloud(x&#x3D;url, type&#x3D;&quot;url&quot;)</span><br></pre></td></tr></table></figure>
    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>sasegoi
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://sasegoi.github.io.git/2020/04/05/Machine-Learning-Lecture-Review-2/" title="机器学习课程复习2">https://sasegoi.github.io.git/2020/04/05/Machine-Learning-Lecture-Review-2/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/04/04/Machine-Learning-Lecture-Review/" rel="prev" title="机器学习课程复习1">
      <i class="fa fa-chevron-left"></i> 机器学习课程复习1
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#机器学习课程复习2"><span class="nav-number">1.</span> <span class="nav-text">机器学习课程复习2</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Lesson-5"><span class="nav-number">1.1.</span> <span class="nav-text">Lesson 5</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Neural-Networks"><span class="nav-number">1.1.1.</span> <span class="nav-text">Neural Networks</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Black-box-method"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">Black box method</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#How-ANN-works"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">How ANN works?</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Regression-Methods"><span class="nav-number">1.1.2.</span> <span class="nav-text">Regression Methods</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Regression-trees-and-model-trees"><span class="nav-number">1.1.2.1.</span> <span class="nav-text">Regression trees and model trees</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lesson-6"><span class="nav-number">1.2.</span> <span class="nav-text">Lesson 6</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Measuring-Model-Performance"><span class="nav-number">1.2.1.</span> <span class="nav-text">Measuring Model Performance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Visualizing-Performance-Tradeoffs"><span class="nav-number">1.2.2.</span> <span class="nav-text">Visualizing Performance Tradeoffs</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#ROC-curve"><span class="nav-number">1.2.2.1.</span> <span class="nav-text">ROC curve</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Estimating-Future-Performance"><span class="nav-number">1.2.3.</span> <span class="nav-text">Estimating Future Performance</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lesson-7"><span class="nav-number">1.3.</span> <span class="nav-text">Lesson 7</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Improving-Model-Performance"><span class="nav-number">1.3.1.</span> <span class="nav-text">Improving Model Performance</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Customized-Tuning"><span class="nav-number">1.3.2.</span> <span class="nav-text">Customized Tuning</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Meta-learning"><span class="nav-number">1.3.3.</span> <span class="nav-text">Meta-learning</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lesson-8"><span class="nav-number">1.4.</span> <span class="nav-text">Lesson 8</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lesson-9-amp-10"><span class="nav-number">1.5.</span> <span class="nav-text">Lesson 9 &amp; 10</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Text-Mining"><span class="nav-number">1.5.1.</span> <span class="nav-text">Text Mining</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Natural-Language-Processing-NLP"><span class="nav-number">1.5.2.</span> <span class="nav-text">Natural Language Processing (NLP)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Text-Categorization"><span class="nav-number">1.5.3.</span> <span class="nav-text">Text Categorization</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Topic-Modeling"><span class="nav-number">1.5.4.</span> <span class="nav-text">Topic Modeling</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Sentiment-Analysis"><span class="nav-number">1.5.5.</span> <span class="nav-text">Sentiment Analysis</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Knowledge-Representation"><span class="nav-number">1.5.6.</span> <span class="nav-text">Knowledge Representation</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Information-Extraction"><span class="nav-number">1.5.7.</span> <span class="nav-text">Information Extraction</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Visualization"><span class="nav-number">1.5.8.</span> <span class="nav-text">Visualization</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">sasegoi</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">null </a>
  </div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">sasegoi</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">88k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">1:20</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->













  

  

  

</body>
</html>
