<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"sasegoi.github.io.git","root":"/","scheme":"Gemini","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="机器学习课程复习1Lesson 1Machine Learning Approaches Supervised Learning predictive models classification KNN, naive Bayes, decision trees, linear regressions, neural networks, SVM">
<meta property="og:type" content="article">
<meta property="og:title" content="机器学习课程复习1">
<meta property="og:url" content="https://sasegoi.github.io.git/2020/04/04/Machine-Learning-Lecture-Review/index.html">
<meta property="og:site_name" content="Sasegoi&#39;s Blog">
<meta property="og:description" content="机器学习课程复习1Lesson 1Machine Learning Approaches Supervised Learning predictive models classification KNN, naive Bayes, decision trees, linear regressions, neural networks, SVM">
<meta property="og:locale" content="zh_CN">
<meta property="article:published_time" content="2020-04-04T10:38:48.000Z">
<meta property="article:modified_time" content="2020-04-05T00:03:28.786Z">
<meta property="article:author" content="sasegoi">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://sasegoi.github.io.git/2020/04/04/Machine-Learning-Lecture-Review/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>机器学习课程复习1 | Sasegoi's Blog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="Sasegoi's Blog" type="application/atom+xml"><!-- hexo-inject:begin --><!-- hexo-inject:end -->
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Sasegoi's Blog</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
      <p class="site-subtitle" itemprop="description">如人饮水，冷暖自知</p>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>首页</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-fw fa-user"></i>关于</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-fw fa-tags"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-fw fa-th"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>归档</a>

  </li>
        <li class="menu-item menu-item-sitemap">

    <a href="/sitemap.xml" rel="section"><i class="fa fa-fw fa-sitemap"></i>站点地图</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://sasegoi.github.io.git/2020/04/04/Machine-Learning-Lecture-Review/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="sasegoi">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sasegoi's Blog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          机器学习课程复习1
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2020-04-04 18:38:48" itemprop="dateCreated datePublished" datetime="2020-04-04T18:38:48+08:00">2020-04-04</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2020-04-05 08:03:28" itemprop="dateModified" datetime="2020-04-05T08:03:28+08:00">2020-04-05</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Machine-Learning/" itemprop="url" rel="index"><span itemprop="name">Machine Learning</span></a>
                </span>
            </span>

          <br>
            <span class="post-meta-item" title="本文字数">
              <span class="post-meta-item-icon">
                <i class="fa fa-file-word-o"></i>
              </span>
                <span class="post-meta-item-text">本文字数：</span>
              <span>30k</span>
            </span>
            <span class="post-meta-item" title="阅读时长">
              <span class="post-meta-item-icon">
                <i class="fa fa-clock-o"></i>
              </span>
                <span class="post-meta-item-text">阅读时长 &asymp;</span>
              <span>27 分钟</span>
            </span>

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h1 id="机器学习课程复习1"><a href="#机器学习课程复习1" class="headerlink" title="机器学习课程复习1"></a>机器学习课程复习1</h1><h2 id="Lesson-1"><a href="#Lesson-1" class="headerlink" title="Lesson 1"></a>Lesson 1</h2><h3 id="Machine-Learning-Approaches"><a href="#Machine-Learning-Approaches" class="headerlink" title="Machine Learning Approaches"></a>Machine Learning Approaches</h3><ul>
<li>Supervised Learning<ul>
<li>predictive models</li>
<li>classification</li>
<li>KNN, naive Bayes, decision trees, linear regressions, neural networks, SVM <a id="more"></a></li>
</ul>
</li>
<li>Unsupervised Learning<ul>
<li>descriptive models</li>
<li>no labels upfront</li>
<li>clustering, association rules, dimension reduction</li>
</ul>
</li>
</ul>
<p>监督式学习：能够通过训练样本集或专家知识构建已知且确定的判定函数，并根据训练集和该判定函数形成模型改进策略，对模型参数进行不断改进，完成模型学习的过程称为监督式学习；如果无法从训练样本集或专家知识构建确定的判定函数，而通过训练集与一给定的判定函数进行模型参数不断改进，完成学习的过程称为非监督式学习。</p>
<h2 id="Lesson-2"><a href="#Lesson-2" class="headerlink" title="Lesson 2"></a>Lesson 2</h2><h3 id="K-Nearest-Neighbours"><a href="#K-Nearest-Neighbours" class="headerlink" title="K-Nearest Neighbours"></a>K-Nearest Neighbours</h3><ul>
<li>Classification using nearest neighbors<ul>
<li>Classifying unlabeled samples<ul>
<li>assign them the class of the most similar labeled examples</li>
</ul>
</li>
<li>Applications<ul>
<li>facial recognition, recommendation systems, detecting diseases</li>
</ul>
</li>
<li>Why lazy?<ul>
<li>there is no abstraction</li>
<li>rote learning, instance-based learning, non-parametric learning</li>
</ul>
</li>
<li>Adequate for<ul>
<li>numerous and sophisticated patterns</li>
<li>homogenous classes (inter/intraclass (dis)similarity)</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">uciurl &lt;- &quot;https:&#x2F;&#x2F;archive.ics.uci.edu&#x2F;ml&#x2F;machine-learning-databases&#x2F;breast-cancer-wisconsin&#x2F;wdbc.data&quot;</span><br><span class="line">download.file(url&#x3D;uciurl, destfile&#x3D;&quot;wdbc.data&quot;, method&#x3D;&quot;curl&quot;)</span><br><span class="line">wdbc &lt;- read.csv(&quot;wdbc.data&quot;, header&#x3D;FALSE, stringsAsFactors&#x3D;FALSE)[-1]</span><br><span class="line">wdbc &lt;- wdbc[sample(nrow(wdbc)),]</span><br><span class="line">features &lt;- c(&quot;radius&quot;, &quot;texture&quot;, &quot;perimeter&quot;, &quot;area&quot;, &quot;smoothness&quot;, </span><br><span class="line">              &quot;compactness&quot;, &quot;concavity&quot;, &quot;concave_points&quot;, &quot;symmetry&quot;,</span><br><span class="line">              &quot;fractal_dimension&quot;)</span><br><span class="line">calcs &lt;- c(&quot;mean&quot;, &quot;se&quot;, &quot;worst&quot;)</span><br><span class="line">colnames(wdbc) &lt;- c(&quot;diagnosis&quot;,</span><br><span class="line">                    paste0(rep(features, 3), &quot;_&quot;, rep(calcs, each&#x3D;10)))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#inspect the dataset</span><br><span class="line">names(wdbc)</span><br><span class="line">str(wdbc)</span><br><span class="line">table(wdbc$diagnosis)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#assign labels and recode the diagnosis variable</span><br><span class="line">wdbc$diagnosis &lt;- factor(wdbc$diagnosis,levels &#x3D; c(&quot;B&quot;,&quot;M&quot;),</span><br><span class="line">                         labels&#x3D;c(&quot;Beningn&quot;,&quot;Malinagnt&quot;))</span><br><span class="line">round(prop.table(table(wdbc$diagnosis))*100,digits &#x3D; 1)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#see the summary table</span><br><span class="line">summary(wdbc)</span><br><span class="line">summary(wdbc$area_mean)</span><br><span class="line">summary(wdbc$smoothness_mean)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#see the range of various variables</span><br><span class="line">#as for now, the impact of area would be much larger than the smoothness in the distance calculation</span><br><span class="line">#we have to normalize the data</span><br><span class="line">normalize &lt;- function(x)&#123;</span><br><span class="line">  return((x-min(x))&#x2F;(max(x)-min(x)))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#example of normalization</span><br><span class="line">normalize(c(1,2,3,4,5))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#lapply function os for taking a list and applying a specified function to each list element</span><br><span class="line">#data frame is a list of equal-length vectors</span><br><span class="line">#let&#39;s use lapply to apply normalization to each feature in the data frame</span><br><span class="line">#convert the output to a data frame thanks to as.data.frame function</span><br><span class="line">wdbc_n &lt;- as.data.frame(lapply(wdbc[2:31],normalize))</span><br><span class="line"></span><br><span class="line">#check the transformation effect</span><br><span class="line">summary(wdbc_n$area_mean)</span><br><span class="line"></span><br><span class="line">#now the feature ranges from 0 to 1</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#divide data into training and test datasets</span><br><span class="line">#training dataset is used to build the kNN model</span><br><span class="line">#test dataset is used to estimate the predictive accuracy of the model</span><br><span class="line">wdbc_train &lt;- wdbc_n[1:469,]</span><br><span class="line">wdbc_test &lt;- wdbc_n[470:569,]</span><br><span class="line"></span><br><span class="line">#remember that the diagnosis is our target variable</span><br><span class="line">#let&#39;s store class labels in factor vectors across our datasets</span><br><span class="line">wdbc_train_labels &lt;- wdbc[1:469,1]</span><br><span class="line">wdbc_test_labels &lt;- wdbc[470:569,1]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#now we start training our model on the data applied</span><br><span class="line">#we use kNN from the class package</span><br><span class="line">library(class)</span><br><span class="line"></span><br><span class="line">#Euclidean distance is a baseline choice</span><br><span class="line">#we have to specify the k number</span><br><span class="line">#let&#39;s try with k&#x3D;21 as it is close to the square root of 469</span><br><span class="line">wdbc_test_pred &lt;- knn(train &#x3D; wdbc_train, test &#x3D; wdbc_test, </span><br><span class="line">                      cl &#x3D; wdbc_train_labels, k&#x3D;21)</span><br><span class="line"></span><br><span class="line">#evaluate model performance</span><br><span class="line">#how well the predicted classes match up with the known values?</span><br><span class="line">install.packages(&quot;gmodels&quot;)</span><br><span class="line">library(gmodels)</span><br><span class="line">CrossTable(x&#x3D;wdbc_test_labels, y&#x3D;wdbc_test_pred,</span><br><span class="line">           prop.chisq &#x3D; FALSE)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wdbc_z &lt;- as.data.frame(scale(wdbc[-1]))</span><br><span class="line">summary(wdbc_z$area_mean)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#the mean after the z-score stand should always be 0</span><br><span class="line">#z-score values larger than 3 or less than -3 suggest extremely rare value</span><br><span class="line">#try to do the prediction once again</span><br><span class="line">wdbc_train &lt;- wdbc_z[1:469,]</span><br><span class="line">wdbc_test &lt;- wdbc_z[470:569,]</span><br><span class="line">wdbc_train_labels &lt;- wdbc[1:469,1]</span><br><span class="line">wdbc_test_labels &lt;- wdbc[470:569,1]</span><br><span class="line">wdbc_test_pred &lt;- knn(train &#x3D; wdbc_train, test &#x3D; wdbc_test,</span><br><span class="line">                      cl &#x3D; wdbc_train_labels, k&#x3D;21)</span><br><span class="line">CrossTable(x &#x3D; wdbc_test_labels, wdbc_test_pred,</span><br><span class="line">           prop.chisq &#x3D; FALSE)</span><br></pre></td></tr></table></figure>

<h3 id="Distance-Measures"><a href="#Distance-Measures" class="headerlink" title="Distance Measures"></a>Distance Measures</h3><ul>
<li>definition: fuction of similarity between objects</li>
<li>for kNN</li>
<li><a href="https://www.sciencedirect.com/topics/computer-science/minkowski-distance" target="_blank" rel="noopener">sciencedirect</a></li>
<li>distance measures <ul>
<li>Euclidean</li>
<li>Manhattan</li>
<li>Minkowski</li>
</ul>
</li>
</ul>
<h3 id="Appropriate-k"><a href="#Appropriate-k" class="headerlink" title="Appropriate k"></a>Appropriate k</h3><ul>
<li>under/overfitting, bias-variance tradeoff</li>
<li>large k<ul>
<li>less impact due to noisy data, may ignore small patterns</li>
</ul>
</li>
<li>depends strongly on the data</li>
<li>square root</li>
<li>various options</li>
</ul>
<h3 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h3><h4 id="Preparing-data-for-kNN"><a href="#Preparing-data-for-kNN" class="headerlink" title="Preparing data for kNN"></a>Preparing data for kNN</h4><ul>
<li>What would happen if some features have much larger values than others?</li>
<li>So we have to adjust scaling in our data.<ul>
<li>min-max normalization (0-1)</li>
<li><del>要继续整理Markdown数学公式的插入！</del></li>
<li>\(X_{new} = \frac{X-min(X)}{max(X)-min(X)}\)</li>
</ul>
</li>
<li>z-score standardization</li>
<li>dummy coding</li>
<li>dividing each value by the standard deviation</li>
</ul>
<h4 id="Confusion-matrix"><a href="#Confusion-matrix" class="headerlink" title="Confusion matrix"></a>Confusion matrix</h4><p>| Confusion Matrix | Actual Positives | Actual Negatives |<br>| ;————–; | ;————–; | ;————–; |<br>| Positive Predictions | True Positives (TP) | False Positives (FP) |<br>| Negative Predictions | False Negatives (FN) | True Negatives (TN) |</p>
<p>Accuracy = (TP+TN)/N<br>Precision = TP/(TP+FP)<br>Recall = TP/(TP+FN)<br>Error = (FP+FN)/N<br>F1 = 2*Recall*Precision/(Recall+Precision)<br>TP+FP = |{retrieved elements}|<br>TP = |{relevant elements}⋂{retrieved elements}|</p>
<h3 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h3><ul>
<li>conditional probability</li>
<li>posterior probability</li>
<li>likelihood</li>
<li>prior probability</li>
<li>marginal likelihood</li>
</ul>
<h4 id="Laplace-estimator"><a href="#Laplace-estimator" class="headerlink" title="Laplace estimator"></a>Laplace estimator</h4><ul>
<li>add a small number to each of the counts in the frequency table</li>
<li>this ensures that each feature has a nonzero probability of occurring with each class</li>
<li>very often, the Laplace estimator is set to 1</li>
</ul>
<h4 id="Numerical-features"><a href="#Numerical-features" class="headerlink" title="Numerical features"></a>Numerical features</h4><ul>
<li>each feature shall be categorical</li>
<li>the preceding algorithm does not deal with numeric data straightforward</li>
<li>How?<ul>
<li>discretize numeric features</li>
<li>look for natural categories</li>
<li>cut points in the data distribution</li>
<li>use quantiles</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#upload the dataset</span><br><span class="line">uciurl &lt;- &quot;https:&#x2F;&#x2F;archive.ics.uci.edu&#x2F;ml&#x2F;machine-learning-databases&#x2F;voting-records&#x2F;house-votes-84.data&quot;</span><br><span class="line">download.file(url&#x3D;uciurl, destfile&#x3D;&quot;house-votes-84.data&quot;, method&#x3D;&quot;curl&quot;)</span><br><span class="line">hv &lt;- read.csv(&quot;house-votes-84.data&quot;, header&#x3D;FALSE, stringsAsFactors&#x3D;FALSE)</span><br><span class="line"></span><br><span class="line">#inspect the dataset</span><br><span class="line">str(hv)</span><br><span class="line">names(hv)</span><br><span class="line">summary(hv)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#install relevant packages</span><br><span class="line">install.packages(&quot;e1071&quot;)</span><br><span class="line">library(e1071)</span><br><span class="line">install.packages(&quot;mlbench&quot;)</span><br><span class="line">library(mlbench)</span><br><span class="line">library(ggplot2)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#another way of retrieving the data, via the mlbench package</span><br><span class="line">data(&quot;HouseVotes84&quot;)</span><br><span class="line">str(HouseVotes84</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#inspect the dataset</span><br><span class="line">summary(HouseVotes84)</span><br><span class="line">summary(HouseVotes84$Class</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#deal with missing data</span><br><span class="line">head(is.na(HouseVotes84))</span><br><span class="line"></span><br><span class="line">#clean the dataset</span><br><span class="line">CleanDataset &lt;- na.omit(HouseVotes84)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#create a plot </span><br><span class="line">qplot(Class, data&#x3D;CleanDataset, geom &#x3D; &quot;bar&quot;) + theme(axis.text.x &#x3D; element_text(angle &#x3D; 45, hjust &#x3D; 1))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#split the dataset into training and testing ones</span><br><span class="line">summary(CleanDataset)</span><br><span class="line">hv_train &lt;- CleanDataset[1:150,]</span><br><span class="line">hv_test &lt;- CleanDataset[151:232,]</span><br><span class="line">vote_hv_train &lt;- hv_train$Class</span><br><span class="line">vote_hv_test &lt;- hv_test$Class</span><br><span class="line">prop.table(table(vote_hv_train))</span><br><span class="line">prop.table(table(vote_hv_test))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#thanks to the caret package you may also split the dataset using the createDataPartition function, eg. createDataPartition(CleanDataset$Class, p&#x3D;0.80, list &#x3D; FALSE)</span><br><span class="line">#I prefer this way! HAHA</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#train the model using Naive Bayes</span><br><span class="line">vote_classifier &lt;- naiveBayes(hv_train, vote_hv_train)</span><br><span class="line"></span><br><span class="line">#see how it works</span><br><span class="line">vote_test_pred &lt;- predict(vote_classifier, hv_test)</span><br><span class="line">head(vote_test_pred)</span><br><span class="line"></span><br><span class="line">install.packages(&quot;gmodels&quot;)</span><br><span class="line">library(gmodels)</span><br><span class="line">CrossTable(vote_hv_test, vote_test_pred,</span><br><span class="line">           prop.chisq &#x3D; FALSE, prop.t &#x3D; FALSE, prop.r &#x3D; FALSE,</span><br><span class="line">           dnn &#x3D; c(&#39;real&#39;, &#39;predicted&#39;))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#let&#39;s try to improve our model with Laplace estimator</span><br><span class="line">vote_classifier_Laplace &lt;- naiveBayes(hv_train, vote_hv_train, laplace &#x3D; 1)</span><br><span class="line">vote_test_pred_impr &lt;- predict(vote_classifier_Laplace, hv_test)</span><br><span class="line">CrossTable(vote_hv_test, vote_test_pred_impr,</span><br><span class="line">           prop.chisq &#x3D; FALSE, prop.t &#x3D; FALSE, prop.r &#x3D; FALSE,</span><br><span class="line">           dnn &#x3D; c(&#39;real&#39;, &#39;predicted&#39;))</span><br></pre></td></tr></table></figure>

<h2 id="Lesson-3"><a href="#Lesson-3" class="headerlink" title="Lesson 3"></a>Lesson 3</h2><h3 id="Decision-Trees"><a href="#Decision-Trees" class="headerlink" title="Decision Trees"></a>Decision Trees</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#upload the dataset</span><br><span class="line">credit_ger &lt;- read.csv(&quot;credit_ger.csv&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#inspect the dataset</span><br><span class="line">str(credit_ger)</span><br><span class="line"></span><br><span class="line">#we see 1000 obs and 21 features of different data types</span><br><span class="line">#let&#39;s take a closer look at a couple of features of loans</span><br><span class="line">#these features may predict a default</span><br><span class="line">#how about checking and saving account balance?</span><br><span class="line">table(credit_ger$checking_status)</span><br><span class="line">table(credit_ger$savings_status)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#duration of loan and the amount of credit seems to be important as well</span><br><span class="line">summary(credit_ger$duration)</span><br><span class="line">summary(credit_ger$credit_amount)</span><br><span class="line"></span><br><span class="line">#was not the loan applicant able to meet the payment terms?</span><br><span class="line">table(credit_ger$class)</span><br><span class="line"></span><br><span class="line">#thus 700 clients were assigned as good applicants and 300 as bad applicants</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">#order our data randomly before splitting </span><br><span class="line">#order function is for arranging our data in ascending or descending order</span><br><span class="line">#runif function is responsible for generating a sequence of random numbers between 0 and 1</span><br><span class="line">#set.seed function is for generating random numbers in a predefined sequence starting from a seed position, here values from 1 to 5</span><br><span class="line">#we are going to create 1000 random numbers as we have 1000 observations</span><br><span class="line">set.seed(12345)</span><br><span class="line">credit_ger_rand &lt;- credit_ger[order(runif(1000)),]</span><br><span class="line"></span><br><span class="line">#check, whether it works</span><br><span class="line">summary(credit_ger$credit_amount)</span><br><span class="line">summary(credit_ger_rand$credit_amount)</span><br><span class="line"></span><br><span class="line">head(credit_ger$credit_amount)</span><br><span class="line">head(credit_ger_rand$credit_amount)</span><br><span class="line"></span><br><span class="line">#ok, so we have our records in a random order</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#split the data in the 1:9 proportion</span><br><span class="line">credit_ger_train &lt;- credit_ger_rand[1:900,]</span><br><span class="line">credit_ger_test &lt;- credit_ger_rand[901:1000,]</span><br><span class="line"></span><br><span class="line">prop.table(table(credit_ger_train$class))</span><br><span class="line">prop.table(table(credit_ger_test$class))</span><br><span class="line"></span><br><span class="line">#the split looks as a fairly equal one</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#train your model</span><br><span class="line">install.packages(&quot;C50&quot;)</span><br><span class="line">library(C50)</span><br><span class="line"></span><br><span class="line">#we skip the 21st column as it refers to the class feature</span><br><span class="line">#class has to be identified as a factor</span><br><span class="line">credit_ger_model &lt;- C5.0(credit_ger_train[-21], credit_ger_train$class)</span><br><span class="line">credit_ger_model</span><br><span class="line">summary(credit_ger_model)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#evaluate the model</span><br><span class="line">credit_ger_predict &lt;- predict(credit_ger_model, credit_ger_test)</span><br><span class="line">install.packages(&quot;gmodels&quot;)</span><br><span class="line">library(gmodels)</span><br><span class="line">CrossTable(credit_ger_test$class, credit_ger_predict,</span><br><span class="line">           prop.chisq &#x3D; FALSE, prop.c &#x3D; FALSE, prop.r &#x3D; FALSE,</span><br><span class="line">           dnn &#x3D; c(&#39;real class&#39;, &#39;predicted class&#39;))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">#improve the model</span><br><span class="line">credit_ger_boost10 &lt;- C5.0(credit_ger_train[-21], credit_ger_train$class, trials &#x3D; 10)</span><br><span class="line">credit_ger_boost10</span><br><span class="line">summary(credit_ger_boost10)</span><br><span class="line"></span><br><span class="line">credit_ger_boost10_predict &lt;- predict(credit_ger_boost10, credit_ger_test)</span><br><span class="line">CrossTable(credit_ger_test$class, credit_ger_boost10_predict,</span><br><span class="line">           prop.chisq &#x3D; FALSE, prop.c &#x3D; FALSE, prop.r &#x3D; FALSE,</span><br><span class="line">           dnn &#x3D; c(&#39;real class&#39;, &#39;predicted class&#39;))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">#how about checking how costly mistakes are?</span><br><span class="line">#assumption: loan default costs the bank four times as much as a missed opportunity</span><br><span class="line">error_cost &lt;- matrix(c(0, 1, 4, 0), nrow &#x3D; 2)</span><br><span class="line">error_cost</span><br><span class="line"></span><br><span class="line">credit_cost &lt;- C5.0(credit_ger_train[-21], credit_ger_train$class, costs&#x3D;error_cost)</span><br><span class="line">credit_cost_predict &lt;- predict(credit_cost, credit_ger_test)</span><br><span class="line">CrossTable(credit_ger_test$class, credit_cost_predict,</span><br><span class="line">           prop.chisq &#x3D; FALSE, prop.c &#x3D; FALSE, prop.r &#x3D; FALSE,</span><br><span class="line">           dnn &#x3D; c(&#39;real class&#39;, &#39;predicted class&#39;))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">library(rpart)</span><br><span class="line">install.packages(&quot;rpart.plot&quot;)</span><br><span class="line">library(rpart.plot)</span><br><span class="line"></span><br><span class="line">rtree &lt;- rpart(class ~ ., credit_ger_train)</span><br><span class="line">rpart.plot(rtree)</span><br><span class="line">#another approach</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#conditional inference</span><br><span class="line">install.packages(&quot;party&quot;)</span><br><span class="line">library(party)</span><br><span class="line">ctree_ &lt;- ctree(class ~ ., credit_ger_train)</span><br><span class="line">plot(ctree_)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#results and cross-validation</span><br><span class="line">printcp(rtree)</span><br><span class="line">plotcp(rtree)</span><br><span class="line">summary(rtree)</span><br><span class="line"></span><br><span class="line">plot(rtree, uniform&#x3D;TRUE,</span><br><span class="line">     main&#x3D;&quot;Classification Tree for Credits&quot;)</span><br><span class="line">text(rtree, use.n&#x3D;TRUE, all&#x3D;TRUE, cex&#x3D;.8)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#prune the tree and plot the tree then</span><br><span class="line">prtree&lt;- prune(rtree, cp&#x3D;   rtree$cptable[which.min(rtree$cptable[,&quot;xerror&quot;]),&quot;CP&quot;])</span><br><span class="line"></span><br><span class="line">plot(prtree, uniform&#x3D;TRUE,</span><br><span class="line">     main&#x3D;&quot;Pruned Classification Tree for Credits&quot;)</span><br><span class="line">text(prtree, use.n&#x3D;TRUE, all&#x3D;TRUE, cex&#x3D;.8)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#another way</span><br><span class="line">prtree2&lt;- prune(rtree, cp&#x3D;0.008) # from cptable  </span><br><span class="line">plot(prtree2, uniform&#x3D;TRUE,</span><br><span class="line">     main&#x3D;&quot;Pruned Regression Tree for Credits&quot;)</span><br><span class="line">text(prtree2, use.n&#x3D;TRUE, all&#x3D;TRUE, cex&#x3D;.8)</span><br></pre></td></tr></table></figure>

<h4 id="Weaknesses"><a href="#Weaknesses" class="headerlink" title="Weaknesses"></a>Weaknesses</h4><ul>
<li>usually biased toward splits on features with a large number of levels</li>
<li>easy to overfit or underfit the model</li>
<li>some relationships troublesome to model due to reliance on axis-parallel splits</li>
<li>even small changes in training data may be crucial with respect to decision logic</li>
<li>large trees challenging to interpret and sometimes counterintuitive</li>
</ul>
<h3 id="Recursive-Partitioning-divide-and-conquer"><a href="#Recursive-Partitioning-divide-and-conquer" class="headerlink" title="Recursive Partitioning - divide and conquer"></a>Recursive Partitioning - divide and conquer</h3><p>Recursive partitioning is a statistical method for multivariable analysis. Recursive partitioning creates a decision tree that strives to correctly classify members of the population by splitting it into sub-populations based on several dichotomous independent variables. The process is termed recursive because each sub-population may in turn be split an indefinite number of times until the splitting process terminates after a particular stopping criterion is reached.</p>
<h4 id="The-best-split"><a href="#The-best-split" class="headerlink" title="The best split"></a>The best split</h4><p>Entropy of all possible values of x</p>
<h4 id="Pruning-the-decision-tree"><a href="#Pruning-the-decision-tree" class="headerlink" title="Pruning the decision tree"></a>Pruning the decision tree</h4><ul>
<li>too large, decisions may be overly specific (overfitted model)</li>
<li>pre-pruning (early stopping)</li>
<li>post-pruning</li>
</ul>
<h3 id="One-Rule-Algorithm"><a href="#One-Rule-Algorithm" class="headerlink" title="One Rule Algorithm"></a>One Rule Algorithm</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">#get the mushroom data</span><br><span class="line">theURL &lt;- &quot;https:&#x2F;&#x2F;archive.ics.uci.edu&#x2F;ml&#x2F;machine-learning-databases&#x2F;mushroom&#x2F;agaricus-lepiota.data&quot;</span><br><span class="line">mushrooms &lt;- read.table(theURL, header &#x3D; FALSE,  sep &#x3D; &quot;,&quot;, stringsAsFactors &#x3D; FALSE)</span><br><span class="line">write.table(mushrooms, file &#x3D; &quot;mushroom.csv&quot;, sep &#x3D; &quot;,&quot;, col.names &#x3D; FALSE)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">#tidy the data</span><br><span class="line">colnames(mushrooms) &lt;- c(&quot;edibility&quot;, &quot;cap_shape&quot;, &quot;cap_surface&quot;, </span><br><span class="line">                        &quot;cap_color&quot;, &quot;bruises&quot;, &quot;odor&quot;, </span><br><span class="line">                        &quot;gill_attachement&quot;, &quot;gill_spacing&quot;, &quot;gill_size&quot;, </span><br><span class="line">                        &quot;gill_color&quot;, &quot;stalk_shape&quot;, &quot;stalk_root&quot;, </span><br><span class="line">                        &quot;stalk_surface_above_ring&quot;, &quot;stalk_surface_below_ring&quot;, &quot;stalk_color_above_ring&quot;, </span><br><span class="line">                        &quot;stalk_color_below_ring&quot;, &quot;veil_type&quot;, &quot;veil_color&quot;, </span><br><span class="line">                        &quot;ring_number&quot;, &quot;ring_type&quot;, &quot;spore_print_color&quot;, </span><br><span class="line">                        &quot;population&quot;, &quot;habitat&quot;)</span><br><span class="line"></span><br><span class="line">install.packages(&quot;tidyverse&quot;)</span><br><span class="line">library(tidyverse)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">#treat features as factors</span><br><span class="line">mushrooms &lt;- mushrooms %&gt;% map_df(function(.x) as.factor(.x))</span><br><span class="line"></span><br><span class="line">#redefine each of the category</span><br><span class="line">levels(mushrooms$edibility) &lt;- c(&quot;edible&quot;, &quot;poisonous&quot;)</span><br><span class="line">levels(mushrooms$cap_shape) &lt;- c(&quot;bell&quot;, &quot;conical&quot;, &quot;flat&quot;, &quot;knobbed&quot;, &quot;sunken&quot;, &quot;convex&quot;)</span><br><span class="line">levels(mushrooms$cap_color) &lt;- c(&quot;buff&quot;, &quot;cinnamon&quot;, &quot;red&quot;, &quot;gray&quot;, &quot;brown&quot;, &quot;pink&quot;, </span><br><span class="line">                                &quot;green&quot;, &quot;purple&quot;, &quot;white&quot;, &quot;yellow&quot;)</span><br><span class="line">levels(mushrooms$cap_surface) &lt;- c(&quot;fibrous&quot;, &quot;grooves&quot;, &quot;scaly&quot;, &quot;smooth&quot;)</span><br><span class="line">levels(mushrooms$bruises) &lt;- c(&quot;no&quot;, &quot;yes&quot;)</span><br><span class="line">levels(mushrooms$odor) &lt;- c(&quot;almond&quot;, &quot;creosote&quot;, &quot;foul&quot;, &quot;anise&quot;, &quot;musty&quot;, &quot;none&quot;, &quot;pungent&quot;, &quot;spicy&quot;, &quot;fishy&quot;)</span><br><span class="line">levels(mushrooms$gill_attachement) &lt;- c(&quot;attached&quot;, &quot;free&quot;)</span><br><span class="line">levels(mushrooms$gill_spacing) &lt;- c(&quot;close&quot;, &quot;crowded&quot;)</span><br><span class="line">levels(mushrooms$gill_size) &lt;- c(&quot;broad&quot;, &quot;narrow&quot;)</span><br><span class="line">levels(mushrooms$gill_color) &lt;- c(&quot;buff&quot;, &quot;red&quot;, &quot;gray&quot;, &quot;chocolate&quot;, &quot;black&quot;, &quot;brown&quot;, &quot;orange&quot;, </span><br><span class="line">                                 &quot;pink&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;white&quot;, &quot;yellow&quot;)</span><br><span class="line">levels(mushrooms$stalk_shape) &lt;- c(&quot;enlarging&quot;, &quot;tapering&quot;)</span><br><span class="line">levels(mushrooms$stalk_root) &lt;- c(&quot;missing&quot;, &quot;bulbous&quot;, &quot;club&quot;, &quot;equal&quot;, &quot;rooted&quot;)</span><br><span class="line">levels(mushrooms$stalk_surface_above_ring) &lt;- c(&quot;fibrous&quot;, &quot;silky&quot;, &quot;smooth&quot;, &quot;scaly&quot;)</span><br><span class="line">levels(mushrooms$stalk_surface_below_ring) &lt;- c(&quot;fibrous&quot;, &quot;silky&quot;, &quot;smooth&quot;, &quot;scaly&quot;)</span><br><span class="line">levels(mushrooms$stalk_color_above_ring) &lt;- c(&quot;buff&quot;, &quot;cinnamon&quot;, &quot;red&quot;, &quot;gray&quot;, &quot;brown&quot;, &quot;pink&quot;, </span><br><span class="line">                                             &quot;green&quot;, &quot;purple&quot;, &quot;white&quot;, &quot;yellow&quot;)</span><br><span class="line">levels(mushrooms$stalk_color_below_ring) &lt;- c(&quot;buff&quot;, &quot;cinnamon&quot;, &quot;red&quot;, &quot;gray&quot;, &quot;brown&quot;, &quot;pink&quot;, </span><br><span class="line">                                             &quot;green&quot;, &quot;purple&quot;, &quot;white&quot;, &quot;yellow&quot;)</span><br><span class="line">levels(mushrooms$veil_type) &lt;- &quot;partial&quot;</span><br><span class="line">levels(mushrooms$veil_color) &lt;- c(&quot;brown&quot;, &quot;orange&quot;, &quot;white&quot;, &quot;yellow&quot;)</span><br><span class="line">levels(mushrooms$ring_number) &lt;- c(&quot;none&quot;, &quot;one&quot;, &quot;two&quot;)</span><br><span class="line">levels(mushrooms$ring_type) &lt;- c(&quot;evanescent&quot;, &quot;flaring&quot;, &quot;large&quot;, &quot;none&quot;, &quot;pendant&quot;)</span><br><span class="line">levels(mushrooms$spore_print_color) &lt;- c(&quot;buff&quot;, &quot;chocolate&quot;, &quot;black&quot;, &quot;brown&quot;, &quot;orange&quot;, </span><br><span class="line">                                        &quot;green&quot;, &quot;purple&quot;, &quot;white&quot;, &quot;yellow&quot;)</span><br><span class="line">levels(mushrooms$population) &lt;- c(&quot;abundant&quot;, &quot;clustered&quot;, &quot;numerous&quot;, &quot;scattered&quot;, &quot;several&quot;, &quot;solitary&quot;)</span><br><span class="line">levels(mushrooms$habitat) &lt;- c(&quot;wood&quot;, &quot;grasses&quot;, &quot;leaves&quot;, &quot;meadows&quot;, &quot;paths&quot;, &quot;urban&quot;, &quot;waste&quot;)</span><br><span class="line"></span><br><span class="line">#inspect changes</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">#veil type may be problematic as it has only 1 level</span><br><span class="line">mushrooms &lt;- mushrooms %&gt;% select(- veil_type)</span><br><span class="line"></span><br><span class="line">#look into the data</span><br><span class="line">table(mushrooms$edibility)</span><br><span class="line"></span><br><span class="line">#almost 50:50 proportions</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#train your model on the data</span><br><span class="line">install.packages(&quot;OneR&quot;)</span><br><span class="line">library(OneR)</span><br><span class="line">mushrooms_1R &lt;- OneR(edibility ~., data&#x3D;mushrooms)</span><br><span class="line">mushrooms_1R</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">summary(mushrooms_1R)</span><br></pre></td></tr></table></figure>

<h3 id="PIPPER-Algorithm"><a href="#PIPPER-Algorithm" class="headerlink" title="PIPPER Algorithm"></a>PIPPER Algorithm</h3><ul>
<li>IREP (Incremental Reduced Error  Pruning Algorithm)</li>
<li>PIPPER (Repeated Incremental Pruning to Produce Error Reduction Alogorithm)</li>
</ul>
<h2 id="Lesson-4"><a href="#Lesson-4" class="headerlink" title="Lesson 4"></a>Lesson 4</h2><h3 id="Support-Vector-Machine-SVM"><a href="#Support-Vector-Machine-SVM" class="headerlink" title="Support Vector Machine (SVM)"></a>Support Vector Machine (SVM)</h3><ul>
<li>SVM combines<ul>
<li>Aspects of instance-based kNN</li>
<li>Linear regression modelling</li>
</ul>
</li>
<li>choose Maximum Margin (Maximum Margin Hyperplane)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"># set pseudorandom number generator</span><br><span class="line">set.seed(10)</span><br><span class="line"></span><br><span class="line"># load the relevant packages</span><br><span class="line">install.packages(&quot;tidyverse&quot;)</span><br><span class="line">library(tidyverse)    # data manipulation and visualization</span><br><span class="line">install.packages(&quot;kernlab&quot;)</span><br><span class="line">library(kernlab)      # SVM methodology</span><br><span class="line">install.packages(&quot;e1071&quot;)</span><br><span class="line">library(e1071)        # SVM methodology</span><br><span class="line">install.packages(&quot;ISLR&quot;)</span><br><span class="line">library(ISLR)         # contains example data set &quot;Khan&quot;</span><br><span class="line">install.packages(&quot;RColorBrewer&quot;)</span><br><span class="line">library(RColorBrewer) # customized coloring of plots</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># generate 40 random obs in 2 classes</span><br><span class="line"># use Maximal Margin Classifier</span><br><span class="line"># construct sample data set - completely separated</span><br><span class="line">x &lt;- matrix(rnorm(20*2), ncol &#x3D; 2)</span><br><span class="line">y &lt;- c(rep(-1,10), rep(1,10))</span><br><span class="line">x[y&#x3D;&#x3D;1,] &lt;- x[y&#x3D;&#x3D;1,] + 3&#x2F;2</span><br><span class="line">dat &lt;- data.frame(x&#x3D;x, y&#x3D;as.factor(y))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># plot data</span><br><span class="line">ggplot(data &#x3D; dat, aes(x &#x3D; x.2, y &#x3D; x.1, color &#x3D; y, shape &#x3D; y)) + </span><br><span class="line">  geom_point(size &#x3D; 2) +</span><br><span class="line">  scale_color_manual(values&#x3D;c(&quot;#000000&quot;, &quot;#FF0000&quot;)) +</span><br><span class="line">  theme(legend.position &#x3D; &quot;none&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># where is the boundary?</span><br><span class="line"># fit Support Vector Machine model to data set</span><br><span class="line">library(e1071) </span><br><span class="line">svmfit &lt;- svm(y~., data &#x3D; dat, kernel &#x3D; &quot;linear&quot;, scale &#x3D; FALSE)</span><br><span class="line"># plot Results</span><br><span class="line">plot(svmfit, dat)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># another way of doing the same</span><br><span class="line"># fit model and produce plot</span><br><span class="line">library(kernlab)   </span><br><span class="line">kernfit &lt;- ksvm(x, y, type &#x3D; &quot;C-svc&quot;, kernel &#x3D; &#39;vanilladot&#39;)</span><br><span class="line">plot(kernfit, data &#x3D; x)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># in reality classes may be mixed</span><br><span class="line"># create another sample data</span><br><span class="line"># construct sample data set - not completely separated</span><br><span class="line">x &lt;- matrix(rnorm(20*2), ncol &#x3D; 2)</span><br><span class="line">y &lt;- c(rep(-1,10), rep(1,10))</span><br><span class="line">x[y&#x3D;&#x3D;1,] &lt;- x[y&#x3D;&#x3D;1,] + 1</span><br><span class="line">dat &lt;- data.frame(x&#x3D;x, y&#x3D;as.factor(y))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># plot data set</span><br><span class="line">ggplot(data &#x3D; dat, aes(x &#x3D; x.2, y &#x3D; x.1, color &#x3D; y, shape &#x3D; y)) + </span><br><span class="line">  geom_point(size &#x3D; 2) +</span><br><span class="line">  scale_color_manual(values&#x3D;c(&quot;#000000&quot;, &quot;#FF0000&quot;)) +</span><br><span class="line">  theme(legend.position &#x3D; &quot;none&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># same approach like in previous example</span><br><span class="line"># fit Support Vector Machine model to data set</span><br><span class="line">svmfit &lt;- svm(y~., data &#x3D; dat, kernel &#x3D; &quot;linear&quot;, cost &#x3D; 10)</span><br><span class="line"># plot results</span><br><span class="line">plot(svmfit, dat)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># fit Support Vector Machine model to data set</span><br><span class="line">kernfit &lt;- ksvm(x,y, type &#x3D; &quot;C-svc&quot;, kernel &#x3D; &#39;vanilladot&#39;, C &#x3D; 100)</span><br><span class="line"># plot results</span><br><span class="line">plot(kernfit, data &#x3D; x)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># what is the scope of misclassification?</span><br><span class="line"># find optimal cost of misclassification</span><br><span class="line">tune.out &lt;- tune(svm, y~., data &#x3D; dat, kernel &#x3D; &quot;linear&quot;,</span><br><span class="line">                 ranges &#x3D; list(cost &#x3D; c(0.001, 0.01, 0.1, 1, 5, 10, 100)))</span><br><span class="line"># extract the best model</span><br><span class="line">(bestmod &lt;- tune.out$best.model)</span><br><span class="line"></span><br><span class="line"># create a table of misclassified observations</span><br><span class="line">ypred &lt;- predict(bestmod, dat)</span><br><span class="line">(misclass &lt;- table(predict &#x3D; ypred, truth &#x3D; dat$y))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"># deal with a larger dataset!</span><br><span class="line">x &lt;- matrix(rnorm(200*2), ncol &#x3D; 2)</span><br><span class="line">x[1:100,] &lt;- x[1:100,] + 2.5</span><br><span class="line">x[101:150,] &lt;- x[101:150,] - 2.5</span><br><span class="line">y &lt;- c(rep(1,150), rep(2,50))</span><br><span class="line">dat &lt;- data.frame(x&#x3D;x,y&#x3D;as.factor(y))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># make a plot</span><br><span class="line">ggplot(data &#x3D; dat, aes(x &#x3D; x.2, y &#x3D; x.1, color &#x3D; y, shape &#x3D; y)) + </span><br><span class="line">  geom_point(size &#x3D; 2) +</span><br><span class="line">  scale_color_manual(values&#x3D;c(&quot;#000000&quot;, &quot;#FF0000&quot;)) +</span><br><span class="line">  theme(legend.position &#x3D; &quot;none&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># data is not linearly separable</span><br><span class="line"># set pseudorandom number generator</span><br><span class="line">set.seed(123)</span><br><span class="line"># sample training data and fit model</span><br><span class="line"># use 100 random objects for developing the boundary</span><br><span class="line">train &lt;- base::sample(200,100, replace &#x3D; FALSE)</span><br><span class="line">svmfit &lt;- svm(y~., data &#x3D; dat[train,], kernel &#x3D; &quot;radial&quot;, gamma &#x3D; 1, cost &#x3D; 1)</span><br><span class="line"># plot classifier</span><br><span class="line">plot(svmfit, dat)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># another approach</span><br><span class="line"># fit radial-based SVM in kernlab</span><br><span class="line">kernfit &lt;- ksvm(x[train,],y[train], type &#x3D; &quot;C-svc&quot;, kernel &#x3D; &#39;rbfdot&#39;, C &#x3D; 1, scaled &#x3D; c())</span><br><span class="line"># plot training data</span><br><span class="line">plot(kernfit, data &#x3D; x[train,])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"># try with another parameters to fit nonlinear boundaries</span><br><span class="line"># tune model to find optimal cost, gamma values</span><br><span class="line">tune.out &lt;- tune(svm, y~., data &#x3D; dat[train,], kernel &#x3D; &quot;radial&quot;,</span><br><span class="line">                 ranges &#x3D; list(cost &#x3D; c(0.1,1,10,100,1000),</span><br><span class="line">                               gamma &#x3D; c(0.5,1,2,3,4)))</span><br><span class="line"># show best model</span><br><span class="line">tune.out$best.model</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># cost of 1 and gamma of 0.5 seem to be adequate (it reduces error in the best manner)</span><br><span class="line"># evaluate the model</span><br><span class="line">(valid &lt;- table(true &#x3D; dat[-train,&quot;y&quot;], pred &#x3D; predict(tune.out$best.model,</span><br><span class="line">                                                       newx &#x3D; dat[-train,])))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">x &lt;- rbind(x, matrix(rnorm(50*2), ncol &#x3D; 2))</span><br><span class="line">y &lt;- c(y, rep(0,50))</span><br><span class="line">x[y&#x3D;&#x3D;0,2] &lt;- x[y&#x3D;&#x3D;0,2] + 2.5</span><br><span class="line">dat &lt;- data.frame(x&#x3D;x, y&#x3D;as.factor(y))</span><br><span class="line"># plot data set</span><br><span class="line">ggplot(data &#x3D; dat, aes(x &#x3D; x.2, y &#x3D; x.1, color &#x3D; y, shape &#x3D; y)) + </span><br><span class="line">  geom_point(size &#x3D; 2) +</span><br><span class="line">  scale_color_manual(values&#x3D;c(&quot;#000000&quot;,&quot;#FF0000&quot;,&quot;#00BA00&quot;)) +</span><br><span class="line">  theme(legend.position &#x3D; &quot;none&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># fit model</span><br><span class="line">svmfit &lt;- svm(y~., data &#x3D; dat, kernel &#x3D; &quot;radial&quot;, cost &#x3D; 10, gamma &#x3D; 1)</span><br><span class="line"># plot results</span><br><span class="line">plot(svmfit, dat)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># construct table</span><br><span class="line">ypred &lt;- predict(svmfit, dat)</span><br><span class="line">(misclass &lt;- table(predict &#x3D; ypred, truth &#x3D; dat$y))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># fit and plot</span><br><span class="line">kernfit &lt;- ksvm(as.matrix(dat[,2:1]),dat$y, type &#x3D; &quot;C-svc&quot;, kernel &#x3D; &#39;rbfdot&#39;, </span><br><span class="line">                C &#x3D; 100, scaled &#x3D; c())</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># create a fine grid of the feature space</span><br><span class="line">x.1 &lt;- seq(from &#x3D; min(dat$x.1), to &#x3D; max(dat$x.1), length &#x3D; 100)</span><br><span class="line">x.2 &lt;- seq(from &#x3D; min(dat$x.2), to &#x3D; max(dat$x.2), length &#x3D; 100)</span><br><span class="line">x.grid &lt;- expand.grid(x.2, x.1)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># get class predictions over grid</span><br><span class="line">pred &lt;- predict(kernfit, newdata &#x3D; x.grid)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"># plot the results</span><br><span class="line">library(RColorBrewer)</span><br><span class="line">cols &lt;- brewer.pal(3, &quot;Set1&quot;)</span><br><span class="line">plot(x.grid, pch &#x3D; 19, col &#x3D; adjustcolor(cols[pred], alpha.f &#x3D; 0.05))</span><br><span class="line"></span><br><span class="line">classes &lt;- matrix(pred, nrow &#x3D; 100, ncol &#x3D; 100)</span><br><span class="line">contour(x &#x3D; x.2, y &#x3D; x.1, z &#x3D; classes, levels &#x3D; 1:3, labels &#x3D; &quot;&quot;, add &#x3D; TRUE)</span><br><span class="line"></span><br><span class="line">points(dat[, 2:1], pch &#x3D; 19, col &#x3D; cols[predict(kernfit)])</span><br></pre></td></tr></table></figure>

<h3 id="Linearly-Separable-Data"><a href="#Linearly-Separable-Data" class="headerlink" title="Linearly Separable Data"></a>Linearly Separable Data</h3><h3 id="Non-linearly-Separable-Data"><a href="#Non-linearly-Separable-Data" class="headerlink" title="Non-linearly Separable Data"></a>Non-linearly Separable Data</h3><h3 id="Kernels-for-Non-linear-Space"><a href="#Kernels-for-Non-linear-Space" class="headerlink" title="Kernels for Non-linear Space"></a>Kernels for Non-linear Space</h3><p>SVMs are able to map a problem into demension space thanks to so-called kernel trick.</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#example from the handbook - Lantz, 2013</span><br><span class="line">#upload the data</span><br><span class="line">letters &lt;- read.csv(&quot;letterdata.csv&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#inspect the dataset</span><br><span class="line">str(letters)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#the dataset is normalized</span><br><span class="line">#we have 20k observations</span><br><span class="line">#let&#39;s use 80% of the data for training and 20% for testing</span><br><span class="line">letters_train &lt;- letters[1:16000, ]</span><br><span class="line">letters_test &lt;- letters[16001:20000, ]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#start with linear SVM (kernlab package)</span><br><span class="line">#vanilladot stands for the linear kernel</span><br><span class="line">install.packages(&quot;kernlab&quot;)</span><br><span class="line">library(kernlab)</span><br><span class="line">letter_classifier &lt;- ksvm(class ~ ., data &#x3D; letters_train, kernel &#x3D; &quot;vanilladot&quot;)</span><br><span class="line">letter_classifier</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#evaluate the model</span><br><span class="line">letter_predictions &lt;- predict(letter_classifier, letters_test)</span><br><span class="line"></span><br><span class="line">#what letter it may be?</span><br><span class="line">head(letter_predictions)</span><br><span class="line"></span><br><span class="line">#how it works?</span><br><span class="line">table(letter_predictions, letters_test$class)</span><br><span class="line"></span><br><span class="line">#diagonal values show the total number of records where the predicted letter matches the true value</span><br><span class="line">#no of mistakes lited as well (like A misidentified as 1 one time)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#compute general accuracy</span><br><span class="line">agreement &lt;- letter_predictions &#x3D;&#x3D; letters_test$class</span><br><span class="line">table(agreement)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#true label indicates how many times a letter was classified properly</span><br><span class="line">#go for fraction of the dataset correctly classified</span><br><span class="line">prop.table(table(agreement))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">#improve the model!</span><br><span class="line">#use more sophisticated kernel function, like Gaussian RBF</span><br><span class="line">letter_classifier_rbf &lt;- ksvm(class ~ ., data &#x3D; letters_train,</span><br><span class="line">                              kernel &#x3D; &quot;rbfdot&quot;)</span><br><span class="line"></span><br><span class="line">#predictions</span><br><span class="line">letter_predictions_rbf &lt;- predict(letter_classifier_rbf,</span><br><span class="line">                                  letters_test)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#is it better than the linear kernel?</span><br><span class="line">agreement_rbf &lt;- letter_predictions_rbf &#x3D;&#x3D; letters_test$class</span><br><span class="line">table(agreement_rbf)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#yes!</span><br><span class="line">prop.table(table(agreement_rbf))</span><br></pre></td></tr></table></figure>

<p>Another example (SVM):</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">#check which packages are installed</span><br><span class="line">packages &lt;- c(&quot;caret&quot;, &quot;data.table&quot;, &quot;corrplot&quot;, &quot;rattle&quot;, &quot;randomForest&quot;, &quot;C50&quot;, &quot;rpart&quot;, &quot;ROCR&quot;, &quot;e1071&quot;,&quot;gmodels&quot;)</span><br><span class="line">if (length(setdiff(packages, rownames(installed.packages()))) &gt; 0) &#123;</span><br><span class="line">  install.packages(setdiff(packages, rownames(installed.packages())))  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#load the packages</span><br><span class="line">library(caret)</span><br><span class="line">library(ggplot2)</span><br><span class="line">library(data.table)</span><br><span class="line">library(corrplot)</span><br><span class="line">library(rattle)</span><br><span class="line">library(randomForest)</span><br><span class="line">library(C50)</span><br><span class="line">library(rpart)</span><br><span class="line">library(ROCR)</span><br><span class="line">library(e1071)</span><br><span class="line">library(gmodels)</span><br><span class="line">library(data.table)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">#import the dataset</span><br><span class="line">cust_data &lt;- fread(&#39;tele_com_churn_org.csv&#39;, header &#x3D; TRUE, sep &#x3D; &quot;,&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">#clean the dataset</span><br><span class="line">#remove reduntant variables, like the first one</span><br><span class="line">cust_data &lt;- cust_data[, -1]</span><br><span class="line"></span><br><span class="line">#drop missing values</span><br><span class="line">cust_data[is.na(cust_data)] &lt;- 0</span><br><span class="line"></span><br><span class="line">#recode the churn status</span><br><span class="line">cust_data$Churn &lt;- replace(cust_data$Churn, cust_data$Churn &#x3D;&#x3D; &quot;No&quot;, 0)</span><br><span class="line">cust_data$Churn &lt;- replace(cust_data$Churn, cust_data$Churn &#x3D;&#x3D; &quot;Yes&quot;, 1)</span><br><span class="line">cust_data$Churn &lt;- as.numeric(cust_data$Churn)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"># Recode Variables: Recode using the library(car) package</span><br><span class="line">install.packages(&quot;car&quot;)</span><br><span class="line">library(car)</span><br><span class="line"></span><br><span class="line">cust_data$gender &lt;- recode(cust_data$gender, &quot;&#39;Male&#39;&#x3D;1; &#39;Female&#39;&#x3D;0&quot;)</span><br><span class="line">cust_data$Partner &lt;- recode(cust_data$Partner, &quot;&#39;Yes&#39;&#x3D;1; &#39;No&#39;&#x3D;0&quot;)</span><br><span class="line">cust_data$Dependents &lt;- recode(cust_data$Dependents, &quot;&#39;Yes&#39;&#x3D;1; &#39;No&#39;&#x3D;0&quot;)</span><br><span class="line">cust_data$PhoneService &lt;- recode(cust_data$PhoneService, &quot;&#39;Yes&#39;&#x3D;1; &#39;No&#39;&#x3D;0&quot;)</span><br><span class="line">cust_data$MultipleLines &lt;- recode(cust_data$MultipleLines, &quot;&#39;Yes&#39;&#x3D;1; &#39;No&#39;&#x3D;0;&#39;No phone service&#39;&#x3D;3&quot;)</span><br><span class="line">cust_data$InternetService &lt;- recode(cust_data$InternetService, &quot;&#39;No&#39;&#x3D;0; &#39;DSL&#39;&#x3D;1;&#39;Fiber optic&#39;&#x3D;2&quot;)</span><br><span class="line">cust_data$OnlineSecurity &lt;- recode(cust_data$OnlineSecurity, &quot;&#39;No&#39;&#x3D;0; &#39;Yes&#39;&#x3D;1;&#39;No internet service&#39;&#x3D;2&quot;)</span><br><span class="line">cust_data$OnlineBackup &lt;- recode(cust_data$OnlineBackup, &quot;&#39;No&#39;&#x3D;0; &#39;Yes&#39;&#x3D;1;&#39;No internet service&#39;&#x3D;2&quot;)</span><br><span class="line">cust_data$DeviceProtection &lt;- recode(cust_data$DeviceProtection, &quot;&#39;No&#39;&#x3D;0; &#39;Yes&#39;&#x3D;1;&#39;No internet service&#39;&#x3D;2&quot;)</span><br><span class="line">cust_data$TechSupport &lt;- recode(cust_data$TechSupport, &quot;&#39;No&#39;&#x3D;0; &#39;Yes&#39;&#x3D;1;&#39;No internet service&#39;&#x3D;2&quot;)</span><br><span class="line">cust_data$StreamingTV &lt;- recode(cust_data$StreamingTV, &quot;&#39;No&#39;&#x3D;0; &#39;Yes&#39;&#x3D;1;&#39;No internet service&#39;&#x3D;2&quot;)</span><br><span class="line">cust_data$StreamingMovies &lt;- recode(cust_data$StreamingMovies, &quot;&#39;No&#39;&#x3D;0; &#39;Yes&#39;&#x3D;1;&#39;No internet service&#39;&#x3D;2&quot;)</span><br><span class="line">cust_data$Contract &lt;- recode(cust_data$Contract, &quot;&#39;Month-to-month&#39;&#x3D;0; &#39;One year&#39;&#x3D;1;&#39;Two year&#39;&#x3D;2&quot;)</span><br><span class="line">cust_data$PaperlessBilling &lt;- recode(cust_data$PaperlessBilling, &quot;&#39;Yes&#39;&#x3D;1; &#39;No&#39;&#x3D;0&quot;)</span><br><span class="line">cust_data$PaymentMethod &lt;- recode(cust_data$PaymentMethod, &quot;&#39;Electronic check&#39;&#x3D;1; &#39;Mailed check&#39;&#x3D;2;&#39;Bank transfer (automatic)&#39;&#x3D;3; &#39;Credit card (automatic)&#39;&#x3D;4&quot;)</span><br><span class="line"></span><br><span class="line">#take the churn as factor variable</span><br><span class="line">cust_data[, &#39;Churn&#39;] &lt;- lapply(cust_data[, &#39;Churn&#39;], factor)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#inspect the data</span><br><span class="line">summary(cust_data)</span><br><span class="line">str(cust_data)</span><br><span class="line"></span><br><span class="line">corrmatrix &lt;- round(cor(cust_data[, - &#39;Churn&#39;]), digits &#x3D; 2)</span><br><span class="line">corrmatrix</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">#switch to factor of categorical variables</span><br><span class="line">cust_data$gender &lt;- factor(cust_data$gender)</span><br><span class="line">cust_data$SeniorCitizen &lt;- factor(cust_data$SeniorCitizen )</span><br><span class="line">cust_data$Partner &lt;- factor(cust_data$Partner)</span><br><span class="line">cust_data$Dependents &lt;- factor(cust_data$Dependents)</span><br><span class="line">cust_data$PhoneService &lt;- factor(cust_data$PhoneService)</span><br><span class="line">cust_data$MultipleLines &lt;- factor(cust_data$MultipleLines)</span><br><span class="line">cust_data$InternetService &lt;- factor(cust_data$InternetService)</span><br><span class="line">cust_data$OnlineSecurity &lt;- factor(cust_data$OnlineSecurity)</span><br><span class="line">cust_data$OnlineBackup &lt;- factor(cust_data$OnlineBackup)</span><br><span class="line">cust_data$DeviceProtection &lt;- factor(cust_data$DeviceProtection)</span><br><span class="line">cust_data$TechSupport &lt;- factor(cust_data$TechSupport)</span><br><span class="line">cust_data$StreamingTV &lt;- factor(cust_data$StreamingTV)</span><br><span class="line">cust_data$StreamingMovies &lt;- factor(cust_data$StreamingMovies)</span><br><span class="line">cust_data$Contract &lt;- factor(cust_data$Contract)</span><br><span class="line">cust_data$PaperlessBilling &lt;- factor(cust_data$PaperlessBilling)</span><br><span class="line">cust_data$PaymentMethod &lt;- factor(cust_data$PaymentMethod)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">#build the model</span><br><span class="line">#80:20 ratio for training and testing parts</span><br><span class="line">set.seed(12345)</span><br><span class="line">cust_data_rand &lt;- cust_data[order(runif(7043)),]</span><br><span class="line">training &lt;- cust_data_rand[1:5635,]</span><br><span class="line">testing &lt;- cust_data_rand[5636:7043,]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"># SVM</span><br><span class="line"># reminder: &#39;cost&#39; quantifies the penalty associated with having an observation on the wrong side of the classification boundary</span><br><span class="line"># instead of specifying a cost up front, we can use the tune function to est various costs and identify which value produces the best fitting model</span><br><span class="line">set.seed(1234)</span><br><span class="line">svm &lt;- tune.svm(Churn ~ ., data &#x3D; training, seq(0.5, 0.9, by &#x3D; 0.1), cost &#x3D; seq(100, 1000, by &#x3D; 100), kernel &#x3D; &quot;radial&quot;, tunecontrol &#x3D; tune.control(cross &#x3D; 10))</span><br><span class="line"></span><br><span class="line">print(svm)</span><br><span class="line">summary(svm)</span><br><span class="line">svm$performances</span><br><span class="line"></span><br><span class="line"># find the best SVM model</span><br><span class="line">svmfit &lt;- svm$best.model</span><br></pre></td></tr></table></figure>


    </div>

    
    
    
        

<div>
<ul class="post-copyright">
  <li class="post-copyright-author">
    <strong>本文作者： </strong>sasegoi
  </li>
  <li class="post-copyright-link">
    <strong>本文链接：</strong>
    <a href="https://sasegoi.github.io.git/2020/04/04/Machine-Learning-Lecture-Review/" title="机器学习课程复习1">https://sasegoi.github.io.git/2020/04/04/Machine-Learning-Lecture-Review/</a>
  </li>
  <li class="post-copyright-license">
    <strong>版权声明： </strong>本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/4.0/" rel="noopener" target="_blank"><i class="fa fa-fw fa-creative-commons"></i>BY-NC-SA</a> 许可协议。转载请注明出处！
  </li>
</ul>
</div>


      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/04/03/Markdown-Summary/" rel="prev" title="Markdown语法总结">
      <i class="fa fa-chevron-left"></i> Markdown语法总结
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#机器学习课程复习1"><span class="nav-number">1.</span> <span class="nav-text">机器学习课程复习1</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Lesson-1"><span class="nav-number">1.1.</span> <span class="nav-text">Lesson 1</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Machine-Learning-Approaches"><span class="nav-number">1.1.1.</span> <span class="nav-text">Machine Learning Approaches</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lesson-2"><span class="nav-number">1.2.</span> <span class="nav-text">Lesson 2</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#K-Nearest-Neighbours"><span class="nav-number">1.2.1.</span> <span class="nav-text">K-Nearest Neighbours</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Distance-Measures"><span class="nav-number">1.2.2.</span> <span class="nav-text">Distance Measures</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Appropriate-k"><span class="nav-number">1.2.3.</span> <span class="nav-text">Appropriate k</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Data-Preparation"><span class="nav-number">1.2.4.</span> <span class="nav-text">Data Preparation</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Preparing-data-for-kNN"><span class="nav-number">1.2.4.1.</span> <span class="nav-text">Preparing data for kNN</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Confusion-matrix"><span class="nav-number">1.2.4.2.</span> <span class="nav-text">Confusion matrix</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Naive-Bayes"><span class="nav-number">1.2.5.</span> <span class="nav-text">Naive Bayes</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Laplace-estimator"><span class="nav-number">1.2.5.1.</span> <span class="nav-text">Laplace estimator</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Numerical-features"><span class="nav-number">1.2.5.2.</span> <span class="nav-text">Numerical features</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lesson-3"><span class="nav-number">1.3.</span> <span class="nav-text">Lesson 3</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Decision-Trees"><span class="nav-number">1.3.1.</span> <span class="nav-text">Decision Trees</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Weaknesses"><span class="nav-number">1.3.1.1.</span> <span class="nav-text">Weaknesses</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Recursive-Partitioning-divide-and-conquer"><span class="nav-number">1.3.2.</span> <span class="nav-text">Recursive Partitioning - divide and conquer</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#The-best-split"><span class="nav-number">1.3.2.1.</span> <span class="nav-text">The best split</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Pruning-the-decision-tree"><span class="nav-number">1.3.2.2.</span> <span class="nav-text">Pruning the decision tree</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#One-Rule-Algorithm"><span class="nav-number">1.3.3.</span> <span class="nav-text">One Rule Algorithm</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#PIPPER-Algorithm"><span class="nav-number">1.3.4.</span> <span class="nav-text">PIPPER Algorithm</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Lesson-4"><span class="nav-number">1.4.</span> <span class="nav-text">Lesson 4</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Support-Vector-Machine-SVM"><span class="nav-number">1.4.1.</span> <span class="nav-text">Support Vector Machine (SVM)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Linearly-Separable-Data"><span class="nav-number">1.4.2.</span> <span class="nav-text">Linearly Separable Data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Non-linearly-Separable-Data"><span class="nav-number">1.4.3.</span> <span class="nav-text">Non-linearly Separable Data</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Kernels-for-Non-linear-Space"><span class="nav-number">1.4.4.</span> <span class="nav-text">Kernels for Non-linear Space</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">sasegoi</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        
  <div class="beian"><a href="http://www.beian.miit.gov.cn/" rel="noopener" target="_blank">null </a>
  </div>

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">sasegoi</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="站点总字数">31k</span>
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-coffee"></i>
    </span>
    <span title="站点阅读时长">28 分钟</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Gemini</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script><!-- hexo-inject:begin --><!-- hexo-inject:end -->













  

  

  

</body>
</html>
