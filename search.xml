<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>机器学习课程复习1</title>
    <url>/2020/04/04/Machine-Learning-Lecture-Review/</url>
    <content><![CDATA[<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h1 id="机器学习课程复习1"><a href="#机器学习课程复习1" class="headerlink" title="机器学习课程复习1"></a>机器学习课程复习1</h1><h2 id="Lesson-1"><a href="#Lesson-1" class="headerlink" title="Lesson 1"></a>Lesson 1</h2><h3 id="Machine-Learning-Approaches"><a href="#Machine-Learning-Approaches" class="headerlink" title="Machine Learning Approaches"></a>Machine Learning Approaches</h3><ul>
<li>Supervised Learning<ul>
<li>predictive models</li>
<li>classification</li>
<li>KNN, naive Bayes, decision trees, linear regressions, neural networks, SVM <a id="more"></a></li>
</ul>
</li>
<li>Unsupervised Learning<ul>
<li>descriptive models</li>
<li>no labels upfront</li>
<li>clustering, association rules, dimension reduction</li>
</ul>
</li>
</ul>
<p>监督式学习：能够通过训练样本集或专家知识构建已知且确定的判定函数，并根据训练集和该判定函数形成模型改进策略，对模型参数进行不断改进，完成模型学习的过程称为监督式学习；如果无法从训练样本集或专家知识构建确定的判定函数，而通过训练集与一给定的判定函数进行模型参数不断改进，完成学习的过程称为非监督式学习。</p>
<h2 id="Lesson-2"><a href="#Lesson-2" class="headerlink" title="Lesson 2"></a>Lesson 2</h2><h3 id="K-Nearest-Neighbours"><a href="#K-Nearest-Neighbours" class="headerlink" title="K-Nearest Neighbours"></a>K-Nearest Neighbours</h3><ul>
<li>Classification using nearest neighbors<ul>
<li>Classifying unlabeled samples<ul>
<li>assign them the class of the most similar labeled examples</li>
</ul>
</li>
<li>Applications<ul>
<li>facial recognition, recommendation systems, detecting diseases</li>
</ul>
</li>
<li>Why lazy?<ul>
<li>there is no abstraction</li>
<li>rote learning, instance-based learning, non-parametric learning</li>
</ul>
</li>
<li>Adequate for<ul>
<li>numerous and sophisticated patterns</li>
<li>homogenous classes (inter/intraclass (dis)similarity)</li>
</ul>
</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">uciurl &lt;- &quot;https:&#x2F;&#x2F;archive.ics.uci.edu&#x2F;ml&#x2F;machine-learning-databases&#x2F;breast-cancer-wisconsin&#x2F;wdbc.data&quot;</span><br><span class="line">download.file(url&#x3D;uciurl, destfile&#x3D;&quot;wdbc.data&quot;, method&#x3D;&quot;curl&quot;)</span><br><span class="line">wdbc &lt;- read.csv(&quot;wdbc.data&quot;, header&#x3D;FALSE, stringsAsFactors&#x3D;FALSE)[-1]</span><br><span class="line">wdbc &lt;- wdbc[sample(nrow(wdbc)),]</span><br><span class="line">features &lt;- c(&quot;radius&quot;, &quot;texture&quot;, &quot;perimeter&quot;, &quot;area&quot;, &quot;smoothness&quot;, </span><br><span class="line">              &quot;compactness&quot;, &quot;concavity&quot;, &quot;concave_points&quot;, &quot;symmetry&quot;,</span><br><span class="line">              &quot;fractal_dimension&quot;)</span><br><span class="line">calcs &lt;- c(&quot;mean&quot;, &quot;se&quot;, &quot;worst&quot;)</span><br><span class="line">colnames(wdbc) &lt;- c(&quot;diagnosis&quot;,</span><br><span class="line">                    paste0(rep(features, 3), &quot;_&quot;, rep(calcs, each&#x3D;10)))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#inspect the dataset</span><br><span class="line">names(wdbc)</span><br><span class="line">str(wdbc)</span><br><span class="line">table(wdbc$diagnosis)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#assign labels and recode the diagnosis variable</span><br><span class="line">wdbc$diagnosis &lt;- factor(wdbc$diagnosis,levels &#x3D; c(&quot;B&quot;,&quot;M&quot;),</span><br><span class="line">                         labels&#x3D;c(&quot;Beningn&quot;,&quot;Malinagnt&quot;))</span><br><span class="line">round(prop.table(table(wdbc$diagnosis))*100,digits &#x3D; 1)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#see the summary table</span><br><span class="line">summary(wdbc)</span><br><span class="line">summary(wdbc$area_mean)</span><br><span class="line">summary(wdbc$smoothness_mean)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#see the range of various variables</span><br><span class="line">#as for now, the impact of area would be much larger than the smoothness in the distance calculation</span><br><span class="line">#we have to normalize the data</span><br><span class="line">normalize &lt;- function(x)&#123;</span><br><span class="line">  return((x-min(x))&#x2F;(max(x)-min(x)))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#example of normalization</span><br><span class="line">normalize(c(1,2,3,4,5))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#lapply function os for taking a list and applying a specified function to each list element</span><br><span class="line">#data frame is a list of equal-length vectors</span><br><span class="line">#let&#39;s use lapply to apply normalization to each feature in the data frame</span><br><span class="line">#convert the output to a data frame thanks to as.data.frame function</span><br><span class="line">wdbc_n &lt;- as.data.frame(lapply(wdbc[2:31],normalize))</span><br><span class="line"></span><br><span class="line">#check the transformation effect</span><br><span class="line">summary(wdbc_n$area_mean)</span><br><span class="line"></span><br><span class="line">#now the feature ranges from 0 to 1</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#divide data into training and test datasets</span><br><span class="line">#training dataset is used to build the kNN model</span><br><span class="line">#test dataset is used to estimate the predictive accuracy of the model</span><br><span class="line">wdbc_train &lt;- wdbc_n[1:469,]</span><br><span class="line">wdbc_test &lt;- wdbc_n[470:569,]</span><br><span class="line"></span><br><span class="line">#remember that the diagnosis is our target variable</span><br><span class="line">#let&#39;s store class labels in factor vectors across our datasets</span><br><span class="line">wdbc_train_labels &lt;- wdbc[1:469,1]</span><br><span class="line">wdbc_test_labels &lt;- wdbc[470:569,1]</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#now we start training our model on the data applied</span><br><span class="line">#we use kNN from the class package</span><br><span class="line">library(class)</span><br><span class="line"></span><br><span class="line">#Euclidean distance is a baseline choice</span><br><span class="line">#we have to specify the k number</span><br><span class="line">#let&#39;s try with k&#x3D;21 as it is close to the square root of 469</span><br><span class="line">wdbc_test_pred &lt;- knn(train &#x3D; wdbc_train, test &#x3D; wdbc_test, </span><br><span class="line">                      cl &#x3D; wdbc_train_labels, k&#x3D;21)</span><br><span class="line"></span><br><span class="line">#evaluate model performance</span><br><span class="line">#how well the predicted classes match up with the known values?</span><br><span class="line">install.packages(&quot;gmodels&quot;)</span><br><span class="line">library(gmodels)</span><br><span class="line">CrossTable(x&#x3D;wdbc_test_labels, y&#x3D;wdbc_test_pred,</span><br><span class="line">           prop.chisq &#x3D; FALSE)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">wdbc_z &lt;- as.data.frame(scale(wdbc[-1]))</span><br><span class="line">summary(wdbc_z$area_mean)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#the mean after the z-score stand should always be 0</span><br><span class="line">#z-score values larger than 3 or less than -3 suggest extremely rare value</span><br><span class="line">#try to do the prediction once again</span><br><span class="line">wdbc_train &lt;- wdbc_z[1:469,]</span><br><span class="line">wdbc_test &lt;- wdbc_z[470:569,]</span><br><span class="line">wdbc_train_labels &lt;- wdbc[1:469,1]</span><br><span class="line">wdbc_test_labels &lt;- wdbc[470:569,1]</span><br><span class="line">wdbc_test_pred &lt;- knn(train &#x3D; wdbc_train, test &#x3D; wdbc_test,</span><br><span class="line">                      cl &#x3D; wdbc_train_labels, k&#x3D;21)</span><br><span class="line">CrossTable(x &#x3D; wdbc_test_labels, wdbc_test_pred,</span><br><span class="line">           prop.chisq &#x3D; FALSE)</span><br></pre></td></tr></table></figure>

<h3 id="Distance-Measures"><a href="#Distance-Measures" class="headerlink" title="Distance Measures"></a>Distance Measures</h3><ul>
<li>definition: fuction of similarity between objects</li>
<li>for kNN</li>
<li><a href="https://www.sciencedirect.com/topics/computer-science/minkowski-distance" target="_blank" rel="noopener">sciencedirect</a></li>
<li>distance measures <ul>
<li>Euclidean</li>
<li>Manhattan</li>
<li>Minkowski</li>
</ul>
</li>
</ul>
<h3 id="Appropriate-k"><a href="#Appropriate-k" class="headerlink" title="Appropriate k"></a>Appropriate k</h3><ul>
<li>under/overfitting, bias-variance tradeoff</li>
<li>large k<ul>
<li>less impact due to noisy data, may ignore small patterns</li>
</ul>
</li>
<li>depends strongly on the data</li>
<li>square root</li>
<li>various options</li>
</ul>
<h3 id="Data-Preparation"><a href="#Data-Preparation" class="headerlink" title="Data Preparation"></a>Data Preparation</h3><h4 id="Preparing-data-for-kNN"><a href="#Preparing-data-for-kNN" class="headerlink" title="Preparing data for kNN"></a>Preparing data for kNN</h4><ul>
<li>What would happen if some features have much larger values than others?</li>
<li>So we have to adjust scaling in our data.<ul>
<li>min-max normalization (0-1)</li>
<li><del>要继续整理Markdown数学公式的插入！</del><br>$X_{new} = \frac{X-min(X)}{max(X)-min(X)}$</li>
</ul>
</li>
<li>z-score standardization</li>
<li>dummy coding</li>
<li>dividing each value by the standard deviation</li>
</ul>
<h4 id="Confusion-matrix"><a href="#Confusion-matrix" class="headerlink" title="Confusion matrix"></a>Confusion matrix</h4><p>| Confusion Matrix | Actual Positives | Actual Negatives |<br>| ;————–; | ;————–; | ;————–; |<br>| Positive Predictions | True Positives (TP) | False Positives (FP) |<br>| Negative Predictions | False Negatives (FN) | True Negatives (TN) |</p>
<p>Accuracy = (TP+TN)/N<br>Precision = TP/(TP+FP)<br>Recall = TP/(TP+FN)<br>Error = (FP+FN)/N<br>F1 = 2*Recall*Precision/(Recall+Precision)<br>TP+FP = |{retrieved elements}|<br>TP = |{relevant elements}⋂{retrieved elements}|</p>
<h3 id="Naive-Bayes"><a href="#Naive-Bayes" class="headerlink" title="Naive Bayes"></a>Naive Bayes</h3><ul>
<li>conditional probability</li>
<li>posterior probability</li>
<li>likelihood</li>
<li>prior probability</li>
<li>marginal likelihood</li>
</ul>
<h4 id="Laplace-estimator"><a href="#Laplace-estimator" class="headerlink" title="Laplace estimator"></a>Laplace estimator</h4><ul>
<li>add a small number to each of the counts in the frequency table</li>
<li>this ensures that each feature has a nonzero probability of occurring with each class</li>
<li>very often, the Laplace estimator is set to 1</li>
</ul>
<h4 id="Numerical-features"><a href="#Numerical-features" class="headerlink" title="Numerical features"></a>Numerical features</h4><ul>
<li>each feature shall be categorical</li>
<li>the preceding algorithm does not deal with numeric data straightforward</li>
<li>How?<ul>
<li>discretize numeric features</li>
<li>look for natural categories</li>
<li>cut points in the data distribution</li>
<li>use quantiles</li>
</ul>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#upload the dataset</span><br><span class="line">uciurl &lt;- &quot;https:&#x2F;&#x2F;archive.ics.uci.edu&#x2F;ml&#x2F;machine-learning-databases&#x2F;voting-records&#x2F;house-votes-84.data&quot;</span><br><span class="line">download.file(url&#x3D;uciurl, destfile&#x3D;&quot;house-votes-84.data&quot;, method&#x3D;&quot;curl&quot;)</span><br><span class="line">hv &lt;- read.csv(&quot;house-votes-84.data&quot;, header&#x3D;FALSE, stringsAsFactors&#x3D;FALSE)</span><br><span class="line"></span><br><span class="line">#inspect the dataset</span><br><span class="line">str(hv)</span><br><span class="line">names(hv)</span><br><span class="line">summary(hv)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#install relevant packages</span><br><span class="line">install.packages(&quot;e1071&quot;)</span><br><span class="line">library(e1071)</span><br><span class="line">install.packages(&quot;mlbench&quot;)</span><br><span class="line">library(mlbench)</span><br><span class="line">library(ggplot2)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#another way of retrieving the data, via the mlbench package</span><br><span class="line">data(&quot;HouseVotes84&quot;)</span><br><span class="line">str(HouseVotes84</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#inspect the dataset</span><br><span class="line">summary(HouseVotes84)</span><br><span class="line">summary(HouseVotes84$Class</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#deal with missing data</span><br><span class="line">head(is.na(HouseVotes84))</span><br><span class="line"></span><br><span class="line">#clean the dataset</span><br><span class="line">CleanDataset &lt;- na.omit(HouseVotes84)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#create a plot </span><br><span class="line">qplot(Class, data&#x3D;CleanDataset, geom &#x3D; &quot;bar&quot;) + theme(axis.text.x &#x3D; element_text(angle &#x3D; 45, hjust &#x3D; 1))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#split the dataset into training and testing ones</span><br><span class="line">summary(CleanDataset)</span><br><span class="line">hv_train &lt;- CleanDataset[1:150,]</span><br><span class="line">hv_test &lt;- CleanDataset[151:232,]</span><br><span class="line">vote_hv_train &lt;- hv_train$Class</span><br><span class="line">vote_hv_test &lt;- hv_test$Class</span><br><span class="line">prop.table(table(vote_hv_train))</span><br><span class="line">prop.table(table(vote_hv_test))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#thanks to the caret package you may also split the dataset using the createDataPartition function, eg. createDataPartition(CleanDataset$Class, p&#x3D;0.80, list &#x3D; FALSE)</span><br><span class="line">#I prefer this way! HAHA</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#train the model using Naive Bayes</span><br><span class="line">vote_classifier &lt;- naiveBayes(hv_train, vote_hv_train)</span><br><span class="line"></span><br><span class="line">#see how it works</span><br><span class="line">vote_test_pred &lt;- predict(vote_classifier, hv_test)</span><br><span class="line">head(vote_test_pred)</span><br><span class="line"></span><br><span class="line">install.packages(&quot;gmodels&quot;)</span><br><span class="line">library(gmodels)</span><br><span class="line">CrossTable(vote_hv_test, vote_test_pred,</span><br><span class="line">           prop.chisq &#x3D; FALSE, prop.t &#x3D; FALSE, prop.r &#x3D; FALSE,</span><br><span class="line">           dnn &#x3D; c(&#39;real&#39;, &#39;predicted&#39;))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#let&#39;s try to improve our model with Laplace estimator</span><br><span class="line">vote_classifier_Laplace &lt;- naiveBayes(hv_train, vote_hv_train, laplace &#x3D; 1)</span><br><span class="line">vote_test_pred_impr &lt;- predict(vote_classifier_Laplace, hv_test)</span><br><span class="line">CrossTable(vote_hv_test, vote_test_pred_impr,</span><br><span class="line">           prop.chisq &#x3D; FALSE, prop.t &#x3D; FALSE, prop.r &#x3D; FALSE,</span><br><span class="line">           dnn &#x3D; c(&#39;real&#39;, &#39;predicted&#39;))</span><br></pre></td></tr></table></figure>

<h2 id="Lesson-3"><a href="#Lesson-3" class="headerlink" title="Lesson 3"></a>Lesson 3</h2><h3 id="Decision-Trees"><a href="#Decision-Trees" class="headerlink" title="Decision Trees"></a>Decision Trees</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#upload the dataset</span><br><span class="line">credit_ger &lt;- read.csv(&quot;credit_ger.csv&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#inspect the dataset</span><br><span class="line">str(credit_ger)</span><br><span class="line"></span><br><span class="line">#we see 1000 obs and 21 features of different data types</span><br><span class="line">#let&#39;s take a closer look at a couple of features of loans</span><br><span class="line">#these features may predict a default</span><br><span class="line">#how about checking and saving account balance?</span><br><span class="line">table(credit_ger$checking_status)</span><br><span class="line">table(credit_ger$savings_status)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#duration of loan and the amount of credit seems to be important as well</span><br><span class="line">summary(credit_ger$duration)</span><br><span class="line">summary(credit_ger$credit_amount)</span><br><span class="line"></span><br><span class="line">#was not the loan applicant able to meet the payment terms?</span><br><span class="line">table(credit_ger$class)</span><br><span class="line"></span><br><span class="line">#thus 700 clients were assigned as good applicants and 300 as bad applicants</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#order our data randomly before splitting </span><br><span class="line">#order function is for arranging our data in ascending or descending order</span><br><span class="line">#runif function is responsible for generating a sequence of random numbers between 0 and 1</span><br><span class="line">#set.seed function is for generating random numbers in a predefined sequence starting from a seed position, here values from 1 to 5</span><br><span class="line">#we are going to create 1000 random numbers as we have 1000 observations</span><br><span class="line">set.seed(12345)</span><br><span class="line">credit_ger_rand &lt;- credit_ger[order(runif(1000)),]</span><br><span class="line"></span><br><span class="line">#check, whether it works</span><br><span class="line">summary(credit_ger$credit_amount)</span><br><span class="line">summary(credit_ger_rand$credit_amount)</span><br><span class="line"></span><br><span class="line">head(credit_ger$credit_amount)</span><br><span class="line">head(credit_ger_rand$credit_amount)</span><br><span class="line"></span><br><span class="line">#ok, so we have our records in a random order</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#split the data in the 1:9 proportion</span><br><span class="line">credit_ger_train &lt;- credit_ger_rand[1:900,]</span><br><span class="line">credit_ger_test &lt;- credit_ger_rand[901:1000,]</span><br><span class="line"></span><br><span class="line">prop.table(table(credit_ger_train$class))</span><br><span class="line">prop.table(table(credit_ger_test$class))</span><br><span class="line"></span><br><span class="line">#the split looks as a fairly equal one</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#train your model</span><br><span class="line">install.packages(&quot;C50&quot;)</span><br><span class="line">library(C50)</span><br><span class="line"></span><br><span class="line">#we skip the 21st column as it refers to the class feature</span><br><span class="line">#class has to be identified as a factor</span><br><span class="line">credit_ger_model &lt;- C5.0(credit_ger_train[-21], credit_ger_train$class)</span><br><span class="line">credit_ger_model</span><br><span class="line">summary(credit_ger_model)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#evaluate the model</span><br><span class="line">credit_ger_predict &lt;- predict(credit_ger_model, credit_ger_test)</span><br><span class="line">install.packages(&quot;gmodels&quot;)</span><br><span class="line">library(gmodels)</span><br><span class="line">CrossTable(credit_ger_test$class, credit_ger_predict,</span><br><span class="line">           prop.chisq &#x3D; FALSE, prop.c &#x3D; FALSE, prop.r &#x3D; FALSE,</span><br><span class="line">           dnn &#x3D; c(&#39;real class&#39;, &#39;predicted class&#39;))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#improve the model</span><br><span class="line">credit_ger_boost10 &lt;- C5.0(credit_ger_train[-21], credit_ger_train$class, trials &#x3D; 10)</span><br><span class="line">credit_ger_boost10</span><br><span class="line">summary(credit_ger_boost10)</span><br><span class="line"></span><br><span class="line">credit_ger_boost10_predict &lt;- predict(credit_ger_boost10, credit_ger_test)</span><br><span class="line">CrossTable(credit_ger_test$class, credit_ger_boost10_predict,</span><br><span class="line">           prop.chisq &#x3D; FALSE, prop.c &#x3D; FALSE, prop.r &#x3D; FALSE,</span><br><span class="line">           dnn &#x3D; c(&#39;real class&#39;, &#39;predicted class&#39;))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#how about checking how costly mistakes are?</span><br><span class="line">#assumption: loan default costs the bank four times as much as a missed opportunity</span><br><span class="line">error_cost &lt;- matrix(c(0, 1, 4, 0), nrow &#x3D; 2)</span><br><span class="line">error_cost</span><br><span class="line"></span><br><span class="line">credit_cost &lt;- C5.0(credit_ger_train[-21], credit_ger_train$class, costs&#x3D;error_cost)</span><br><span class="line">credit_cost_predict &lt;- predict(credit_cost, credit_ger_test)</span><br><span class="line">CrossTable(credit_ger_test$class, credit_cost_predict,</span><br><span class="line">           prop.chisq &#x3D; FALSE, prop.c &#x3D; FALSE, prop.r &#x3D; FALSE,</span><br><span class="line">           dnn &#x3D; c(&#39;real class&#39;, &#39;predicted class&#39;))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">library(rpart)</span><br><span class="line">install.packages(&quot;rpart.plot&quot;)</span><br><span class="line">library(rpart.plot)</span><br><span class="line"></span><br><span class="line">rtree &lt;- rpart(class ~ ., credit_ger_train)</span><br><span class="line">rpart.plot(rtree)</span><br><span class="line">#another approach</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#conditional inference</span><br><span class="line">install.packages(&quot;party&quot;)</span><br><span class="line">library(party)</span><br><span class="line">ctree_ &lt;- ctree(class ~ ., credit_ger_train)</span><br><span class="line">plot(ctree_)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#results and cross-validation</span><br><span class="line">printcp(rtree)</span><br><span class="line">plotcp(rtree)</span><br><span class="line">summary(rtree)</span><br><span class="line"></span><br><span class="line">plot(rtree, uniform&#x3D;TRUE,</span><br><span class="line">     main&#x3D;&quot;Classification Tree for Credits&quot;)</span><br><span class="line">text(rtree, use.n&#x3D;TRUE, all&#x3D;TRUE, cex&#x3D;.8)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#prune the tree and plot the tree then</span><br><span class="line">prtree&lt;- prune(rtree, cp&#x3D;   rtree$cptable[which.min(rtree$cptable[,&quot;xerror&quot;]),&quot;CP&quot;])</span><br><span class="line"></span><br><span class="line">plot(prtree, uniform&#x3D;TRUE,</span><br><span class="line">     main&#x3D;&quot;Pruned Classification Tree for Credits&quot;)</span><br><span class="line">text(prtree, use.n&#x3D;TRUE, all&#x3D;TRUE, cex&#x3D;.8)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#another way</span><br><span class="line">prtree2&lt;- prune(rtree, cp&#x3D;0.008) # from cptable  </span><br><span class="line">plot(prtree2, uniform&#x3D;TRUE,</span><br><span class="line">     main&#x3D;&quot;Pruned Regression Tree for Credits&quot;)</span><br><span class="line">text(prtree2, use.n&#x3D;TRUE, all&#x3D;TRUE, cex&#x3D;.8)</span><br></pre></td></tr></table></figure>

<h4 id="Weaknesses"><a href="#Weaknesses" class="headerlink" title="Weaknesses"></a>Weaknesses</h4><ul>
<li>usually biased toward splits on features with a large number of levels</li>
<li>easy to overfit or underfit the model</li>
<li>some relationships troublesome to model due to reliance on axis-parallel splits</li>
<li>even small changes in training data may be crucial with respect to decision logic</li>
<li>large trees challenging to interpret and sometimes counterintuitive</li>
</ul>
<h3 id="Recursive-Partitioning-divide-and-conquer"><a href="#Recursive-Partitioning-divide-and-conquer" class="headerlink" title="Recursive Partitioning - divide and conquer"></a>Recursive Partitioning - divide and conquer</h3><p>Recursive partitioning is a statistical method for multivariable analysis. Recursive partitioning creates a decision tree that strives to correctly classify members of the population by splitting it into sub-populations based on several dichotomous independent variables. The process is termed recursive because each sub-population may in turn be split an indefinite number of times until the splitting process terminates after a particular stopping criterion is reached.</p>
<h4 id="The-best-split"><a href="#The-best-split" class="headerlink" title="The best split"></a>The best split</h4><p>Entropy of all possible values of x</p>
<h4 id="Pruning-the-decision-tree"><a href="#Pruning-the-decision-tree" class="headerlink" title="Pruning the decision tree"></a>Pruning the decision tree</h4><ul>
<li>too large, decisions may be overly specific (overfitted model)</li>
<li>pre-pruning (early stopping)</li>
<li>post-pruning</li>
</ul>
<h3 id="One-Rule-Algorithm"><a href="#One-Rule-Algorithm" class="headerlink" title="One Rule Algorithm"></a>One Rule Algorithm</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#get the mushroom data</span><br><span class="line">theURL &lt;- &quot;https:&#x2F;&#x2F;archive.ics.uci.edu&#x2F;ml&#x2F;machine-learning-databases&#x2F;mushroom&#x2F;agaricus-lepiota.data&quot;</span><br><span class="line">mushrooms &lt;- read.table(theURL, header &#x3D; FALSE,  sep &#x3D; &quot;,&quot;, stringsAsFactors &#x3D; FALSE)</span><br><span class="line">write.table(mushrooms, file &#x3D; &quot;mushroom.csv&quot;, sep &#x3D; &quot;,&quot;, col.names &#x3D; FALSE)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#tidy the data</span><br><span class="line">colnames(mushrooms) &lt;- c(&quot;edibility&quot;, &quot;cap_shape&quot;, &quot;cap_surface&quot;, </span><br><span class="line">                        &quot;cap_color&quot;, &quot;bruises&quot;, &quot;odor&quot;, </span><br><span class="line">                        &quot;gill_attachement&quot;, &quot;gill_spacing&quot;, &quot;gill_size&quot;, </span><br><span class="line">                        &quot;gill_color&quot;, &quot;stalk_shape&quot;, &quot;stalk_root&quot;, </span><br><span class="line">                        &quot;stalk_surface_above_ring&quot;, &quot;stalk_surface_below_ring&quot;, &quot;stalk_color_above_ring&quot;, </span><br><span class="line">                        &quot;stalk_color_below_ring&quot;, &quot;veil_type&quot;, &quot;veil_color&quot;, </span><br><span class="line">                        &quot;ring_number&quot;, &quot;ring_type&quot;, &quot;spore_print_color&quot;, </span><br><span class="line">                        &quot;population&quot;, &quot;habitat&quot;)</span><br><span class="line"></span><br><span class="line">install.packages(&quot;tidyverse&quot;)</span><br><span class="line">library(tidyverse)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#treat features as factors</span><br><span class="line">mushrooms &lt;- mushrooms %&gt;% map_df(function(.x) as.factor(.x))</span><br><span class="line"></span><br><span class="line">#redefine each of the category</span><br><span class="line">levels(mushrooms$edibility) &lt;- c(&quot;edible&quot;, &quot;poisonous&quot;)</span><br><span class="line">levels(mushrooms$cap_shape) &lt;- c(&quot;bell&quot;, &quot;conical&quot;, &quot;flat&quot;, &quot;knobbed&quot;, &quot;sunken&quot;, &quot;convex&quot;)</span><br><span class="line">levels(mushrooms$cap_color) &lt;- c(&quot;buff&quot;, &quot;cinnamon&quot;, &quot;red&quot;, &quot;gray&quot;, &quot;brown&quot;, &quot;pink&quot;, </span><br><span class="line">                                &quot;green&quot;, &quot;purple&quot;, &quot;white&quot;, &quot;yellow&quot;)</span><br><span class="line">levels(mushrooms$cap_surface) &lt;- c(&quot;fibrous&quot;, &quot;grooves&quot;, &quot;scaly&quot;, &quot;smooth&quot;)</span><br><span class="line">levels(mushrooms$bruises) &lt;- c(&quot;no&quot;, &quot;yes&quot;)</span><br><span class="line">levels(mushrooms$odor) &lt;- c(&quot;almond&quot;, &quot;creosote&quot;, &quot;foul&quot;, &quot;anise&quot;, &quot;musty&quot;, &quot;none&quot;, &quot;pungent&quot;, &quot;spicy&quot;, &quot;fishy&quot;)</span><br><span class="line">levels(mushrooms$gill_attachement) &lt;- c(&quot;attached&quot;, &quot;free&quot;)</span><br><span class="line">levels(mushrooms$gill_spacing) &lt;- c(&quot;close&quot;, &quot;crowded&quot;)</span><br><span class="line">levels(mushrooms$gill_size) &lt;- c(&quot;broad&quot;, &quot;narrow&quot;)</span><br><span class="line">levels(mushrooms$gill_color) &lt;- c(&quot;buff&quot;, &quot;red&quot;, &quot;gray&quot;, &quot;chocolate&quot;, &quot;black&quot;, &quot;brown&quot;, &quot;orange&quot;, </span><br><span class="line">                                 &quot;pink&quot;, &quot;green&quot;, &quot;purple&quot;, &quot;white&quot;, &quot;yellow&quot;)</span><br><span class="line">levels(mushrooms$stalk_shape) &lt;- c(&quot;enlarging&quot;, &quot;tapering&quot;)</span><br><span class="line">levels(mushrooms$stalk_root) &lt;- c(&quot;missing&quot;, &quot;bulbous&quot;, &quot;club&quot;, &quot;equal&quot;, &quot;rooted&quot;)</span><br><span class="line">levels(mushrooms$stalk_surface_above_ring) &lt;- c(&quot;fibrous&quot;, &quot;silky&quot;, &quot;smooth&quot;, &quot;scaly&quot;)</span><br><span class="line">levels(mushrooms$stalk_surface_below_ring) &lt;- c(&quot;fibrous&quot;, &quot;silky&quot;, &quot;smooth&quot;, &quot;scaly&quot;)</span><br><span class="line">levels(mushrooms$stalk_color_above_ring) &lt;- c(&quot;buff&quot;, &quot;cinnamon&quot;, &quot;red&quot;, &quot;gray&quot;, &quot;brown&quot;, &quot;pink&quot;, </span><br><span class="line">                                             &quot;green&quot;, &quot;purple&quot;, &quot;white&quot;, &quot;yellow&quot;)</span><br><span class="line">levels(mushrooms$stalk_color_below_ring) &lt;- c(&quot;buff&quot;, &quot;cinnamon&quot;, &quot;red&quot;, &quot;gray&quot;, &quot;brown&quot;, &quot;pink&quot;, </span><br><span class="line">                                             &quot;green&quot;, &quot;purple&quot;, &quot;white&quot;, &quot;yellow&quot;)</span><br><span class="line">levels(mushrooms$veil_type) &lt;- &quot;partial&quot;</span><br><span class="line">levels(mushrooms$veil_color) &lt;- c(&quot;brown&quot;, &quot;orange&quot;, &quot;white&quot;, &quot;yellow&quot;)</span><br><span class="line">levels(mushrooms$ring_number) &lt;- c(&quot;none&quot;, &quot;one&quot;, &quot;two&quot;)</span><br><span class="line">levels(mushrooms$ring_type) &lt;- c(&quot;evanescent&quot;, &quot;flaring&quot;, &quot;large&quot;, &quot;none&quot;, &quot;pendant&quot;)</span><br><span class="line">levels(mushrooms$spore_print_color) &lt;- c(&quot;buff&quot;, &quot;chocolate&quot;, &quot;black&quot;, &quot;brown&quot;, &quot;orange&quot;, </span><br><span class="line">                                        &quot;green&quot;, &quot;purple&quot;, &quot;white&quot;, &quot;yellow&quot;)</span><br><span class="line">levels(mushrooms$population) &lt;- c(&quot;abundant&quot;, &quot;clustered&quot;, &quot;numerous&quot;, &quot;scattered&quot;, &quot;several&quot;, &quot;solitary&quot;)</span><br><span class="line">levels(mushrooms$habitat) &lt;- c(&quot;wood&quot;, &quot;grasses&quot;, &quot;leaves&quot;, &quot;meadows&quot;, &quot;paths&quot;, &quot;urban&quot;, &quot;waste&quot;)</span><br><span class="line"></span><br><span class="line">#inspect changes</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#veil type may be problematic as it has only 1 level</span><br><span class="line">mushrooms &lt;- mushrooms %&gt;% select(- veil_type)</span><br><span class="line"></span><br><span class="line">#look into the data</span><br><span class="line">table(mushrooms$edibility)</span><br><span class="line"></span><br><span class="line">#almost 50:50 proportions</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#train your model on the data</span><br><span class="line">install.packages(&quot;OneR&quot;)</span><br><span class="line">library(OneR)</span><br><span class="line">mushrooms_1R &lt;- OneR(edibility ~., data&#x3D;mushrooms)</span><br><span class="line">mushrooms_1R</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">summary(mushrooms_1R)</span><br></pre></td></tr></table></figure>

<h3 id="PIPPER-Algorithm"><a href="#PIPPER-Algorithm" class="headerlink" title="PIPPER Algorithm"></a>PIPPER Algorithm</h3><ul>
<li>IREP (Incremental Reduced Error  Pruning Algorithm)</li>
<li>PIPPER (Repeated Incremental Pruning to Produce Error Reduction Alogorithm)</li>
</ul>
<h2 id="Lesson-4"><a href="#Lesson-4" class="headerlink" title="Lesson 4"></a>Lesson 4</h2><h3 id="Support-Vector-Machine-SVM"><a href="#Support-Vector-Machine-SVM" class="headerlink" title="Support Vector Machine (SVM)"></a>Support Vector Machine (SVM)</h3><ul>
<li>SVM combines<ul>
<li>Aspects of instance-based kNN</li>
<li>Linear regression modelling</li>
</ul>
</li>
<li>choose Maximum Margin (Maximum Margin Hyperplane)</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># set pseudorandom number generator</span><br><span class="line">set.seed(10)</span><br><span class="line"></span><br><span class="line"># load the relevant packages</span><br><span class="line">install.packages(&quot;tidyverse&quot;)</span><br><span class="line">library(tidyverse)    # data manipulation and visualization</span><br><span class="line">install.packages(&quot;kernlab&quot;)</span><br><span class="line">library(kernlab)      # SVM methodology</span><br><span class="line">install.packages(&quot;e1071&quot;)</span><br><span class="line">library(e1071)        # SVM methodology</span><br><span class="line">install.packages(&quot;ISLR&quot;)</span><br><span class="line">library(ISLR)         # contains example data set &quot;Khan&quot;</span><br><span class="line">install.packages(&quot;RColorBrewer&quot;)</span><br><span class="line">library(RColorBrewer) # customized coloring of plots</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># generate 40 random obs in 2 classes</span><br><span class="line"># use Maximal Margin Classifier</span><br><span class="line"># construct sample data set - completely separated</span><br><span class="line">x &lt;- matrix(rnorm(20*2), ncol &#x3D; 2)</span><br><span class="line">y &lt;- c(rep(-1,10), rep(1,10))</span><br><span class="line">x[y&#x3D;&#x3D;1,] &lt;- x[y&#x3D;&#x3D;1,] + 3&#x2F;2</span><br><span class="line">dat &lt;- data.frame(x&#x3D;x, y&#x3D;as.factor(y))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># plot data</span><br><span class="line">ggplot(data &#x3D; dat, aes(x &#x3D; x.2, y &#x3D; x.1, color &#x3D; y, shape &#x3D; y)) + </span><br><span class="line">  geom_point(size &#x3D; 2) +</span><br><span class="line">  scale_color_manual(values&#x3D;c(&quot;#000000&quot;, &quot;#FF0000&quot;)) +</span><br><span class="line">  theme(legend.position &#x3D; &quot;none&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># where is the boundary?</span><br><span class="line"># fit Support Vector Machine model to data set</span><br><span class="line">library(e1071) </span><br><span class="line">svmfit &lt;- svm(y~., data &#x3D; dat, kernel &#x3D; &quot;linear&quot;, scale &#x3D; FALSE)</span><br><span class="line"># plot Results</span><br><span class="line">plot(svmfit, dat)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># another way of doing the same</span><br><span class="line"># fit model and produce plot</span><br><span class="line">library(kernlab)   </span><br><span class="line">kernfit &lt;- ksvm(x, y, type &#x3D; &quot;C-svc&quot;, kernel &#x3D; &#39;vanilladot&#39;)</span><br><span class="line">plot(kernfit, data &#x3D; x)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># in reality classes may be mixed</span><br><span class="line"># create another sample data</span><br><span class="line"># construct sample data set - not completely separated</span><br><span class="line">x &lt;- matrix(rnorm(20*2), ncol &#x3D; 2)</span><br><span class="line">y &lt;- c(rep(-1,10), rep(1,10))</span><br><span class="line">x[y&#x3D;&#x3D;1,] &lt;- x[y&#x3D;&#x3D;1,] + 1</span><br><span class="line">dat &lt;- data.frame(x&#x3D;x, y&#x3D;as.factor(y))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># plot data set</span><br><span class="line">ggplot(data &#x3D; dat, aes(x &#x3D; x.2, y &#x3D; x.1, color &#x3D; y, shape &#x3D; y)) + </span><br><span class="line">  geom_point(size &#x3D; 2) +</span><br><span class="line">  scale_color_manual(values&#x3D;c(&quot;#000000&quot;, &quot;#FF0000&quot;)) +</span><br><span class="line">  theme(legend.position &#x3D; &quot;none&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># same approach like in previous example</span><br><span class="line"># fit Support Vector Machine model to data set</span><br><span class="line">svmfit &lt;- svm(y~., data &#x3D; dat, kernel &#x3D; &quot;linear&quot;, cost &#x3D; 10)</span><br><span class="line"># plot results</span><br><span class="line">plot(svmfit, dat)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># fit Support Vector Machine model to data set</span><br><span class="line">kernfit &lt;- ksvm(x,y, type &#x3D; &quot;C-svc&quot;, kernel &#x3D; &#39;vanilladot&#39;, C &#x3D; 100)</span><br><span class="line"># plot results</span><br><span class="line">plot(kernfit, data &#x3D; x)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># what is the scope of misclassification?</span><br><span class="line"># find optimal cost of misclassification</span><br><span class="line">tune.out &lt;- tune(svm, y~., data &#x3D; dat, kernel &#x3D; &quot;linear&quot;,</span><br><span class="line">                 ranges &#x3D; list(cost &#x3D; c(0.001, 0.01, 0.1, 1, 5, 10, 100)))</span><br><span class="line"># extract the best model</span><br><span class="line">(bestmod &lt;- tune.out$best.model)</span><br><span class="line"></span><br><span class="line"># create a table of misclassified observations</span><br><span class="line">ypred &lt;- predict(bestmod, dat)</span><br><span class="line">(misclass &lt;- table(predict &#x3D; ypred, truth &#x3D; dat$y))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># deal with a larger dataset!</span><br><span class="line">x &lt;- matrix(rnorm(200*2), ncol &#x3D; 2)</span><br><span class="line">x[1:100,] &lt;- x[1:100,] + 2.5</span><br><span class="line">x[101:150,] &lt;- x[101:150,] - 2.5</span><br><span class="line">y &lt;- c(rep(1,150), rep(2,50))</span><br><span class="line">dat &lt;- data.frame(x&#x3D;x,y&#x3D;as.factor(y))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># make a plot</span><br><span class="line">ggplot(data &#x3D; dat, aes(x &#x3D; x.2, y &#x3D; x.1, color &#x3D; y, shape &#x3D; y)) + </span><br><span class="line">  geom_point(size &#x3D; 2) +</span><br><span class="line">  scale_color_manual(values&#x3D;c(&quot;#000000&quot;, &quot;#FF0000&quot;)) +</span><br><span class="line">  theme(legend.position &#x3D; &quot;none&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># data is not linearly separable</span><br><span class="line"># set pseudorandom number generator</span><br><span class="line">set.seed(123)</span><br><span class="line"># sample training data and fit model</span><br><span class="line"># use 100 random objects for developing the boundary</span><br><span class="line">train &lt;- base::sample(200,100, replace &#x3D; FALSE)</span><br><span class="line">svmfit &lt;- svm(y~., data &#x3D; dat[train,], kernel &#x3D; &quot;radial&quot;, gamma &#x3D; 1, cost &#x3D; 1)</span><br><span class="line"># plot classifier</span><br><span class="line">plot(svmfit, dat)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># another approach</span><br><span class="line"># fit radial-based SVM in kernlab</span><br><span class="line">kernfit &lt;- ksvm(x[train,],y[train], type &#x3D; &quot;C-svc&quot;, kernel &#x3D; &#39;rbfdot&#39;, C &#x3D; 1, scaled &#x3D; c())</span><br><span class="line"># plot training data</span><br><span class="line">plot(kernfit, data &#x3D; x[train,])</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># try with another parameters to fit nonlinear boundaries</span><br><span class="line"># tune model to find optimal cost, gamma values</span><br><span class="line">tune.out &lt;- tune(svm, y~., data &#x3D; dat[train,], kernel &#x3D; &quot;radial&quot;,</span><br><span class="line">                 ranges &#x3D; list(cost &#x3D; c(0.1,1,10,100,1000),</span><br><span class="line">                               gamma &#x3D; c(0.5,1,2,3,4)))</span><br><span class="line"># show best model</span><br><span class="line">tune.out$best.model</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># cost of 1 and gamma of 0.5 seem to be adequate (it reduces error in the best manner)</span><br><span class="line"># evaluate the model</span><br><span class="line">(valid &lt;- table(true &#x3D; dat[-train,&quot;y&quot;], pred &#x3D; predict(tune.out$best.model,</span><br><span class="line">                                                       newx &#x3D; dat[-train,])))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">x &lt;- rbind(x, matrix(rnorm(50*2), ncol &#x3D; 2))</span><br><span class="line">y &lt;- c(y, rep(0,50))</span><br><span class="line">x[y&#x3D;&#x3D;0,2] &lt;- x[y&#x3D;&#x3D;0,2] + 2.5</span><br><span class="line">dat &lt;- data.frame(x&#x3D;x, y&#x3D;as.factor(y))</span><br><span class="line"># plot data set</span><br><span class="line">ggplot(data &#x3D; dat, aes(x &#x3D; x.2, y &#x3D; x.1, color &#x3D; y, shape &#x3D; y)) + </span><br><span class="line">  geom_point(size &#x3D; 2) +</span><br><span class="line">  scale_color_manual(values&#x3D;c(&quot;#000000&quot;,&quot;#FF0000&quot;,&quot;#00BA00&quot;)) +</span><br><span class="line">  theme(legend.position &#x3D; &quot;none&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># fit model</span><br><span class="line">svmfit &lt;- svm(y~., data &#x3D; dat, kernel &#x3D; &quot;radial&quot;, cost &#x3D; 10, gamma &#x3D; 1)</span><br><span class="line"># plot results</span><br><span class="line">plot(svmfit, dat)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># construct table</span><br><span class="line">ypred &lt;- predict(svmfit, dat)</span><br><span class="line">(misclass &lt;- table(predict &#x3D; ypred, truth &#x3D; dat$y))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># fit and plot</span><br><span class="line">kernfit &lt;- ksvm(as.matrix(dat[,2:1]),dat$y, type &#x3D; &quot;C-svc&quot;, kernel &#x3D; &#39;rbfdot&#39;, </span><br><span class="line">                C &#x3D; 100, scaled &#x3D; c())</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># create a fine grid of the feature space</span><br><span class="line">x.1 &lt;- seq(from &#x3D; min(dat$x.1), to &#x3D; max(dat$x.1), length &#x3D; 100)</span><br><span class="line">x.2 &lt;- seq(from &#x3D; min(dat$x.2), to &#x3D; max(dat$x.2), length &#x3D; 100)</span><br><span class="line">x.grid &lt;- expand.grid(x.2, x.1)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># get class predictions over grid</span><br><span class="line">pred &lt;- predict(kernfit, newdata &#x3D; x.grid)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># plot the results</span><br><span class="line">library(RColorBrewer)</span><br><span class="line">cols &lt;- brewer.pal(3, &quot;Set1&quot;)</span><br><span class="line">plot(x.grid, pch &#x3D; 19, col &#x3D; adjustcolor(cols[pred], alpha.f &#x3D; 0.05))</span><br><span class="line"></span><br><span class="line">classes &lt;- matrix(pred, nrow &#x3D; 100, ncol &#x3D; 100)</span><br><span class="line">contour(x &#x3D; x.2, y &#x3D; x.1, z &#x3D; classes, levels &#x3D; 1:3, labels &#x3D; &quot;&quot;, add &#x3D; TRUE)</span><br><span class="line"></span><br><span class="line">points(dat[, 2:1], pch &#x3D; 19, col &#x3D; cols[predict(kernfit)])</span><br></pre></td></tr></table></figure>

<h3 id="Linearly-Separable-Data"><a href="#Linearly-Separable-Data" class="headerlink" title="Linearly Separable Data"></a>Linearly Separable Data</h3><h3 id="Non-linearly-Separable-Data"><a href="#Non-linearly-Separable-Data" class="headerlink" title="Non-linearly Separable Data"></a>Non-linearly Separable Data</h3><h3 id="Kernels-for-Non-linear-Space"><a href="#Kernels-for-Non-linear-Space" class="headerlink" title="Kernels for Non-linear Space"></a>Kernels for Non-linear Space</h3><p>SVMs are able to map a problem into demension space thanks to so-called kernel trick.</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#example from the handbook - Lantz, 2013</span><br><span class="line">#upload the data</span><br><span class="line">letters &lt;- read.csv(&quot;letterdata.csv&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#inspect the dataset</span><br><span class="line">str(letters)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#the dataset is normalized</span><br><span class="line">#we have 20k observations</span><br><span class="line">#let&#39;s use 80% of the data for training and 20% for testing</span><br><span class="line">letters_train &lt;- letters[1:16000, ]</span><br><span class="line">letters_test &lt;- letters[16001:20000, ]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#start with linear SVM (kernlab package)</span><br><span class="line">#vanilladot stands for the linear kernel</span><br><span class="line">install.packages(&quot;kernlab&quot;)</span><br><span class="line">library(kernlab)</span><br><span class="line">letter_classifier &lt;- ksvm(class ~ ., data &#x3D; letters_train, kernel &#x3D; &quot;vanilladot&quot;)</span><br><span class="line">letter_classifier</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#evaluate the model</span><br><span class="line">letter_predictions &lt;- predict(letter_classifier, letters_test)</span><br><span class="line"></span><br><span class="line">#what letter it may be?</span><br><span class="line">head(letter_predictions)</span><br><span class="line"></span><br><span class="line">#how it works?</span><br><span class="line">table(letter_predictions, letters_test$class)</span><br><span class="line"></span><br><span class="line">#diagonal values show the total number of records where the predicted letter matches the true value</span><br><span class="line">#no of mistakes lited as well (like A misidentified as 1 one time)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#compute general accuracy</span><br><span class="line">agreement &lt;- letter_predictions &#x3D;&#x3D; letters_test$class</span><br><span class="line">table(agreement)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#true label indicates how many times a letter was classified properly</span><br><span class="line">#go for fraction of the dataset correctly classified</span><br><span class="line">prop.table(table(agreement))</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#improve the model!</span><br><span class="line">#use more sophisticated kernel function, like Gaussian RBF</span><br><span class="line">letter_classifier_rbf &lt;- ksvm(class ~ ., data &#x3D; letters_train,</span><br><span class="line">                              kernel &#x3D; &quot;rbfdot&quot;)</span><br><span class="line"></span><br><span class="line">#predictions</span><br><span class="line">letter_predictions_rbf &lt;- predict(letter_classifier_rbf,</span><br><span class="line">                                  letters_test)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#is it better than the linear kernel?</span><br><span class="line">agreement_rbf &lt;- letter_predictions_rbf &#x3D;&#x3D; letters_test$class</span><br><span class="line">table(agreement_rbf)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#yes!</span><br><span class="line">prop.table(table(agreement_rbf))</span><br></pre></td></tr></table></figure>

<p>Another example (SVM):</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#check which packages are installed</span><br><span class="line">packages &lt;- c(&quot;caret&quot;, &quot;data.table&quot;, &quot;corrplot&quot;, &quot;rattle&quot;, &quot;randomForest&quot;, &quot;C50&quot;, &quot;rpart&quot;, &quot;ROCR&quot;, &quot;e1071&quot;,&quot;gmodels&quot;)</span><br><span class="line">if (length(setdiff(packages, rownames(installed.packages()))) &gt; 0) &#123;</span><br><span class="line">  install.packages(setdiff(packages, rownames(installed.packages())))  </span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">#load the packages</span><br><span class="line">library(caret)</span><br><span class="line">library(ggplot2)</span><br><span class="line">library(data.table)</span><br><span class="line">library(corrplot)</span><br><span class="line">library(rattle)</span><br><span class="line">library(randomForest)</span><br><span class="line">library(C50)</span><br><span class="line">library(rpart)</span><br><span class="line">library(ROCR)</span><br><span class="line">library(e1071)</span><br><span class="line">library(gmodels)</span><br><span class="line">library(data.table)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#import the dataset</span><br><span class="line">cust_data &lt;- fread(&#39;tele_com_churn_org.csv&#39;, header &#x3D; TRUE, sep &#x3D; &quot;,&quot;)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#clean the dataset</span><br><span class="line">#remove reduntant variables, like the first one</span><br><span class="line">cust_data &lt;- cust_data[, -1]</span><br><span class="line"></span><br><span class="line">#drop missing values</span><br><span class="line">cust_data[is.na(cust_data)] &lt;- 0</span><br><span class="line"></span><br><span class="line">#recode the churn status</span><br><span class="line">cust_data$Churn &lt;- replace(cust_data$Churn, cust_data$Churn &#x3D;&#x3D; &quot;No&quot;, 0)</span><br><span class="line">cust_data$Churn &lt;- replace(cust_data$Churn, cust_data$Churn &#x3D;&#x3D; &quot;Yes&quot;, 1)</span><br><span class="line">cust_data$Churn &lt;- as.numeric(cust_data$Churn)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Recode Variables: Recode using the library(car) package</span><br><span class="line">install.packages(&quot;car&quot;)</span><br><span class="line">library(car)</span><br><span class="line"></span><br><span class="line">cust_data$gender &lt;- recode(cust_data$gender, &quot;&#39;Male&#39;&#x3D;1; &#39;Female&#39;&#x3D;0&quot;)</span><br><span class="line">cust_data$Partner &lt;- recode(cust_data$Partner, &quot;&#39;Yes&#39;&#x3D;1; &#39;No&#39;&#x3D;0&quot;)</span><br><span class="line">cust_data$Dependents &lt;- recode(cust_data$Dependents, &quot;&#39;Yes&#39;&#x3D;1; &#39;No&#39;&#x3D;0&quot;)</span><br><span class="line">cust_data$PhoneService &lt;- recode(cust_data$PhoneService, &quot;&#39;Yes&#39;&#x3D;1; &#39;No&#39;&#x3D;0&quot;)</span><br><span class="line">cust_data$MultipleLines &lt;- recode(cust_data$MultipleLines, &quot;&#39;Yes&#39;&#x3D;1; &#39;No&#39;&#x3D;0;&#39;No phone service&#39;&#x3D;3&quot;)</span><br><span class="line">cust_data$InternetService &lt;- recode(cust_data$InternetService, &quot;&#39;No&#39;&#x3D;0; &#39;DSL&#39;&#x3D;1;&#39;Fiber optic&#39;&#x3D;2&quot;)</span><br><span class="line">cust_data$OnlineSecurity &lt;- recode(cust_data$OnlineSecurity, &quot;&#39;No&#39;&#x3D;0; &#39;Yes&#39;&#x3D;1;&#39;No internet service&#39;&#x3D;2&quot;)</span><br><span class="line">cust_data$OnlineBackup &lt;- recode(cust_data$OnlineBackup, &quot;&#39;No&#39;&#x3D;0; &#39;Yes&#39;&#x3D;1;&#39;No internet service&#39;&#x3D;2&quot;)</span><br><span class="line">cust_data$DeviceProtection &lt;- recode(cust_data$DeviceProtection, &quot;&#39;No&#39;&#x3D;0; &#39;Yes&#39;&#x3D;1;&#39;No internet service&#39;&#x3D;2&quot;)</span><br><span class="line">cust_data$TechSupport &lt;- recode(cust_data$TechSupport, &quot;&#39;No&#39;&#x3D;0; &#39;Yes&#39;&#x3D;1;&#39;No internet service&#39;&#x3D;2&quot;)</span><br><span class="line">cust_data$StreamingTV &lt;- recode(cust_data$StreamingTV, &quot;&#39;No&#39;&#x3D;0; &#39;Yes&#39;&#x3D;1;&#39;No internet service&#39;&#x3D;2&quot;)</span><br><span class="line">cust_data$StreamingMovies &lt;- recode(cust_data$StreamingMovies, &quot;&#39;No&#39;&#x3D;0; &#39;Yes&#39;&#x3D;1;&#39;No internet service&#39;&#x3D;2&quot;)</span><br><span class="line">cust_data$Contract &lt;- recode(cust_data$Contract, &quot;&#39;Month-to-month&#39;&#x3D;0; &#39;One year&#39;&#x3D;1;&#39;Two year&#39;&#x3D;2&quot;)</span><br><span class="line">cust_data$PaperlessBilling &lt;- recode(cust_data$PaperlessBilling, &quot;&#39;Yes&#39;&#x3D;1; &#39;No&#39;&#x3D;0&quot;)</span><br><span class="line">cust_data$PaymentMethod &lt;- recode(cust_data$PaymentMethod, &quot;&#39;Electronic check&#39;&#x3D;1; &#39;Mailed check&#39;&#x3D;2;&#39;Bank transfer (automatic)&#39;&#x3D;3; &#39;Credit card (automatic)&#39;&#x3D;4&quot;)</span><br><span class="line"></span><br><span class="line">#take the churn as factor variable</span><br><span class="line">cust_data[, &#39;Churn&#39;] &lt;- lapply(cust_data[, &#39;Churn&#39;], factor)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#inspect the data</span><br><span class="line">summary(cust_data)</span><br><span class="line">str(cust_data)</span><br><span class="line"></span><br><span class="line">corrmatrix &lt;- round(cor(cust_data[, - &#39;Churn&#39;]), digits &#x3D; 2)</span><br><span class="line">corrmatrix</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#switch to factor of categorical variables</span><br><span class="line">cust_data$gender &lt;- factor(cust_data$gender)</span><br><span class="line">cust_data$SeniorCitizen &lt;- factor(cust_data$SeniorCitizen )</span><br><span class="line">cust_data$Partner &lt;- factor(cust_data$Partner)</span><br><span class="line">cust_data$Dependents &lt;- factor(cust_data$Dependents)</span><br><span class="line">cust_data$PhoneService &lt;- factor(cust_data$PhoneService)</span><br><span class="line">cust_data$MultipleLines &lt;- factor(cust_data$MultipleLines)</span><br><span class="line">cust_data$InternetService &lt;- factor(cust_data$InternetService)</span><br><span class="line">cust_data$OnlineSecurity &lt;- factor(cust_data$OnlineSecurity)</span><br><span class="line">cust_data$OnlineBackup &lt;- factor(cust_data$OnlineBackup)</span><br><span class="line">cust_data$DeviceProtection &lt;- factor(cust_data$DeviceProtection)</span><br><span class="line">cust_data$TechSupport &lt;- factor(cust_data$TechSupport)</span><br><span class="line">cust_data$StreamingTV &lt;- factor(cust_data$StreamingTV)</span><br><span class="line">cust_data$StreamingMovies &lt;- factor(cust_data$StreamingMovies)</span><br><span class="line">cust_data$Contract &lt;- factor(cust_data$Contract)</span><br><span class="line">cust_data$PaperlessBilling &lt;- factor(cust_data$PaperlessBilling)</span><br><span class="line">cust_data$PaymentMethod &lt;- factor(cust_data$PaymentMethod)</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">#build the model</span><br><span class="line">#80:20 ratio for training and testing parts</span><br><span class="line">set.seed(12345)</span><br><span class="line">cust_data_rand &lt;- cust_data[order(runif(7043)),]</span><br><span class="line">training &lt;- cust_data_rand[1:5635,]</span><br><span class="line">testing &lt;- cust_data_rand[5636:7043,]</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># SVM</span><br><span class="line"># reminder: &#39;cost&#39; quantifies the penalty associated with having an observation on the wrong side of the classification boundary</span><br><span class="line"># instead of specifying a cost up front, we can use the tune function to est various costs and identify which value produces the best fitting model</span><br><span class="line">set.seed(1234)</span><br><span class="line">svm &lt;- tune.svm(Churn ~ ., data &#x3D; training, seq(0.5, 0.9, by &#x3D; 0.1), cost &#x3D; seq(100, 1000, by &#x3D; 100), kernel &#x3D; &quot;radial&quot;, tunecontrol &#x3D; tune.control(cross &#x3D; 10))</span><br><span class="line"></span><br><span class="line">print(svm)</span><br><span class="line">summary(svm)</span><br><span class="line">svm$performances</span><br><span class="line"></span><br><span class="line"># find the best SVM model</span><br><span class="line">svmfit &lt;- svm$best.model</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>Machine Learning</category>
      </categories>
      <tags>
        <tag>Machine Learning</tag>
      </tags>
  </entry>
  <entry>
    <title>Markdown语法总结</title>
    <url>/2020/04/03/Markdown-Summary/</url>
    <content><![CDATA[<h1 id="Markdown语法总结"><a href="#Markdown语法总结" class="headerlink" title="Markdown语法总结"></a>Markdown语法总结</h1><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>Hello, world! 本文是 Sasegoi’s Blog 的第一篇文章，很高兴可以成功地完成 hexo 与 github 的配置，非常感谢 Bilibili 作者 hojun_cn 与 CodeSheep 的教学视频。本 Blog 主要内容为个人学习的笔记，所以可能存在一些问题，烦请各位不吝赐教，集思广益。<a id="more"></a></p>
<h2 id="基本语法"><a href="#基本语法" class="headerlink" title="基本语法"></a>基本语法</h2><h3 id="标题-Headers"><a href="#标题-Headers" class="headerlink" title="标题(Headers)"></a>标题(Headers)</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># This is an &lt;h1&gt; tag</span><br><span class="line">## This is an &lt;h2&gt; tag</span><br><span class="line">###### This is an &lt;h6&gt; tag</span><br></pre></td></tr></table></figure>

<p>以上代码为 atx 形式。</p>
<h3 id="强调-Emphasis"><a href="#强调-Emphasis" class="headerlink" title="强调(Emphasis)"></a>强调(Emphasis)</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">*This text will be italic*</span><br><span class="line">_This will also be italic_</span><br><span class="line"></span><br><span class="line">**This text will be bold**</span><br><span class="line">__This will also be bold__</span><br><span class="line"></span><br><span class="line">_You **can** combine them_</span><br></pre></td></tr></table></figure>

<p>如果需要在文本中显示成对的 <code>*</code> 或者 <code>_</code>，需要在符号前加入 <code>\</code>。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">This is \*example\* \_text\_</span><br></pre></td></tr></table></figure>

<h3 id="无序列表-Disorder-List"><a href="#无序列表-Disorder-List" class="headerlink" title="无序列表(Disorder List)"></a>无序列表(Disorder List)</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">* Item 1</span><br><span class="line">* Item 2</span><br><span class="line">  * Item 2a</span><br><span class="line">  * Item 2b</span><br></pre></td></tr></table></figure>

<p>或者替换 <code>*</code> 为 <code>-</code></p>
<h3 id="有序列表-Ordered-List"><a href="#有序列表-Ordered-List" class="headerlink" title="有序列表(Ordered List)"></a>有序列表(Ordered List)</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">1. Item 1</span><br><span class="line">1. Item 2</span><br><span class="line">  1. Item 2a</span><br><span class="line">  1. Item 2b</span><br></pre></td></tr></table></figure>

<h3 id="超链接-Links"><a href="#超链接-Links" class="headerlink" title="超链接(Links)"></a>超链接(Links)</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">http:&#x2F;&#x2F;github.com - automatic!</span><br><span class="line">[GitHub](http:&#x2F;&#x2F;github.com)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">[here][3]</span><br><span class="line">[3]http:&#x2F;&#x2F;github.com</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;http:&#x2F;&#x2F;github.com&gt;</span><br></pre></td></tr></table></figure>

<h3 id="图片-Images"><a href="#图片-Images" class="headerlink" title="图片(Images)"></a>图片(Images)</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">![GitHub Logo](&#x2F;images&#x2F;logo.png)</span><br><span class="line">Format: ![Alt Text](url)</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">![Alt text](http:&#x2F;&#x2F;www.izhangbo.cn&#x2F;wp-content&#x2F;themes&#x2F;minty&#x2F;img&#x2F;logo.png &quot;Optional title&quot;)</span><br></pre></td></tr></table></figure>

<h3 id="块引用-Blockquotes"><a href="#块引用-Blockquotes" class="headerlink" title="块引用(Blockquotes)"></a>块引用(Blockquotes)</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">As Kanye West said:</span><br><span class="line"></span><br><span class="line">&gt; We&#39;re living the future so</span><br><span class="line">&gt; the present is our past.</span><br></pre></td></tr></table></figure>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&gt;给引用的文本开始位置都加一个 &#39;&gt;&#39;，</span><br><span class="line">&gt;便可组成一个块引用。在块引用中，可以结合</span><br><span class="line">&gt;其他markdown元素一块使用，比如列表。</span><br><span class="line">&gt;**强调**</span><br><span class="line">也可以只在第一行加大于号，其他位置不加。</span><br><span class="line"></span><br><span class="line">&gt;- 块引用里使用列表，需要和上面的内容隔开一个空行</span><br><span class="line">&gt;- 记得加空格哦。</span><br></pre></td></tr></table></figure>

<h3 id="表格-Table"><a href="#表格-Table" class="headerlink" title="表格(Table)"></a>表格(Table)</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">First Header | Second Header</span><br><span class="line">------------ | -------------</span><br><span class="line">Content from cell 1 | Content from cell 2</span><br><span class="line">Content in the first column | Content in the second column</span><br></pre></td></tr></table></figure>
<h3 id="删除线"><a href="#删除线" class="headerlink" title="删除线"></a>删除线</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">这就是 ~~删除线~~</span><br></pre></td></tr></table></figure>

<h3 id="Emoji"><a href="#Emoji" class="headerlink" title="Emoji"></a>Emoji</h3><p>Github supports <a href="https://github.com/ikatyang/emoji-cheat-sheet/blob/master/README.md" target="_blank" rel="noopener">emoji</a></p>
<h2 id="Reference"><a href="#Reference" class="headerlink" title="Reference"></a>Reference</h2><p><a href="https://guides.github.com/features/mastering-markdown/" target="_blank" rel="noopener">Github Guides</a><br><a href="http://www.izhangbo.cn/info/57e1d9a3c2cb6.html" target="_blank" rel="noopener">izhangbo</a><br><a href="http://xianbai.me/learn-md/index.html" target="_blank" rel="noopener">xiaobai</a></p>
]]></content>
      <categories>
        <category>Markdown Learning</category>
      </categories>
      <tags>
        <tag>Markdown</tag>
      </tags>
  </entry>
</search>
